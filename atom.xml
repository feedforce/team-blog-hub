<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://localhost:3000</id>
    <title>Feedforce Engineers' Blogs</title>
    <updated>2021-09-07T23:14:19.905Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="http://localhost:3000"/>
    <subtitle>フィードフォースグループに所属するエンジニアのブログ記事をまとめています。</subtitle>
    <rights>© Feedforce Inc.</rights>
    <entry>
        <title type="html"><![CDATA[Amazon EKS で高負荷時に CoreDNS が原因で稀にネットワークエラーが発生していた時のトラブルシュート]]></title>
        <id>https://developer.feedforce.jp/entry/2021/09/02/134725</id>
        <link href="https://developer.feedforce.jp/entry/2021/09/02/134725"/>
        <updated>2021-09-02T04:47:25.000Z</updated>
        <summary type="html"><![CDATA[ソーシャルPLUS の開発チームでインフラエンジニア をやっています id:mayuki123 です。今月からフィードフォースから分社化をした株式会社ソーシャルPLUS の所属となりましたが、仕事内容は変わらずにサービスのインフラ改善を進めていく事になるかと思います。2019年11月に技術スタックを整理してみたという記事から2年弱経過していますが、ソーシャルPLUSのインフラ環境は、一部アプリケーションについてはコンテナ環境を Amazon EKS にホスティングして本番運用するようになりました。あと数ヶ月ほどで全ての環境がEC2からコンテナに置き換えられると良いなと思っています(願望)。developer.feedforce.jpそして、既に利用されている機能の一部を Amazon EKS に移行して、しばらく経過した時にアプリケーションでネットワークエラーが稀に発生していました。原因調査をした結果が CoreDNS の負荷によるものと発覚するまでのトラブルシュートの流れについて、記事として書き残しておきます。発生していた事象Datadog を活用した原因調査アプリケーションの負荷状況EKS 上のコンテナの調査EKS のCoreDNS の調査CoreDNS のデバッグログの有効化Kubernetes の名前解決についてCoreDNS の負荷軽減ドメインの末尾にドット (.) を追加する/etc/resolv.conf で ndots:1 の設定をするその他の CoreDNS の負荷軽減の方法最終的な結果おわりにおまけ発生していた事象ソーシャルPLUSでは、バックエンドのアプリケーションでエラーが発生した時に、Bugsnag を利用して Slack 通知するようにしています。ある時にMysql2::Error::ConnectionError が発生しました。単発のネットワークエラーの場合はアプリケーションがリトライする事でサービス影響がない事も多く、一時的な問題と思って静観する事があるかと思います。しかし、また数日後に同じ事象が発生しました。ハインリッヒの1：29：300の法則のように、ちょっとした異常を見落としていると重大なサービス障害となってしまう可能性があるので、原因調査を始めます。Datadog を活用した原因調査ソーシャルPLUSでは、モニタリングサービスの Datadog を利用しているのでメトリクスやログの調査を出来るようになっています。どこが原因かを探り始めました。アプリケーションの負荷状況まずはアプリケーションで利用するサーバの負荷状況を確認する所から始めました。Mysql2::Error::ConnectionError が発生した時刻は EKS の Node の CPU 使用率が 70% ほどで、アプリケーションで負荷のかかる処理の最中でした。また、データベースの負荷は少し前に負荷対策の改善をした事もあって、今回の事件の犯人ではなさそうです。他にもEC2 と DB 間でネットワークのボトルネックがないかなどの確認はしましたが、CPU 使用率が高い以外の問題は特に見つかりませんでした。完全犯罪でしょうか。EKS 上のコンテナの調査サーバ単体の問題ではないとすると、Amazon EKS で何か起きている事を疑うことにしました。EKSで動かしているコンテナのログは Datadog Logs に送っているので、エラーが発生していたアプリケーション以外のログ を確認していると、MySQL の ConnectionError が発生した時間帯に下記の Warning のメッセージが出ている事に気づきました。このログは Amazon Kinesis Data Firehose にログを送る Fluent Bit のコンテナで発生しており、エラーが発生してたアプリケーションとは異なるノードに存在してました。[yyyy/mm/dd hh:mm:ss] [ warn] [net] getaddrinfo(host='kinesis.ap-northeast-1.amazonaws.com'): Name or service not known同時刻に特定のアプリケーション以外のコンテナも影響を受けていることから、EKS の中で問題がありそうです。元々、EKSに関する技術ブログは目を通すようにしていた事もあり、Kubernetes の DNS の名前解決で問題が発生する場合があるというのは知っていたので、CoreDNSに焦点を当てて調べることにしました。アウトプットをしてくれる人たちには、いつも感謝をしています。Production Ready EKS CoreDNS Configuration | by Serkan Capkan | Cloutive Technology Solutions - Tech Blog | MediumEKSでDNSを安定させるために対応したこと - Chatwork Creator's Noteスマホゲームの API サーバにおける EKS の運用事例 | エンジニアブログ | GREE EngineeringEKS のCoreDNS の調査Datadog Agent で Kurbernetes の各種メトリクスを収集していて、EKS の CoreDNS の状況も Datadog の Metric Explorer で確認する事が出来るようになっています。Datadog で取得可能な CoreDNS のメトリクスcoredns.request_count を確認すると特定の時間帯で CoreDNS へのリクエストが多い状態で、このタイミングでの CoreDNS Pod の CPU 負荷は10％前後でしたが、それ以外に不審なメトリクスは存在しませんでした。まだ事象の原因との確信は持てないですが、負荷がそれなりにかかっていることは確かなのでリクエストが多くなる理由を調べます。CoreDNS のデバッグログの有効化まずは CoreDNS のデバッグログを確認したいとなるかと思いますが、EKS の CoreDNS はデフォルトだとデバッグログの出力がオフの状態のため、どのようなリクエストが到達しているのかは確認する事ができません。CoreDNS のログを有効化する方法は AWS のナレッジベースにある記事に方法が記載されています。Amazon EKS での DNS 障害のトラブルシューティングこの記事によると、Namespace(kube-system) に Configmap (coredns) があるので、Corefile 設定に log を追加するとデバッグログ が出力されるようになります。# kubectl -n kube-system edit configmap corednskind: ConfigMapapiVersion: v1data:  Corefile: |    .:53 {        log    # Enabling CoreDNS Logging        errors        health        kubernetes cluster.local in-addr.arpa ip6.arpa {          pods insecure          upstream          fallthrough in-addr.arpa ip6.arpa        }        ...上記の設定をすると CoreDNS のPod の標準出力にデバッグログ が出力されるようになります。私の触っていた EKS の環境の場合は、数分ほどで CoreDNS の Pod で reload が発生して元の設定（デバッグログ がオフ）に戻るようになってました。Kubernetes の名前解決について次にKubernetes 上のコンテナはどのように名前解決するのかを知っておく必要があります。Kurbernetes の Pod の DNS リゾルバー(/etc/resolv.conf) のデフォルト設定は下記のようになっています。% kubectl exec fluent-bit-46zvl -- cat /etc/resolv.confnameserver 172.20.0.10search logging.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5この状態で Fluent Bit のコンテナから Amazon Kinesis の API エンドポイントに疎通する場合は、CoreDNS に8回のリクエストが発生します。これは、IPv4 , IPv6 の2種類の名前解決を search の数だけ名前解決を試みた後で EKS 外に名前解決をする設定になっているからです。この設定になっているおかげで Kubernetes の Service を使った名前解決が出来るようになっています。また、options ndots:5 の設定は . の数が 5個以上の時は最初から外部に名前解決するようになります。そのため、Amazon Aurora や ElastiCache などのデータベースへの クラスターエンドポイントは . の数が五個以上あるので、CoreDNSへのリクエスト回数は少なくて済みます。ここを意識しなくてよいのはありがたいですね。ソーシャルPLUSというプロダクトの特性上、EKS 内のアプリケーションから外部サービスの API を実行する機会が多々あります。特定のタイミングで外部のサービスに大量のAPIリクエストを実行した際に、CoreDNS へのリクエストが増大してしまい不安定になってしまったのではと考えられます。CoreDNS の負荷軽減Kurbernetes 上のコンテナの名前解決を知ると、外部サービスのAPI を実行する際には CoreDNS へのリクエストが多くなる事が分かりました。ここで、CoreDNS へのリクエスト数を減らす方法は下記の二つがあります。これも AWS のナレッジベースに方法が記載されているので、詳細は下記の記事を読む方が良いと思います。Amazon EKS での DNS 障害のトラブルシューティングドメインの末尾にドット (.) を追加する接続先のドメインの最後に . をつけると、EKS の内部で名前解決を複数回しないようになり、CoreDNS へのリクエストの総数が減ります。一例をあげると、example.com ではなく、 example.com. とする事で最初から EKS の外部に名前解決をしてくれるようになります。ドメインが SDK の内部で定義されているような場合など、変更出来ない場合はこの方法は利用出来ないかと思います。/etc/resolv.conf で ndots:1 の設定をする/etc/resolv.conf に options ndots:5 とデフォルトで設定されている数値を 1 にする事で、ドメインに . が含まれている場合は常に EKS の外部に名前解決するようになります。Kubernetes の Manifest に spec.dnsConfig パラメータを設定する事で Pod 単位で変更が出来ます。ただし、この設定をすると EKS 内部で名前解決をしなくなってしまいますが、<name>.<namespace>.svc.cluster.local. のように最後に . をつけると名前解決出来ました。Kurbernetes の Service の数が多いとこの方法を周知させるのも大変だと思います。apiVersion: apps/v1kind: Deploymentmetadata:  name: hogespec:  template:    spec:      dnsConfig:        options:          - name: ndots            value: "1"その他の CoreDNS の負荷軽減の方法上記の二つの方法は CoreDNS へのリクエスト数を減らすことで、負荷を軽減するようなアプローチでした。CoreDNS の Pod 数はデフォルトで 2個となりますが、CoreDNS のPod をオートスケールする手段もあります。Autoscale the DNS Service in a Clusterまた、Daemonset で DNS キャッシュをノード単位で配置するという方法もあります。KubernetesクラスターでNodeLocal DNSキャッシュを使用するこの辺りは他の方が書いた技術ブログも多くあるかと思うので、この記事では特に説明はしないです。最終的な結果ソーシャルPLUSでは最終的に根本原因の CoreDNS へのリクエスト数を減らすために /etc/resolv.conf で ndots:1 の設定をするようにしました。この設定をアプリケーションの Pod に適応した所、CoreDNS へのリクエスト数は 25% ほどと目に見えて減少させる事が出来ました。キャプチャは載せてないですが、CoreDNS の Pod の CPU使用率も 以前の半分ほどになったので、負荷軽減の目的は達成しました。ここまで、確信を持てないまま CoreDNS の負荷軽減に取り組みましたが、元々のネットワークエラーであった Mysql2::Error::ConnectionError のエラーは再発しなくなりました。また、EKS 上の他のコンテナも Name or service not known のような名前解決が出来ないといったエラーも発生しなくなりました。CoreDNS の負荷を減らす事で悩まされていた問題の解消が出来たと思います。今回のように比較的早い段階で気づく事が出来たので、お客さんへのサービス影響のある問題に発展せずに済みました。今後、利用者数が増えてより負荷のかかる状況になってきた時には再発する可能性はありますが、早い段階で気付けるように日々確認するダッシュボードにメトリクスを追加するようにしています。その時がきた場合は CoreDNS の Pod 数の調整や DNS キャッシュの導入が必要になりそうです。おわりに最終的には Pod の DNS 設定を調整するだけでネットワークエラーは解決しました。この記事では、結果だけではなくて解決に至るまでの経緯をメインにまとめてみました。実施していて良かったと思うことを下記にまとめます。これらの事が出来ていなければ、今回のようなネットワークエラーはたまに発生する事象として、根本原因の追及は出来なかったと思うので、サービスのオブザーバビリティを整備する事や日々の情報収集は大事ですね。アプリケーションのエラーを Slack に通知していたKurbernetes のメトリクスを Datadog で確認できる状態だったコンテナのログを一元的に Datadog Logs  で閲覧できるようにしていた他の人の技術ブログから Kubernetes の CoreDNS が不安定になることを知っていたこの記事に間違っている内容や、もっと良い改善方法がある事をご存知の方がいましたら、優しく教えてください。おまけ現在、ソーシャルPLUS では作りたい機能が山ほどある状況でまだまだ成長するサービスになると思うので、成長を続けるサービスに携わりたいエンジニアやデザイナーのご応募をお待ちしております！サイトにはまだないですが、インフラエンジニアも近いうちに募集をする事にはなると思います。open.talentio.comopen.talentio.comopen.talentio.comフィードフォース の他のサービスもエンジニアを募集してますので、興味があればご応募お待ちしております！engineers.recruit.feedforce.jp]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looker で Join 先の view の primary_key をいい感じにテストする方法をようやく見つけた]]></title>
        <id>https://developer.feedforce.jp/entry/2021/08/30/150000</id>
        <link href="https://developer.feedforce.jp/entry/2021/08/30/150000"/>
        <updated>2021-08-30T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、id:masutaka26 です。先週はまるっと夏休みにしてました。今日からまた Looker と戯れる日々が始まります。丸１年 Looker と戯れてきて最近ようやく、Join 先の view でも primary_key が壊れてないことを保証するテストの書き方が分かったので、今回紹介します。Looker における primary_key の役割primary_key の実装例LookML 開発におけるテストJoin 先の view は primary_key をテスト出来ないことがあるJoin 先の view の primary_key をいい感じにテストするまとめと所感おまけLooker における primary_key の役割Looker には Symmetric Aggregates という、合計を重複させない素晴らしい仕組みがあります。以前このブログでも紹介しました。その Symmetric Aggregates では primary_key が重要な役割を果たします。適切に設定されていないと、以下のような問題が発生します。primary_key が重複すると fanout エラーが発生することがあるprimary_key が null だと Measure が 0 になることがあるこのような問題は大概、ふわっと質問が来て発覚します。今のタスクを保留にして調査することは精神的になかなか辛いものがあり、それなりに時間も費やすことになるため、可能な限り事前に避けたいところです。primary_key の実装例私が所属する Feedmatic では、ウェブ広告や Google Analytics のデータを扱っています。正規化されたきれいなデータは少なく、Rails の id のようなユニークなカラムは存在しないことが多いです。そのため、このようにいくつかの Dimension を組み合わせて primary_key を定義します。dimension: id {  primary_key: yes  type: string  sql: CONCAT(${dimension1}, ${dimension2}, IFNULL(${dimension3}, '')) ;;  hidden: yes}※ DWH は BigQuery を使っています。これで済めばよいのですが、上の例だとある日突然 dimension2 が null になり始めたり、全ての string 型の Dimension を使っても重複し始めることがあります。データの性格は理解していたつもりでしたが、実際どちらもありました。😭LookML 開発におけるテスト以上の課題を解決するために、test パラメータが使えます。例えばこのような parent explore があったとします。Join がないのでシンプルです。explore: parent {  ...}view: parent {  dimension: id {    primary_key: yes    ...  }  measure: count {    type: count  }}私はこのようなテストを書いて、全ての parent.id が null でないことと、重複しないことを保証させています。test: parent_id_is_not_null {  explore_source: parent {    column: id {}    sorts: [parent.id: asc]    limit: 1  }  assert: id_is_not_null {    expression: NOT is_null(${parent.id}) ;;  }}test: parent_id_is_unique {  explore_source: parent {    column: id {}    column: count {}    sorts: [parent.count: desc]    limit: 1  }  assert: id_is_unique {    expression: ${parent.count} = 1 ;;  }}👉 ソート時に null が先頭と末尾のどちらに来るかは、DWH の実装によります。Join 先の view は primary_key をテスト出来ないことがあるさて、child view を Join する必要が出てきました。explore: parent {  join: child {    type: left_outer    relationship: one_to_many    sql_on: ... ;;  }}view: parent {  dimension: id {    primary_key: yes    ...  }  measure: count {    type: count  }}view: child {  dimension: id {    primary_key: yes    ...  }  measure: count {    type: count    hidden: yes  }}同じように child.id のテストを書きましたが、うまくいきません。is_not_null はまだしも、is_unique がダメです。# これは OKtest: parent_child_id_is_not_null {  explore_source: parent {    column: id { field: child.id }    sorts: [child.id: asc]    limit: 1  }  assert: child_id_is_not_null {    expression: NOT is_null(${child.id}) ;;  }}# parent の count になり、テストが通らない。test: parent_child_id_is_unique {  explore_source: parent {    column: id { field: child.id }    column: count { field: child.count }    sorts: [parent.count: desc]    limit: 1  }  assert: child_id_is_unique {    expression: ${child.count} = 1 ;;  }}よく考えれば当たり前の話で、Join した状態でテストを書いているからです。そもそも child view の primary_key のテストをしたいだけなのに、Join は邪魔です。Join 先の view の primary_key をいい感じにテストするchild view と同じファイルに、こっそり child explore を定義します。hidden にして存在を消しています。さらに required_access_grants で、開発者以外の URL 直打ちによるアクセスも防いでいます。view: child {  dimension: id {    primary_key: yes    ...  }  measure: count {    type: count    hidden: yes  }}# Define for testexplore: child {  hidden: yes  required_access_grants: [can_view_explores_for_tests]}access_grant である can_view_explores_for_tests はこのような定義です。# For testsaccess_grant: can_view_explores_for_tests {  user_attribute: view_explores_for_tests  allowed_values: ["yes"]}User attribute である view_explores_for_tests は、今回のような「Join 先の view をテストすること」全般に使います。User Access は None、Default Value も no です。開発者用の Group を作り、その Group value を yes にしました。ここまでやらずとも全員アクセス不可でも良いのですが、テストが落ちた時に「クエリの探索」からの調査ができなくなるので、開発者にはアクセス権を与えるポリシーにしています。あとは parent explore と同じようにテストを書くだけです。test: child_id_is_not_null {  explore_source: child {    column: id {}    sorts: [child.id: asc]    limit: 1  }  assert: id_is_not_null {    expression: NOT is_null(${child.id}) ;;  }}test: child_id_is_unique {  explore_source: child {    column: id {}    column: count {}    sorts: [child.count: desc]    limit: 1  }  assert: id_is_unique {    expression: ${child.count} = 1 ;;  }}テストは通っても、LookML validation error が発生するかもしれません。その時は fields パラメータを使って、露出する field を限定すると良いでしょう。# Define for testexplore: child {  hidden: yes  required_access_grants: [can_view_explores_for_tests]  fields: [child.id, child.count] # Avoid LookML validation error}このテクニックは Error: Unknown or Inaccessible Field – Looker Help Center でも紹介されています。まとめと所感LookML 開発者で、且つテストを書いていて、且つ Join 先の view の primary_key に課題を抱えている、大変ニッチな層向けに記事を書きました。どこかの誰かに参考になれば幸いです。もっと良い方法や、今回のやり方はここがマズイとかあれば @masutaka にお知らせ頂けると大変うれしいです。🙏Feedmatic では今回のような view は数十もあり、primary_key のテストはまだ書き始めたばかりです。Looker ではテストは直列でしか実行されないようで、書けば書くほど全テストが遅くなるのはモヤモヤしています。さすがに要望しようと思ってますが。それに関連して、最近ディレクトリやファイル構成を再検討しました。次回はその記事を書く予定です。おまけ今回の記事を書く過程で、中の人が書いたベストプラクティスを見つけました。今回の記事ほど細かいテクニックは書かれていませんが、全体を網羅した良記事なので要チェックです。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manjaro でハイバネート出来るようにしたメモ]]></title>
        <id>https://masutaka.net/chalow/2021-08-25-1.html</id>
        <link href="https://masutaka.net/chalow/2021-08-25-1.html"/>
        <updated>2021-08-25T14:59:59.000Z</updated>
        <summary type="html"><![CDATA[４月から個人 PC を MacBook Air から Linux マシンに変えました。[2021-05-06-1]スライド P41 に書いたとおり、たまにスリープから復帰しなくてつらいです。仕方がないので「スリープしない設定にする。常にAC電源ON🤗」という運用でカバー（）をしていました。😭いつまでも続けるわけにもいかないので、今回重い腰を上げてハイバネート出来るようにしました。Linux (Xfce4) のスリープ方式私が使っているウィンドウマネージャーは Xfce4 です。電源管理の設定等で出てくるスリープ関連の用語と、自分なりの理解は以下のとおりです。情報源は Wikipedia です。😆・サスペンドメモリ以外の給電停止した状態。ACPI では S3 に相当する。・ハイバネートメモリの内容をストレージに移し電源断。ACPI では S4 に相当する。・ハイブリッドスリープサスペンドとハイバネートの中間だと思われるが、試したらすぐハイバネートしただけだった...。macOS (Big Sur) のスリープ方式macOS ではサスペンドやハイバネートの使い分けは不要で、意識するのは「スリープ」のみです。以下のような「セーフスリープ」という機構を備えているそうです。・スリープ状態になると、メモリの内容を保持したままストレージにも保存する・スリープ中にバッテリーが切れていなければ、復帰時にメモリの内容を使う・スリープ中にバッテリーが切れていたら、復帰時にストレージの内容を使う奨励はされていませんが、pmset コマンドでスリープ方式を「セーフスリープ」から「スリープ」または「ディープスリープ」に変更は出来るようです。「スリープ」は ACPI の S3 に相当し、「ディープスリープ」は S4 に相当するようなので、Xfce4 の「ハイブリッドスリープ」は「セーフスリープ」が本来の振る舞いなのかも。※ 状態としての「スリープ」と、その種類としての「スリープ」（と「セーフスリープ」「ディープスリープ」）があるので、文章がややこしいです。Linux と macOS の振る舞いの違いmacOS の素晴らしい点に、ユーザーにスリープをほぼ意識させない設計があると思います。サスペンドやハイバネートのような用語が現れないこともさることながら、MacBook なら蓋を開けばログイン画面が表示されるだけです。多少復帰がもたつくことはあるのはご愛嬌。Linux はサスペンドは同じとしても、ハイバネートは OS の起動から始まるので、「あれ？バッテリー切れてた？」と一瞬不安になります。Windows も同じという理解です。どの OS もバックグラウンドの処理は同じだと思います。macOS はユーザーへの見せ方がきれいですね。Manjaro でのハイバネート設定方法「サスペンドとハイバネート - ArchWiki」に従いました。以下の流れです。1. スワップファイルを作る2. 関連するカーネルパラメータを GRUB（ブートローダー）に追加する3. 関連するフックを initramfs に追加する1. スワップファイルを作る今回は /swapfile という 18GB のスワップファイルを作りました。$ sudo fallocate -l 18G /swapfile$ sudo chmod 600 /swapfile$ sudo mkswap /swapfile$ sudo swapon /swapfileこの VivoBook のメモリは 16GB です。RedHat の記事に従い、スワップファイルのサイズは 18GB にしました。無事作られました。$ swapon -showFilename Type Size Used Priority/swapfile file 18874364 0 -2永続化するために /etc/fstab に以下を追記しました。/swapfile none swap defaults 0 0ちなみに、今回のような任意サイズのファイルは dd でも作れます。ただ、dd は実際にファイル書き込みが発生するため、可能なら fallocate を使うほうが無駄がなくて良いと思います。参考記事: Linuxでサイズ指定してダミーファイルを作成する方法 - conf t2. 関連するカーネルパラメータを GRUB（ブートローダー）に追加する/etc/default/grub を以下のように変更しました。/swapfile が、どのデバイスの、どの位置（オフセット）に存在するかを教えています。--- /tmp/grub.orig2021-08-25 01:12:57.415049508 +0900+++ /etc/default/grub2021-08-23 23:42:08.875333968 +0900@@ -2,7 +2,7 @@ GRUB_TIMEOUT=5 GRUB_TIMEOUT_STYLE=hidden GRUB_DISTRIBUTOR="Manjaro"-GRUB_CMDLINE_LINUX_DEFAULT="quiet apparmor=1 security=apparmor udev.log_priority=3"+GRUB_CMDLINE_LINUX_DEFAULT="quiet apparmor=1 security=apparmor udev.log_priority=3 resume=UUID=52d772c6-e49d-4be0-9963-c9aae2a9e4f9 resume_offset=52037632" GRUB_CMDLINE_LINUX="" # If you want to enable the save default function, uncomment the followingGRUB_CMDLINE_LINUX がユーザー用の変数かと思い、初めはこれを使ってしまいましたが、リカバリモード用でした（GNU GRUB Manual 2.06: Simple configuration）。"resume=UUID=52d772c6-e49d-4be0-9963-c9aae2a9e4f9" で、/swapfile が置いてあるデバイスを教えています。雑に調べると /dev/nvme0n1p5 です。/swapfile なので当たり前ですが、"/" がマウントポイントです。$ df /swapfileFilesystem 1K-blocks Used Available Use% Mounted on/dev/nvme0n1p5 527306168 62862840 437583404 13% /"resume=/dev/nvme0n1p5" でも問題はないとは思いますが、外付けストレージの追加等で名前が変わるリスクはゼロではないため、UUID による永続的な命名方法を使いました。各デバイスの UUID 確認方法は簡単です（永続的なブロックデバイスの命名 - ArchWiki）。$ ls -alF /dev/disk/by-uuidtotal 0drwxr-xr-x 2 root root 100 8月 24 00:14 ./drwxr-xr-x 8 root root 160 8月 24 00:14 ../lrwxrwxrwx 1 root root 15 8月 24 00:14 2E80-83B2 -> ../../nvme0n1p1lrwxrwxrwx 1 root root 15 8月 24 00:14 52d772c6-e49d-4be0-9963-c9aae2a9e4f9 -> ../../nvme0n1p5lrwxrwxrwx 1 root root 15 8月 24 00:14 DA82730D8272ECFF -> ../../nvme0n1p4"resume_offset=52037632" で、/swapfile が /dev/nvme0n1p5 の先頭からどの位置（オフセット）に存在するかを教えています。filefrag というコマンドで分かるようです。$ sudo filefrag -v /swapfile | head -n 5Filesystem type is: ef53File size of /swapfile is 19327352832 (4718592 blocks of 4096 bytes) ext: logical_offset: physical_offset: length: expected: flags: 0: 0.. 0: 52037632.. 52037632: 1: 1: 1.. 2047: 52037633.. 52039679: 2047: unwrittenext4 はフラグメンテーションが起きづらいそうだけど、e4defrag で手動デフラグしたらオフセットは変わり得るのでは？と思いました。ちょっと怖いな。どうなんでしょう？/etc/default/grub を変更したら /boot/grub/grub.cfg を再作成します。$ sudo grub-mkconfig -o /boot/grub/grub.cfg3. 関連するフックを initramfs に追加する/etc/mkinitcpio.conf を以下のように変更しました。カーネル起動時に resume というモジュールがロードされるのかな。filesystems の前に追加するのがとても重要だそうです。--- /tmp/mkinitcpio.conf.orig2021-08-25 01:40:50.793227026 +0900+++ /etc/mkinitcpio.conf2021-08-23 23:07:04.396638336 +0900@@ -49,7 +49,7 @@ # ## NOTE: If you have /usr on a separate partition, you MUST include the # usr, fsck and shutdown hooks.-HOOKS="base udev autodetect modconf block keyboard keymap filesystems fsck"+HOOKS="base udev autodetect modconf block keyboard keymap resume filesystems fsck" # COMPRESSION # Use this to compress the initramfs image. By default, gzip compression/etc/mkinitcpio.conf を変更したら /boot/initramfs-*.img を再作成します。$ sudo mkinitcpio -p linux510linux510 は /etc/mkinitcpio.d/ 以下のファイルと対応していました。今回は /etc/mkinitcpio.d/linux510.preset しかなかったので、linux510 になりました。ちなみに mkinitcpio を引数なしで実行すると、dry run モードによる確認が出来ます。$ sudo mkinitcpio==> Starting dry run: 5.10.59-1-MANJARO -> Running build hook: [base] -> Running build hook: [udev] -> Running build hook: [autodetect] -> Running build hook: [modconf] -> Running build hook: [block] -> Running build hook: [keyboard] -> Running build hook: [keymap] -> Running build hook: [resume] -> Running build hook: [filesystems] -> Running build hook: [fsck]==> Generating module dependencies==> Dry run complete, use -g IMAGE to generate a real image注意点GRUB や Linux カーネルが更新されたら、2 や 3 の再設定が必要みたいです。面倒なので私は mitamae でゆるふわ構成管理しています。まとめハイバネート出来た時は、思わずおぉー！と声が出ました。他の OS では当たり前すぎるハイバネートですが、自分で設定すると感慨深いものがあります。これで VivoBook から気軽に AC 電源コードを抜けるようになりました。なんと当たり前な...。今までは気がつくと電源が落ちていたので、AC 電源コードも気も抜けませんでした。😭サスペンドは復帰時に画面が表示されず、電源ボタンを長押しするしかなくなることがあるので、まだ使えていません。前述の「サスペンドとハイバネート - ArchWiki」によると、この現象は多数報告されているそうです。そして、そのデバッグのベストプラクティスがこの記事とのこと。長いので気が向いたら調べます。たぶん、Linux カーネルと ATI のドライバの相性（）が悪いのだと思います。他の参考記事・Arch Linux 快適デスクトップ環境の構築 2019 - Qiita・Linux におけるラップトップマシン向け電力管理スイートの紹介（Pm-utils, Laptop-mode-tools, Powertop, TLP） – 怠惰の形而上学初めからインストールされていた TLP があればサスペンド（スリープ）は問題なく管理されているそう。Laptop-mode-tools はもう古いそう。なのにサスペンドだけだと数時間しかバッテリーがもたないのは、推して知るべし...。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[私が１年かけて辿り着いた Looker の情報収集方法を紹介する]]></title>
        <id>https://developer.feedforce.jp/entry/2021/08/16/150000</id>
        <link href="https://developer.feedforce.jp/entry/2021/08/16/150000"/>
        <updated>2021-08-16T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんばんは、徳川家ｙ id:masutaka26 です。以前紹介したように、去年から Looker を使ったウェブ広告数値の可視化や BI *1 に取り組んでいます。LookML 開発者として LookML を書き始めて困ったのが、Looker の情報が少ないように見えたことです。LookML を含む Looker のドキュメントは充実しているのですが、それらを組み合わせた応用的なフロー情報が少なく感じました。ビジネスユーザー向けの情報も同様です。現在は網羅的、かつ集約した情報を取得できているので、その方法をご紹介します。「情報源」をリストアップする少し脱線...「情報源」の取得方法への課題今はどうなったか？「Looker Community」のフィードが存在した件「Looker の記事一覧 | DevelopersIO」のフィードを作った件まとめ追記「情報源」をリストアップする初めはこれらをたまに見に行ったり、Slack の /feed subscribe *2 で購読したりしてました。Looker Community公式フォーラム。英語で Question や Conversation が出来るLooker 日本語コミュニティフォーラム「Looker Community」の日本語版。Release Notes の日本語訳には本当に感謝 🙏「ニュースと告知」「ヘルプとサポート」「コラム」はそれぞれ Subscribe 出来る。メールで通知される  Looker 日本語コミュニティフォーラムLooker APAC Forum | Facebookリリース案内や事例紹介などLooker の記事一覧 | DevelopersIOご存知クラスメソッドさんのブログ。国内最多の記事量と投稿頻度Lookerの記事一覧 | ZennZenn にもそれなりの頻度で投稿されるLooker - QiitaQiita はもう少し頻度は落ちるかな#looker lang:ja - Twitter 検索以上の情報をふわっと取得できる。looker lang:ja や #looker だとノイズが多いのでこれに落ち着いた少し脱線...Looker Community には過去一度だけ質問しました。How do I dynamically switch view name in sql parameter of dimension? | Looker Community私は日本語サポートに頼ることが多い傾向です。最近はだいぶ減らせています。扱う情報を外に出せないので、外に出せるところまで昇華するのは難しいですね。🌀Looker の水野さんが日本語訳して下さっている、Looker のリリースノート *3 は、去年の 12 月から社内向けにこんな記事を書いて、Looker に徹底的に向き合うようにしています。Looker 21.12 のリリースノートを眺めてみた今まで書いた記事です。Looker のリリースノートを眺めてみたシリーズ「情報源」の取得方法への課題RSS/Atom（フィード）を配信していないサイトがほとんどで、見に行くのがかなり面倒でした。そのものズバリなフィードは Zenn と Qiita だけです。クラスメソッドさんは Looker タグのフィードが存在せず、当時は Twitter で捕捉してました。今はどうなったか？ほぼすべてを社内の Slack channel #news-looker に集約させることが出来ました。以下が実際に購読しているフィードです。https://community.looker.com/feed/buzzcapture「Looker Community」のフィード。後述するhttps://feed43.com/developersio-looker.xml「Looker の記事一覧 | DevelopersIO」のフィード。後述するhttps://zenn.dev/topics/looker/feed「Lookerの記事一覧 | Zenn」のフィードhttps://qiita.com/tags/looker/feed「Looker - Qiita」のフィードTwitter は IFTTT を使って、同 channel に POST しています。If New tweet from search #looker lang:ja -rtThen Post to channelChannel: #news-lookerMessage: @{{UserName}} : {{Text}} (via Twitter {{LinkToTweet}})「Looker APAC Forum | Facebook」は集約できませんでしたが、Twitter にも流れることがあるので、一旦考えないことにしました。「Looker Community」のフィードが存在した件Looker の水野さんに聞いたら、調べて教えて下さいました。🙏(1) 新しいトピックの投稿https://community.looker.com/feed/topics(2) 全ての新しい投稿（最初の投稿（タイトル＋ボディ）+ 全てのリプライ）https://community.looker.com/feed/buzzcaptureアナウンス記事です。LookerコミュニティのRSSフィード | Looker Community(1) と (2) は両方とも「Looker 日本語コミュニティフォーラム」の情報も流れてきます。今は (2) を購読しており、トラフィックはそれなりにあります。もちろんほぼ全て英語です。辛かったら (1) にすると良いと思います。https://community.looker.com/ の HTML には RSS/Atom 情報がないので、これらのフィードに気づける人は少ないと思います。Looker さんには是非お願いしたいところです。「Looker の記事一覧 | DevelopersIO」のフィードを作った件ないものは仕方がないので、Feed43 というサービスで作りました。出来たのが https://feed43.com/developersio-looker.xml です。どなたでも購読可能です。よろしければどうぞ。HTML をパースしているだけなので、HTML 構造が変わったら壊れることはあると思います。気づけたら直します。本当は https://dev.classmethod.jp/tags/looker/ のフィードがあれば良いのですけどね。今後に期待です。[Update] そのものズバリ https://dev.classmethod.jp/feed/?tag=looker を Twitterで教えて頂きました。ありがとうございます！まとめ私が１年かけて辿り着いた、Looker の情報取得方法をご紹介しました。これで Looker の情報は大量にインプット出来たので、今後はコミュニティにアウトプットしていきます。🔥※ ネタは少しあるけど、時間がない。(^^;皆さんにも参考になれば幸いです。他にもあれば @masutaka に教えて下さい！追記他にもあれば @masutaka に教えて下さい！記事にも登場して頂いた Looker の水野さん @tomoya_cs をフォローするとさらに捗ると思います。Lookerの情報収集本当に至難と思いますが、まとめていただきありがとうございます😭あとは私をフォローいただけると最新情報が入手しやすくなるかと（アウトプットがんばります🙇‍♂️） https://t.co/eDQz8A5VCC— tomoya | Looker CS (@tomoya_cs) 2021年8月19日  *1:Business Intelligence*2:Slack に RSS フィードを追加する | Slack*3:例: Looker 21.12 リリースノート | Looker Community]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chrome から Firefox に乗り換えたよ]]></title>
        <id>https://masutaka.net/chalow/2021-08-07-1.html</id>
        <link href="https://masutaka.net/chalow/2021-08-07-1.html"/>
        <updated>2021-08-07T14:59:59.000Z</updated>
        <summary type="html"><![CDATA[[2011-06-03-1] に Firefox から Chrome に乗り換えて以来です。ちょうど 10 年。iPhone から Android への乗り換え [2020-07-14-1] も 10 年だったし、飽きづらいマスタカも 10 年が周期みたいです。まあ、普通の人は乗り換えたことをいちいち記録しないし、もっとカジュアルにあっちこっち使う気はしてます。このブログの名前が「マスタカの ChangeLog メモ」なので記録せざるを得ないのです。もう大変。(>o 買えよ (^^;loading...loading...今回は乗り換えのハードルを低くするために個人 PC だけ乗り換えました。PC も速くなった [2021-05-06-1] ので、2019 年とは違います。会社の MacBook Pro はまだ（？）Chrome です。Firefox でまず戸惑うのが、Chrome のユーザー（プロファイル）切り替えに相当する機能が見当たらないことです。4 年前と変わらず、なぜか Multi-Account Containers を推してきますが、これはタブ単位でセッションを切り替える機能です。Firefox アカウントは切り替えられないので代わりにはなりません。代わりに "about:profiles" が使えます。アドレスバーにこれを入力して開くと、素朴な画面が開いてプロファイルの管理や起動が出来ます。"$ firefox -P" などと "-P" オプション付きで Firefox を開いても、似た機能を持つプロファイルマネージャーが起動します。公式ヘルプ: プロファイルマネージャーを使用して、Firefox のプロファイルを作成または削除する｜Firefox ヘルプ前回と同じように、私が使う数少ないアドオンの乗り換え記録も残しておきます。各タイトルは Chrome 拡張名です。1Passwordこれは普通にあります。X は気にしないでください。1Password X – パスワード保管庫Chrome 版は２つあって初心者を悩ませそうです。前者が X です。気にしないでください...。1Password – パスワード保管庫1Password 拡張機能 (パソコンのアプリが必要)ato-ichinenON にしている間、Google 検索を直近 1 年にするだけの Chrome 拡張です。技術系の調査は鮮度が大事なことがあるからです。よく考えたらブックマークレットで十分なので、こんなのを作りました。どうでも良いですが、名前は G1nen にしました。javascript:location.href=location.href+'&tbs=qdr:y'ついでに G2en というブックマークレットも作りました。これは Google の検索対象を英語サイトにします（情報源）。日本語の情報はノイズになることが多いので。だったら常に英語にしろよという意見は認めます。javascript:location.href=location.href+'&gl=us&hl=en&gws_rd=cr&pws=0'Create Link今開いているページや選択したリンクを、Markdown や任意のフォーマットでクリップボードにコピーする Chrome 拡張です。esa とか書いてるとよく使いますよね。Format Link に移行しました。若干上位互換風で、正規表現を使った置換も出来ます。Emoty絵文字を一覧から選んでクリップボードにコピー出来る Chrome 拡張です。フォーマットは :smile: などで、Unicode 絵文字ではありません。Emoji Cheatsheet に移行しました。こちらは Unicode 絵文字にも対応しています。Google Analytics オプトアウト アドオン (by Google)その名の通りの Chrome 拡張です。Chrome の機能を使って、自分のサイトや会社のサイトだけをオプトアウトしていました。宗教上の理由（）がないので、全サイトはオプトアウトしていません。良さげなアドオンが見つかっていません。情報求む。Google 翻訳その名の通りの Chrome 拡張です。選択範囲だけを翻訳したり、ページ全体を日本語に置き換えたり便利ですよね。To Google Translate に移行しました。https://translate.google.com/ が別タブで開いてしまうので大げさではあります。まあいいか...。Grammarly for Chromeスペルチェッカーサービス Grammarly の Chrome 拡張です。今年も気がついたら課金していたので、来年は課金しないようにしました。Grammarly for Firefox があります。ただ、Chrome と違って動作するサイトを絞れないので、フォームへの書き込みを邪魔することが多いです...。Keepa - Amazon Price TrackerAmazon の商品ページに価格変動グラフを挿入してくれる Chrome 拡張です。「安くなった。買い時だ。」「高いけど買っちゃおう。」などと判断の助けになります。← 結局買う奴Amazon Price Tracker - Keepa.com があります。LINELINE の Chrome アプリです。長めの文章が面倒な時に使ってました。Firefox 版はないみたいです。LINE をあまり使わないので問題なし。Save to PocketPocket の Chrome 拡張です。結局 Pocket に戻って来てしまいます。Firefox にはこの機能が組み込まれています。2015-04-21 にそんなニュースがありました。速報： MozillaがPocket（旧Read It Later）をFirefox本体に統合（追記あり） - Mozilla Fluxtwitter画像原寸ボタンtwitter.com や TweetDeck で小さくて見づらい画像を原寸サイズで、別タブにバババッと開く Chrome 拡張です。Twitter 原寸びゅーに移行しました。モーダルで開き、ショートカットキーでサイズを変えられるので、こちらの方が便利です。はてなブックマークご存知はてブの Chrome 拡張です。Hatena Bookmark があります。なぜか英語。新たに Firefox にインストールしたアドオンChrome では要らなかったが、Firefox でインストールしたアドオンを書いていきます。Copy Link Textリンクテキストをコピーするアドオンです。↑ だと "Copy Link Text" をコピーできます。信じられないことに Firefox ではこれが出来ないのです。4 年前のやつが残ってて、何をするアドオンか思い出すのに時間がかかりました...。Firefox で不要になったアドオン反対に、Chrome で使っていたが、Firefox で代わりは要らなかった Chrome 拡張を書いていきます。Close download barファイルをダウンロードすると現れるうざいフッターを Option-w や Alt-w で閉じられる Chrome 拡張です。地味だけど便利です。まとめ10 年ぶりに Firefox に戻ってきました。1 週間使っていますが、課題はありつつも、Firefox だと忘れてしまう程度には主張してきません。割と快適です。アドオンがエラーを起こすのはたまたまなんですかね？この記事を書いている間、Format Link が使えませんでした...。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[３月に無償提供となった Microsoft Power Automate Desktop を今さらながら使ってみた]]></title>
        <id>https://masutaka.net/chalow/2021-07-28-1.html</id>
        <link href="https://masutaka.net/chalow/2021-07-28-1.html"/>
        <updated>2021-07-28T14:59:59.000Z</updated>
        <summary type="html"><![CDATA[※ 会社の「インフラ共有会」用に書いた記事の転載です。Windows PC を買った [2021-05-06-1] ので、個人で試せることに気づいてちょろっと試したって経緯です。Microsoft、自社製RPAツールを全Windows 10ユーザーに無償提供　マウスクリックやキーボード入力をGUIで自動化：Microsoft Ignite 2021 - ITmedia NEWS・動作するのは目の前の Windows PC。エクセルとかのマクロをパワーアップしたものに近いかも・ スケジュール（時間を指定した自動実行）の機能は見つけられなかった。寝ている間に自動実行とかは出来ない気がした・Chrome の「要素を検証」みたいな感覚で、クリックやフォームの位置を指定できる・要素名を変更するなど細かい微調整は出来ないようなので、id 名等が少しでも変わるとフローが動かなくなるかも・アクションの種類は多彩で、頑張れば「LINEビジネスアカウントでログインして、レポートをローカルフォルダにダウンロードし、それを Google スプレッドシートにインポート」は出来ると思う・今起動しているブラウザにも接続できるようなので、２段階認証突破のハードルは多少低そうPower Automate と Power Automate Desktop との違いはイマイチ理解できていない。難しい...。・Power Automate・Power Automate Desktop ← 今回はこっち多彩なアクション※ フローの新規作成画面です。[多彩なアクション]フローを作った様子https://www.myfitnesspal.com/ja/account/login にログインして csv ファイルをダウンロードするフローを作ったが、MyFitnesspal は「CSV ファイルをダウンロードするためのリンクを含んだメールが届きます」だと知った。試しに作るには大きすぎるため心が折れた。(^^;[設定例]インストール方法や実行方法などの紹介動画うぃんどうずしょしんしゃなので、わかりやすいどうがをみたよ。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Firestore エミュレーターを使ったテスト同士の競合が起きないようにしていい感じにテストできるようにした話]]></title>
        <id>https://developer.feedforce.jp/entry/2021/07/07/103917</id>
        <link href="https://developer.feedforce.jp/entry/2021/07/07/103917"/>
        <updated>2021-07-07T01:39:17.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、エンジニアの id:len_prog です。私が所属している EC Booster チームでは、「カイゼンカード」機能の開発に Firebase を採用しています。しかし、Firebase を採用したことで苦労したことが全く無かったわけではありません。そこで、今回の記事では、いくつかあったハマりごとの中でも特に厄介だったものについて対策を書いていきます。Firestore Emulator のプロジェクト共有時のデータ競合Firebase Local Emulator Suite を使って Firestore に接続するテストを書いていた際に、調査の結果、これは、接続先プロジェクトがすべてのテストで同じになってしまっているのが原因ということが分かりました。この状態で同じドキュメントを書き換えるテストが並列で走ってしまった場合、実行タイミングによってはドキュメントが予期せぬ状態になってしまいます。このままでは役に立つテストが書けないと思い試行錯誤した結果、テストごとに違うプロジェクトの Firestore に接続することでそれぞれのテストが独立した状態で実行でき、結果としてデータ競合が防げることが分かりました。以下、サンプルアプリケーションを用いてこの方法について書いていきます。サンプルアプリケーションの概要今回は、サンプルとして簡易的な RPG を開発することを想定します。characters コレクションで管理されています。{  name: string;  level: number;  job: string;}また、このゲームでは以下の行動のみが可能と仮定します(これだけじゃゲームとして成り立たないと思いますが、簡単のためということでお許しください)キャラクターは、レベルアップすることができるキャラクターは、転職することができる転職すると、キャラクターのレベルが1に戻るなお、アプリケーション上においてキャラクターのレベルアップは、characterLevelUpUseCase、キャラクターの転職は characterJobChangeUseCase という関数を呼ぶことで行えることとします。ここからは、実際にこれら2つの関数のテストコードが競合する様子を見ていきます。データ競合発生時の構成characterJobChangeUseCase と characterLevelUpUseCase が my-game プロジェクトの Firestore を共有してしまっています。// functions/src/usecases/characterJobChangeUseCase.spec.tsimport * as admin from "firebase-admin";import { characterJobChangeUseCase } from "@/usecases/characterJobChangeUseCase";admin.initializeApp({  projectId: "my-game", // ここが問題});const charactersCollection = admin  .firestore()  .collection("characters");describe(characterJobChangeUseCase, () => {  const targetCharacterId = "target-character-id";  beforeEach(async () => {    await charactersCollection.doc(targetCharacterId).set({        name: "アルス",        level: 10,        job: "すっぴん";    });  });  afterEach(async () => {    await charactersCollection.doc(targetCharacterId).delete();  });  it("キャラクターが転職した場合、レベルが1に戻ること", async () => {    await characterJobChangeUseCase(targetCharacterId); // characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1に戻る    const jobChangedCharacter = (await charactersCollection.doc(targetCharacterId).get()).data();    expect(jobChangedCharacter.level).toBe(1); // 実行タイミング次第では、1になるはずが11になってしまう！  });});// functions/src/usecases/characterLevelUpUseCase.spec.tsimport * as admin from "firebase-admin";import { characterLevelUpUseCase } from "@/usecases/characterLevelUpUseCase";admin.initializeApp({  projectId: "my-game", // ここが問題});const charactersCollection = admin  .firestore()  .collection("characters");describe(characterLevelUpUseCase, () => {  const targetCharacterId = "target-character-id";  beforeEach(async () => {    await charactersCollection.doc(targetCharacterId).set({        name: "アルス",        level: 10,        job: "すっぴん";    });  });  afterEach(async () => {    await charactersCollection.doc(targetCharacterId).delete();  });  it("キャラクターがレベルアップした場合、レベルが1上がること", async () => {    await characterLevelUpUseCase(targetCharacterId); // characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1上がる    const grownCharacter = (await charactersCollection.doc(targetCharacterId).get()).data();    expect(grownCharacter.level).toBe(11); // 実行タイミング次第では、11になるはずが1に戻ってしまう！  });});見ての通り、両方のテストが my-game プロジェクトの Firestore の、ID: target-character-id のドキュメントを更新してしまっています。キャラクターが転職したのにレベルが1に戻らない、キャラクターがレベルアップしたはずなのになぜかレベル1に戻ってしまうなど予期せぬ状態になってしまい、テストが落ちてしまう可能性があります。この状態ではテストコードが信用できないので、テストごとに向き先プロジェクトを変えてこの問題を解決していきます。データ競合解決後の構成上図②③のようにテストごとに接続先プロジェクトを独立させることで、他のテストとの並列実行が原因のデータ競合を防ぐことができます。admin.initializeApp()の第一引数に他のテストと重複しないプロジェクトID を渡すようにします。// functions/src/usecases/characterJobChangeUseCase.spec.tsadmin.initializeApp({  projectId: "character-job-change-use-case-spec", //  図の②に対応});// functions/src/usecases/characterLevelUpUseCase.spec.tsadmin.initializeApp({  projectId: "character-level-up-use-case-spec", // 図の③に対応});変更後のコードの全体像は以下のようになります。// functions/src/usecases/characterJobChangeUseCase.spec.tsimport * as admin from "firebase-admin";import { characterJobChangeUseCase } from "@/usecases/characterJobChangeUseCase";admin.initializeApp({  projectId: "character-job-change-use-case-spec", //  図の②に対応});// ここから下は構成変更前のコードと同じconst charactersCollection = admin  .firestore()  .collection("characters");describe(characterJobChangeUseCase, () => {  const targetCharacterId = "target-character-id";  beforeEach(async () => {    await charactersCollection.doc(targetCharacterId).set({        name: "アルス",        level: 10,        job: "すっぴん";    });  });  afterEach(async () => {    await charactersCollection.doc(targetCharacterId).delete();  });  it("キャラクターが転職した場合、レベルが1に戻ること", async () => {    await characterJobChangeUseCase(targetCharacterId); // characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1に戻る    const jobChangedCharacter = (await charactersCollection.doc(targetCharacterId).get()).data();    expect(jobChangedCharacter.level).toBe(1); // 転職するとレベルが1に戻ることを検証できるようになった  });});// functions/src/usecases/characterLevelUpUseCase.spec.tsimport * as admin from "firebase-admin";import { characterLevelUpUseCase } from "@/usecases/characterLevelUpUseCase";admin.initializeApp({  projectId: "character-level-up-use-case-spec", // 図の③に対応});// ここから下は構成変更前のコードと同じconst charactersCollection = admin  .firestore()  .collection("characters");describe(characterLevelUpUseCase, () => {  const targetCharacterId = "target-character-id";  beforeEach(async () => {    await charactersCollection.doc(targetCharacterId).set({        name: "アルス",        level: 10,        job: "すっぴん";    });  });  afterEach(async () => {    await charactersCollection.doc(targetCharacterId).delete();  });  it("キャラクターがレベルアップした場合、レベルが1上がること", async () => {    await characterLevelUpUseCase(targetCharacterId); // characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1上がる    const grownCharacter = (await charactersCollection.doc(targetCharacterId).get()).data();    expect(grownCharacter.level).toBe(11); // レベルアップした場合にレベルが1上がることを検証できるようになった  });});このようにテストごとに向き先プロジェクトを変えることで、それぞれのテストで担保したいことをちゃんと担保できるようになります。ちょっと微妙な点上記の方法でテストごとに独立した環境の Firestore を操作できるようになり、データ競合を防げるようになりました。しかし、この方法にはひとつだけ微妙な点があります。競合解決後の構成図を再掲します。上図①の接続先は、$ firebase use で指定したプロジェクトか、$ firebase emulators:start に --projectを渡した場合にはそのプロジェクトになり、そのほかの方法で変えることは今のところできないようです。そのため、プロジェクトをテストごとに分けた場合、上図②③のテスト中にテスト自体は動くものの、Firebase Emulator の UI からデータの内容を見ることはできなくなります。一応、接続先を $ firebase use で指定しているものに切り替えるようコードを書き換えたりすればデバッグはできますが、いちいち書き換えの手間が生じるので若干面倒です。また、これは Firebase Enulator の UI で立ち上がっているすべてのプロジェクトの Firestore を見られるようになれば解決する問題ではあり、実際に firebase/firebase-tools-ui リポジトリに issue も立っていますが、すぐに対応が終わりそうには見えない状況なので、しばらくは不便な状況が続くことが予想されます。所感Firebase は便利ですが、当然ながら全くハマらずに開発できる銀の弾丸ではないですね。今後も日々の開発で得た Firebase や GCP 周りの TIPS を書いていけたらと思っておりますので、よろしくお願いいたします 🙏]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Apple Silicon Mac で複数 Terraform バージョンを管理するために asdf-terraform-build を作った]]></title>
        <id>https://blog.tsub.me/post/create-asdf-terraform-build/</id>
        <link href="https://blog.tsub.me/post/create-asdf-terraform-build/"/>
        <updated>2021-06-19T08:41:36.000Z</updated>
    </entry>
    <entry>
        <title type="html"><![CDATA[一年間の育休から復帰しました]]></title>
        <id>https://developer.feedforce.jp/entry/2021/06/16/120000</id>
        <link href="https://developer.feedforce.jp/entry/2021/06/16/120000"/>
        <updated>2021-06-16T03:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、インフラエンジニアの id:tsub511 です。実は去年の 1 月から 1 年間育休を取っており、今年の 1 月から復帰していました。復帰してから記事を書くのが遅くなってしまいましたが、社内勉強会で話したスライドや育休から復帰してみてどうだったかをまとめてみました。社内勉強会で話した今年の 3 月頃に社内の技術勉強会 FFTT で発表しました。(技術勉強会という立て付けですが、技術に限らず本人が話したいことを話して良い場です)なぜ一年間の育休を取得したのか、育児の知見などがスライド内に書いてありますので気になる方はご覧ください！ちなみに当時のスライド内ではねんねトレーニング (ネントレ) をやっていると書いてありますが、実は現在はやってません。理由はネントレの効果が見られなくなったためです。今年の 4 月くらいから寝付きが悪くなり、胸や背中をトントンしないと寝てくれなくなってしまいました。それ以降夜通し寝てくれないことも増えてしまい、再度ネントレにチャレンジしましたが夜泣きは改善されなかったため、夜泣きの原因は寝かしつけ方法と直接関係ないのでは？と思いネントレをやめました。ちょうどその頃から奥歯が生え始めていたので、それが原因だったのではないかとは思っています。ここ最近は歯の痛みが落ち着いたのか、夜泣きが少し減ってきたような気がします。と思ってたらまた夜泣きが復活しました... 😭やっぱりセルフねんねしてくれてた頃と比べて寝かしつけにかかる時間は増えましたが...寝かしつけで悩んでいる方にとって少しでも参考になれば幸いです。(自分はめっちゃ悩んだ)育休から復帰した感想スライド内でも触れていましたが、やはり一番感じたことは一年間というそれなりに長い期間にも関わらず、普段有給を取るのとそこまで大きく変わらないくらいの感覚でした。もちろん育休を取るという話をしてからチーム内での調整は行いましたが、取得する障壁は特にありませんでした。チームメンバーも育休は取得する前提で育休期間中はどう進めていくのか、という話にフォーカスしている印象でした。そして、一年間の育休を終えて復帰する際にもすんなり業務に戻ることができました。人事やチームメンバーが自分の育休中の変化を事前に記事にまとめてくれていたこともあり、キャッチアップも大体 2 週間くらいで完了しました。復帰がスムーズにいった理由の 1 つにチーム体制や使用技術に大きな変化がなかったことも大きいと思います。以下の記事でも軽く触れていますので良ければご覧ください。働き方の変化育休取得前後 (子どもが生まれる前後) での大きな変化の 1 つとして、働き方がかなり変わりました。子どもが生まれる前は 10:00 ~ 19:00 で働いており、モチベーションや体力がある日は少し長めに働いたり、反対に効率の悪い日は早めに退勤したりという働き方をしていました。子どもが生まれた後は保育園や子どもの就寝時間の都合で 8:00 ~ 17:00 で働くこととなり、保育園の送り迎えや夕飯の支度などがあるので長めに働くということができなくなりました。また、プライベートの時間で技術的な勉強をやることもほとんどなくなってしまいました。やる時間が全くないわけではないのですが、ゲームなどでリフレッシュしないと育児疲れが厳しいのと、まとまった時間が取りづらいのが理由です。子どもが起きている間はなかなか PC を広げて作業しづらいですし、寝ている間も夜泣きなどでいつ泣くか分からないので集中して作業ができないです。とはいえプライベートで開発する時間を取れなくても個人的にはそこまでストレスになっていなくて、今はそういう時期と割り切っています。また仕事で直接使えるような技術の検証であれば、チームの計画に入れて業務時間内で進められるのでなんとかなっています。終わりに育休から復帰してまだ半年程度なので育児もまだまだこれからという感じですが、仕事との両立を引き続き頑張っていきたいと思います 💪ちなみに自分が育休から復帰した前後で他のエンジニアも育休を取得していました。よければこちらの記事もぜひご覧ください！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[『ここがつらいよ普段使いのLinux』という発表をした]]></title>
        <id>https://developer.feedforce.jp/entry/2021/06/11/180000</id>
        <link href="https://developer.feedforce.jp/entry/2021/06/11/180000"/>
        <updated>2021-06-11T09:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:masutaka26 です。いよいよ明後日は RIZIN.28 ですね！東京ドームで MMA（総合格闘技）のイベントが行われるのは、約 17 年半ぶりだそうです（Wikipedia 調べ）。ドキが胸胸します。本日、週次の社内勉強会 FFTT で『ここがつらいよ普段使いのLinux』という発表をしました。タイトルは違いますが、気にしないで下さい。Mac が重い時に IME 切り替えが一瞬遅れて、例えば「feedforce」とタイプする時に「ふぇえ...」になる現象に悩まされていました。そこで約 10 年ぶりに Windows PC を購入して、同じく約 10 年ぶりに Linux を普段使いし始めました。数々の諸問題が発生しましたが、無事全部解決（？）したお話です。キーボードショートカットがつらいタッチパッドがつらい指紋認証出来なくてつらいたまにスリープから復帰しなくてつらい（一番つらい）ちょっとした画像編集に GIMP を使うのはつらいみんなも Mac を捨てて Linux を使うといいと思うよ！それでは！だれでもできるLinuxセットアップ―無料(ただ)なのに頼れるOSリヌクス作者:鈴木 哲哉オーエス出版Amazon]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[【2021年夏】半期に1度の Engineer’s Principles Award 受賞者を紹介します]]></title>
        <id>https://developer.feedforce.jp/entry/2021/06/11/164253</id>
        <link href="https://developer.feedforce.jp/entry/2021/06/11/164253"/>
        <updated>2021-06-11T07:42:53.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。人事の今岡と申します。2021年もあっという間に6月ですね。フィードフォースでは先日オンライン納会が開催され、半期に一度の「Engineer’s Principles Award 2021 Summer」の受賞者が発表されました。今回アワードを受賞した開発メンバーと表彰内容をご紹介します。前回の表彰者紹介はコチラdeveloper.feedforce.jpEngineer’s Principles Award とはEngineer’s Principles とは、フィードフォースの開発メンバー向けに現場が主体となって設定した、5つの行動指針です。半期に一度、開発メンバー同士で投票を行い、行動指針の項目ごとに最も体現しているメンバーが選ばれ表彰されます。Engineer’s Principles についてはこちらmedia.feedforce.jp受賞者紹介※表彰コメントは本来社内向けのものであるため一部変更させていただいています。受賞者によって各種アカウントを載せています。🏆「Stay Humble; 常に謙虚であるべし」受賞者@len_prog さん表彰コメント： Len (@len_prog) ,  Blog@katsunn さん表彰コメント：  のもち(@nomo_017)🏆「Be Positive & Proactive; 常に肯定的・主体的であるべし」受賞者@sukechannnn さん表彰コメント：  sukechannnn (@sukechannnn) ,  sukechannnn@daido1976 さん表彰コメント：  Daido Shota (@daido1976) ,    daido1976🏆「Be Prepared; 常に来たるべき機会に備えるべし」受賞者@daido1976 さん表彰コメント：  Daido Shota (@daido1976) ,    daido1976@namikingsoft さん表彰コメント： namikingsoft🏆「Share All; 己の知見、試行、失敗、遍く共有すべし」受賞者@masutaka さん表彰コメント： Takashi Masuda (@masutaka) ,  masutaka ,  Blog@kogai さん表彰コメント： 茶碗 (@iamchawan) , kogai ,  Blog🏆「Just Do It; 全力でやりきるべし」受賞者@namikingsoftさん表彰コメント： namikingsoft周囲の賞賛・承認を共有するよい機会に以上、延べ9名の受賞者でした。表彰コメントは、開発メンバー同士の投票時に自由記述できるコメントがもとになっているので、周囲からの賞賛・承認の声を全社で共有できるよい機会となっています。前回に引き続き連続受賞しているメンバーもいますが、投票コメントには毎回違ったエピソードが集まっており、日ごろから継続的に実践をしているからこそ周りのエンジニアの目に留まるのだと感じました。受賞者のみなさん、おめでとうございました！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dependabot の Terraform 1.0 対応が完了した件]]></title>
        <id>https://masutaka.net/chalow/2021-06-10-1.html</id>
        <link href="https://masutaka.net/chalow/2021-06-10-1.html"/>
        <updated>2021-06-10T14:59:59.000Z</updated>
        <summary type="html"><![CDATA[[2021-05-27-1] のつづき。この間に 1.0 がリリースされてましたね。先ほど、Lockfile (.terraform.lock.hcl) の対応が完了したそうです。https://github.com/dependabot/dependabot-core/issues/1176#issuecomment-858490407これで Dependabot での terraform 対応が完了しました。・terraform version は自動更新できない・provider version を自動更新できる。その際、.terraform.lock.hcl も更新されるはず・以前の Dependabot で対応されていた module version はどうなんだろう？使ってないので分からないdependabot.yml のドキュメントはこちら。circleci-tfupdate-orb はそろそろアーカイブしたいな。でも Dependabot だと terraform version は更新出来ないんだよね...。追記(2021-06-11):公式でもアナウンスされてました。Dependabot now supports Terraform 1.0｜GitHub Changelog]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[プランニングの難しさを乗り越えて...スクラム開発が良い感じになった話]]></title>
        <id>https://developer.feedforce.jp/entry/2021/05/31/104813</id>
        <link href="https://developer.feedforce.jp/entry/2021/05/31/104813"/>
        <updated>2021-05-31T01:48:13.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。フィードフォースの EC Booster チームで開発（主にプロダクトオーナー）をしている @sukechannnn です。元々ずっとバックエンドエンジニアでしたが、最近プロダクトオーナーをやるようになりました（理由はのちほど！）。昨年のアドベントカレンダーで 半年モブプロしたらチームが大きく成長した話 というブログを書いたのですが、2021年3月から モブプロを取り入れたスクラム開発 をしています。それに伴って、"モブプロ" と "個人タスク⇢レビュー" の両軸で開発するようになりました（先日リリースしたカイゼンカード はスクラムで開発しました）。今は良い感じに回っていますが、そうなるまでに色々と試行錯誤したので、そこで得た学びをお伝えできればと思います。全員リモートワークで開発するなら、モブプロを取り入れたスクラムはおすすめです！モブプロの良さと難しさそうだ、スクラムしよう！プランニングが終わらない問題原因は「issue が散らかっていること」だったissue をグルーピング、優先順位はそれぞれでまとめモブプロの良さと難しさモブプロ中心の開発を初めた当初は、以下の利点を感じていました。ドメイン知識の共有がしやすいコンテキストの共有がしやすい（"何をどう作るか" という議論もしやすい）レビューが要らないリモートワークでもさみしくない（だいじ）しばらくモブプロを続ける中で、開発メンバー全員がドメイン知識やフロント〜バックエンド全体の技術的な知識を共有している状態になりました。なので、なにか悩みがあってモブプロで共有すると「わかる〜」となるし、何より単純に仲良くなったと思います（ﾖｼｯ!!）。一方で、だんだんと モブプロだけ の開発が窮屈になってきました。知識の共有が進んできて "全員でやらなくても良くない？" というタスクが増えてきた個人でじっくり考えた方が良いタスクもあるのが分かった（新しい技術の調査、設計の見直しなど）これはチームが成長したことで出てきた嬉しい悩みなのですが、とはいえ完全にモブプロを辞めるのも上述したメリットを失いそうで怖い...。チーム全員で「今後どう開発していこう？」というのを話し合い、モブプロを取り入れたスクラム開発 を試してみることにしました。そうだ、スクラムしよう！スクラム開発をしようと思ったのは、ストーリーポイント*1で見積もって ベロシティ*2を測りたい という別の目的もありました。モブプロで開発していると新機能のメイン開発は着実に進んでいくのですが、それ以外の細かいタスク（主に保守系）が見積もりづらい状況で、空いた時間にやるという形になってしまっていました（それ用に時間は設けていましたが）。モブプロ以外の個人タスクを計画的にやりたい、見積もりもしっかりやりたい、ということで、スクラムを導入することで、モブプロと個人開発のいいとこ取り をしようと考えました。新機能開発などのコンテキストの共有が重要なタスクは引き続きモブプロでやるストーリーポイントで見積もるそれ以外は個人タスクとして各自で進められるように、プランニングでしっかり整理する個人タスクもストーリーポイントで見積もる全てのタスクをストーリーポイントで見積もるのでベロシティが測れるようになる振り返りで見積もりの精度を上げられるめっちゃ良さそう...そう思っていざやってみたところ、１つ大きな壁にぶち当たってしまいました。プランニングが終わらない問題エッセンシャルスクラムにもある通り、１週間のプランニングに２時間以上かけるべきではありません。僕らは「１スプリント=１週間」で回しているため、２時間の予定で始めたプランニングですが、これが終わらない...。最初から何回かは４時間以上かかり、全員ヘトヘトになってしまいました。モブプロはプランニングが簡単です。全員やることが同じなので、基本的にタスクが直列で繋がっていきます。そのため「今スプリントはここから⇢ここまで」という感じで Sprint Backlog 的なものを決めることができました。しかし、スクラムの見積もりはもっと横断的なものです。単純に、今取り組んでいるものだけ見れば良いのではなく、これから取り組むものをたくさんある issue から選ぶ必要があります。そう、この たくさんある issue の中から今スプリントにやるタスクを選ぶこと に時間がかかってしまうのです。以前にもスクラム開発を試したことがあるのですが、その時もこれが原因でプランニングがとても大変でした。気にするトピックが多すぎてだんだん何について議論してるか分からなくなり、空中戦になってしまうんですよね...。その原因は、主に以下の２つでした。バックログの整理/管理に責任を持つ人（プロダクトオーナー*3）がいなかったissue の数と種類が多く、バックログリファインメント*4をしても整理しきれなかったプロダクトオーナー不在の問題は、元々それっぽいことをしていた僕が、改めてプロダクトオーナーやりますと手を上げ、バックログ管理の責任を持つことになりました。それでも、バックログリファインメントが上手く行かない問題は残っていました。リファインメントの概念は理解していて、しっかり時間も取っていたのに、いざプランニングをすると色々な issue を見すぎて伸びてしまう...。過去に何度も直面したこの問題に、改めて取り組むことにしました。原因は「issue が散らかっていること」だった僕たちが開発している EC Booster は、ショッピング広告の自動運用やデータフィードの更新など、様々なジョブが裏で動いています。そのため、運用作業が日々発生し、運用の中で見つかる例外ケースやバグの修正が多々あります。また、フロントエンドとバックエンドを全員が開発するため、１つのリポジトリで管理していることもあり、色々な種類の issue が１つのレーンに入り乱れてしまっていました。そのため、優先順位を付けるのも難しく、また「次スプリントで何をどこまでやるか？」を判断するのが難しくなってしまっていました。プロダクトバックログを整理しなければ、というのは分かっているのですが、スクラムに関する本やブログには整理の方法は書いてありません。どうやって整理したら分かりやすくなるかな...と考えていたところ、同僚が共有してくれた以下の記事が参考になりました。エンジニア歴17年の俺が、事業系の開発タスクをバンバン投げてくる非エンジニアに、保守の必要性を死ぬほど分かりやすく説明する。この記事の中で「issueには "種類" がある」と言っていて、issue の種類別に整理された図が載っていました。これだ...！issue をグルーピング、優先順位はそれぞれで上記の記事を参考に、issue を 新機能開発、バグ修正/運用改善、ライブラリーアップデート に分けて、それぞれのレーンで優先順位を付けるようにしました。issue をグルーピング、優先順位はそれぞれでissue の種類が同じなので、優先順位を付けるのは簡単です。さらに、スプリントバックログに入れるタスクを 新機能開発：運用系 = ６：４ の割合にする、という決めを作りました。さらに、何回かスプリントを回してベロシティも見えてきました。ここまで情報が揃うと 次のスプリントで何をやるか決める基準 ができてきます。そもそもの「次のプランニングでどの issue について話すか？」というのも、それぞれのレーンで優先順位が高い issue を６：４のバランスとベロシティを参考に選べるようになりました。プランニングの前にプロダクトオーナーが（開発チームと協力しながら）当たりを付けておくことで、プランニングで話すトピックを事前に共有できるようになり、開発メンバーそれぞれが事前に頭を整理しておくこともできるようになりました。これにより、プランニングがかなりスムーズに進むようになったので、いよいよスクラムが回り始めました。新機能開発はモブプロの同期的な開発で、それ以外のタスクは個人タスク⇢レビューという非同期な開発で進められるようになり、デリバリーの最大化を目指しつつ、個人の稼働率も上げられるようになりました。GitHub Project を使ってタスク管理してる様子...横に長いんですが、情報が整理されてる方が優先順位を付けやすいまとめissue をグルーピングしそれぞれで優先順位を付けたことで、プランニングが時間内に収まるようになっただけでなく、プランニングで話すトピックを絞ったことでより深い議論をすることができるようになりました。今は「モブプロを取り入れたスクラム」がとても良い感じに回っています！↓ EC Booster チームでの「スプリントの回し方」資料を公開しているので、気になった方はぜひ見てみてください！（もっとこうしたら良いよ！という助言などあれば頂けると嬉しいです！）こんな感じ開発している EC Booster ですが、ただ今 バックエンド（Ruby, Rails）が得意なエンジニアを猛烈に必要としています！！！もしちょっっっとでも興味があれば、 僕とお話しましょう！ 以下から気軽に応募してください！https://open.talentio.com/1/c/feedforce/requisitions/detail/19785最後まで読んでいただき、ありがとうございました！*1:ストーリーポイント：プロダクトバックログ（タスク）を見積もるためにチームが使う単位で、前回の見積もりに対する相対評価を用いる*2:ベロシティ：スプリントの期間でチームが届けることができる見積もり（ストーリーポイント）の合計のこと*3:プロダクトオーナー：プロダクトバックログの管理をする人で、優先順位を付けることに責任を持つ（１人の人間が務める、委員会ではない）*4:バックログリファインメント：プランニングの前にプロダクトバックログを見直し、プランニング可能な状態にしておくこと]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[暑がりの私にはマスクチェーンがとても合ってる]]></title>
        <id>https://masutaka.net/chalow/2021-05-30-1.html</id>
        <link href="https://masutaka.net/chalow/2021-05-30-1.html"/>
        <updated>2021-05-30T14:59:59.000Z</updated>
        <summary type="html"><![CDATA[去年の秋くらいからマスクチェーン（ストラップ）を使っています。私はとても暑がりなのと、マスクの着用感が好きではないので、かなり重宝しています。どのくらい暑がりかと言うと、中学生の時は暑くない水泳部に入ったり、毎日スーツを着る会社には絶対に入社しなかったり、今の会社が上場した時も暑かったので（スーツが必要な）東証には行かなかったり、まあそんな感じです。外に出た時にスッと外せるのがとても良いです。店で...]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dependabot の Terraform 0.15 対応が進んでいる件]]></title>
        <id>https://masutaka.net/chalow/2021-05-27-1.html</id>
        <link href="https://masutaka.net/chalow/2021-05-27-1.html"/>
        <updated>2021-05-27T14:59:59.000Z</updated>
        <summary type="html"><![CDATA[最近この Issue が活発になってきました。Terraform 0.15 support · Issue #1176 · dependabot/dependabot-core実はプロバイダーバージョンのアップデートだけならもう使えます。・プロバイダーバージョンのアップデートはもう動く・↑の後に必要な .terraform.lock.hcl の更新は実装中とのこと。現在は手動で "$ terraform init -upgrade" が必要・terraform バージョンのアップデートはロードマップに含まれていない↓プライベートリポジトリで動いている様子。[dependabot terraform1][dependabot terraform2]dependabot.yml のドキュメントはここにあるので、試してみるのも良いかもしれません。↓このブログが置いてあるリポジトリの .github/dependabot.yml です。daily はやり過ぎなのであとで減らそう。version: 2updates: - package-ecosystem: "terraform" directory: "/terraform/aws" schedule: interval: "daily" time: "12:00" timezone: "Asia/Tokyo" assignees: - "masutaka" open-pull-requests-limit: 10 - package-ecosystem: "terraform" directory: "/terraform/heroku" schedule: interval: "daily" time: "12:00" timezone: "Asia/Tokyo" assignees: - "masutaka" open-pull-requests-limit: 10今までの経緯とかTerraform 0.11 まで、Dependabot はモジュールのアップデートだけ対応していました。https://dependabot.com/docs/config-file/ でその痕跡を確認できます。※ Dependabot は GitHub に統合されたので、https://dependabot.com はもう古いです。ですが、Terraform 0.12 で HCL2 が導入されてから、全く動かなくなりました。前述の Issue も要望の嵐。@minamijoyo が tfupdate を作ってくれたので、私もその CircleCI Orb を作ってみるなど。※ 現在この Orb は terraform 本体とプロバイダーバージョンのアップデートのみ対応しています。terraform.lock.hcl の更新には対応していないので、手動で $ terraform init -upgrade が必要です。https://github.com/minamijoyo/tfupdate-circleci-example のサンプルコードは対応しているみたいです。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[エンジニアキャリアパスをアップデートしました]]></title>
        <id>https://developer.feedforce.jp/entry/career_path_revised_2021</id>
        <link href="https://developer.feedforce.jp/entry/career_path_revised_2021"/>
        <updated>2021-05-24T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、meihong です。株式会社フィードフォースでは定期評価ではなく本人の希望するタイミングで評価を行う制度を導入しています。具体的には、各等級ごとに満たすべき基準・条件、またはスキルがあらかじめ提示されており、それを満たしていれば次の等級に進める制度になります。media.feedforce.jpこの基準やスキルを私たちはキャリアパスと呼んでいますが、今回、エンジニアのキャリアパスをアップデートしましたのでご紹介したいと思います。なぜキャリアパスをアップデートしたのかもともとのキャリアパスは導入当初に設計されたものをベースに、マネージャやエンジニア、新規事業向けエンジニアといった個々人の志向に応じて細分化されていました。これはこれでよくできたものだったのですが、しばらく運用している中でいくつかの課題点を感じるようになってきました。例えば、志向ごとに分かれすぎていて、志向を横断した動きが想定しづらくなった。独り立ちと判断される等級であるメンバーとその一つ上のシニアの境界に「見えない高い壁」が存在するようになった。シニア以上の等級になるとチームや会社を牽引することを求められ、技術をそれ以上深掘りすることに対して会社がどう考えているのかが見えづらくなった。といったところです。特にキャリアパス全体として、職種問わず等級が上がれば上がるほどチームや会社への影響力が求められる設計になっています。もちろんエンジニアも全体への影響力は持つべきなのですが、その持ち方は他の職種と異なり、技術力の広さ、深さといった持ち方もあるのではないかと考えるようになりました。ここで、個人的にはプロフェッショナルとしてのスキルは体積であり、その底面積はスキルの幅広さだと考えています。極端に底面積が狭いのはさすがに現時点では厳しいとは思いますが、底面積がそれなりである代わりに高さ(= 深さ)がある底面積が広い (= 引き出しが多い) 反面高さはそこまででもないの両者は体積という意味では同じはずです。この両者が共存できる余地が欲しいと考えていました。そんな中、弊社デザイナーのキャリアパスがアップデートされました。その中でも目を引いたのは、必須スキルと専門スキルという考え方です。必須スキルはデザイナーとして必ず持っていて欲しいスキルである一方、専門スキルは本人の志向、特性に応じてピックアップできるというもので、大学の専攻を思い出す建て付けでした。これをパクるこれにインスパイアされて、エンジニアのキャリアパスもアップデートすることにしました。どのように更新したのか結果から先にお伝えしておくと、大まかに以下のような方向性に改訂しました。志向ごとのキャリアパスは止めた。旧来の「志向」を専門スキルに分解し、専門スキルの組み合わせで個々人の志向・特性を表現できるようにした。等級が上がれば上がるほど満たすべき専門スキルの最低数が増えるようにした。その結果として、例えばバックエンドエンジニアに特化フルスタックエンジニアフルスタックな知識をベースに事業の 0 → 1 フェイズに参画できるエンジニアカスタマーサクセスエンジニアアジャイルコーチといった、実際に社内に存在している各エンジニアの志向や得意なポイントを表現できるようになりました。産みの苦しみここに至るまでには色々な葛藤がありました。社内の esa にキャリアパスを更新したいと宣言はしたものの、社内のエンジニア個々人の顔を思い浮かべつつ何を専門スキルとして設定するかを考えると想像以上に難しい問題だということに気付きました。必須スキルと専門スキルそもそも必須スキルと専門スキルとは何か、そこの定義から考えることにしました。必須スキルとは文字通り、全てのエンジニアが共通に要求されるスキルセットのことです。どちらかというと「バックエンド」「フロントエンド」といった用語で定義されるスキルセットというよりも「フィードフォースに所属するエンジニアとしての振る舞い方」ではないでしょうか。そう考えながら改訂前のキャリアパスを改めて眺めていると、改訂前のキャリアパスはその振る舞いを定義していることに気付きました。その結果、改訂前のキャリアパスが必須スキルのベースとなりました。そうです、キャリアパスの改訂によって、より要求水準が上がったとも言えます。一方、専門スキルは、本人の得意分野、志向、特性を定義するものです。その志向・方向性で貢献するのであれば、各等級ごとにどの水準の成果を出すべきか。それを定義するものが専門スキルになります。専門スキルとはどうあるべきか本人の志向を定義するものが専門スキルと説明しましたが、例えばカスタマーサクセスエンジニアやエンジニアリングマネージャといった職種にしてもエンジニアの延長である以上はエンジニアとしての「共通言語」を身につけているべきです。その「共通言語」とは、例えば設計力であったり、フロントエンドやバックエンドのスキルが該当します。こういった知識を前提として例えば事業開発であったりチームビルディングを行うべきで、これらの知識がなければエンジニアとの「共通言語」を持っていないと判断せざるを得ません。一方で、「フロントエンド力」と「バックエンド力」が同じくらい強いエンジニアというのは SSR エンジニアで、そうそう市場には存在しません。そこで、フルスタックとはいえどこかの分野に軸足を置くことができる制度というのも必須に感じました。ただ、ここの軸足とはあくまでも「フロントエンド」「バックエンド」「インフラ」といった区分けで、エンジニアとしてコードを書き続ける選択をするのであれば、フロントエンド/バックエンド/インフラといった区分に関係なく設計力・実装力が担保されているべきでしょう。17 の専門スキルここのバランス感が非常に難しい点でしたが、これを元に 17 の専門スキルを定義しました。ただし、17 の専門スキルは完全に独立しているわけではなく、以下 6 つは本人の志向を定義するものとして、必ずどれか一つが必須選択としました。バックエンドフロントエンドデータベース基盤カスタマーサクセス組織支援さらに、上記のうち以下 4 つを選択した場合は「実装・設計」と呼ばれるスキルが必須となります。バックエンドフロントエンドデータベース基盤これにより、コードを書き続けるのであればただコードを書くだけでなく、実装力・設計力が要求される建て付けを実現しました。また、詳細は省きますが、さらにいくつかの例外を設置することで、「全ての分野で等しく強い SSR なフルスタックエンジニア」が求められないようにしています。様々なエッジケースを考慮したせいでちょっと複雑になった感の否めない新しいキャリアパスですが、以前のものと比べるとその分より柔軟なものになったと思います。今回は敢えて詳細を省きましたが、ご興味をお持ちいただけたら是非カジュアル面談でねっちょりとご説明します！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ノートパソコンクーラーを買い替えた]]></title>
        <id>https://masutaka.net/chalow/2021-05-23-1.html</id>
        <link href="https://masutaka.net/chalow/2021-05-23-1.html"/>
        <updated>2021-05-23T14:59:59.000Z</updated>
        <summary type="html"><![CDATA[2 年使っていた冷却パッドが寿命っぽいので、エレコムのノートパソコンクーラーを買いました。心配していた冷却効果は申し分なく、ひとまずホッとしています。元々こちらの冷却パッドを使っていました。ゲーミング PC 向けみたいです。この動画お気に入りです。つい何度も見てしまう中毒性があります。少々デカイのが難点でしたが、冷却効果は申し分ありませんでした。ただ 2 年も使うとホコリが溜まってきます。分解は無理そう...]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[広告の複数媒体に対するCPA最小化・ROAS最大化となる予算配分を計算しよう]]></title>
        <id>https://developer.feedforce.jp/entry/2021/05/13/093842</id>
        <link href="https://developer.feedforce.jp/entry/2021/05/13/093842"/>
        <updated>2021-05-17T00:38:42.000Z</updated>
        <summary type="html"><![CDATA[こんにちは　機械学習エンジニアの八百俊哉です。今回は複数媒体へ広告を出稿する際に、多くの方が悩まれるであろう「各媒体への予算配分」に関して有効な配分手法を紹介します。今回の記事で登場する広告用語媒体・・・広告の配信先や配信手法ROAS・・・広告経由で発生した売り上げを広告費用で割った値(広告の費用対効果)CPA・・・1件のコンバージョン(目標)を獲得するのにかかった広告コスト広告運用者が抱える課題とは？1つの媒体のみで運用している場合は別ですが、複数の媒体で広告配信を行っている場合は、どの媒体に対していくら予算を割り振れば良いのかわからない場合があると思います。過去の実績を元に成果が良い媒体に対して、多く予算を割り振れば良いことは理解しているものの、「どれくらい」「どの媒体から」予算を割り振れば良いのかは経験則や簡単な分析で決めている方も多いのではないでしょうか？今回はこれらの課題を解決するために、数学的に根拠のある予算配分方法について紹介しようと思います。まず今回の手法を紹介するにあたり、例題がある方が話が進めやすいので以下の広告運用者さんを例に考えます。広告運用者○○さん現在A,B,Cの３媒体で広告配信を行っています。全体のROASを高めるために予算配分を見直したいと考えています。３媒体での合計予算は3万円です。では、実際にどのようにして最適な予算を求めるのか見ていきましょう。過去の実績から各媒体の実績をシミュレーションします最初に過去の実績から各媒体での予算とROASの傾向を、式で表現します。ここでROASを最大化するということは、限られた予算の中で売り上げを最大化すると言い換えることができるので、今回は  を2次回帰で近似します。今回の例だと媒体A,B,Cに対してそれぞれ近似式が用意できるので以下のように表現できます。(各媒体の予算をとします)ラグランジュの未定乗数法を用いて最適な予算配分を見つけるラグランジュの未定乗数法とは？ラグランジュの未定乗数法とは、束縛条件のもとで最適化を行うための数学的な方法である。いくつかの変数に対して、いくつかの関数の値を固定するという束縛条件のもとで、別のある1つの関数の極値を求めるja.wikipedia.org少し小難しく聞こえますが、今回の例題に当てはめて考えてみます。いくつかの変数に対して(各媒体の予算)いくつかの関数の値を固定する束縛条件(3媒体の総予算は3万円)別のある関数の極値を求める(3媒体の売り上げが最大となるポイントを求める)ラグランジュの未定乗数法とは、上のような条件を満たす予算を見つけてくれる手法です。ラグランジュの未定乗数法では、媒体A,B,Cのそれぞれの に対して近似式が二階微分可能である必要があるため、今回の例では2次回帰で近似を行いました。また今回は、3媒体の総予算(束縛条件)が広告によって全て使用されるという仮説のもとで計算しています。予算を全て使わない場合は、計算が複雑になってしまうので今回は紹介しません。実際にどのように計算するのか検証しますまず初めに束縛条件 を定義します。今回の束縛条件は、それぞれの予算を足し合わせたものが30000円になるということですので、以下のように書けます。ここで式(1)を変換し、とおきます。また、今回最大にしたい3媒体の総売り上げをと置きます。ここで未定乗数とを用いてラグランジュ関数を作ります。それぞれの変数で偏微分すると以下のようになります。これら4変数の4元連立方程式を説くと、予算30000円で総売り上げが最大になる予算配分が求まります。今回は、ROASを最大化するための方法を紹介しましたがCPAを最小化する場合は2次回帰式を求める際にとしてcvを最大化するようにラグランジュを適応することで求めることができます。また今回の例では3媒体までの予算配分を計算しましたが、媒体数を増やしても計算速度が極端に遅くなることがないところが今回の手法の良いところです。この手法の課題点ここまで「ネット広告の複数媒体に対するCPA最小化・ROAS最大化となる予算配分」を紹介しましたが、この手法には2つほど課題があります。まず一つ目が、最適予算にマイナスの結果が得られる可能性があるということです。売り上げを最大化しようとするあまり、もともとROASが低い媒体に対しては予算を割り振らずにマイナスの予算を割り振り、そのほかのROASが高い媒体により多くの予算を割り振ろうとしてしまうことが確認できています。次に、媒体の周期性や外部要因を一切考慮していないということです。広告は少なからず外部要因によって成果が左右されますが、この手法では過去の実績のみを用いて最適予算を割り振るので外部要因は一切考慮されていないということに注意が必要です。まとめいかがだったでしょうか。今回は、ラグランジュの未定乗数法を用いて複数媒体への予算配分方法を紹介しました。流石に手作業では計算できないので私はpythonで上記の流れを実装しています。ラグランジュの未定乗数法は、理系の方は大学の数学の講義で習っていたかもしれないです。私も大学の時に習いましたが、当時は何に使うのか一切わかりませんでした。社会人になって学生の時に学んだことが活用できると、学んだ甲斐があったと感じることができて良いです。最後まで読んでいただきありがとうございます。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Docusaurus v2 を日本語化]]></title>
        <id>https://zenn.dev/tmd45/articles/docusaurus-config-lang-ja</id>
        <link href="https://zenn.dev/tmd45/articles/docusaurus-config-lang-ja"/>
        <updated>2021-05-01T09:30:35.000Z</updated>
        <summary type="html"><![CDATA[Docusaurus 2.0.0-alpha.75 で確認しています。多言語対応で記事を作成し、ヘッダーから言語選択する UI も標準で用意されている Docusaurus ですが、セットアップした状態（多言語対応化していない状態）では英語 lang="en" がデフォルトの言語となっています。                      <!DOCTYPE html><html lang="en" data-theme="light" dir="ltr" data-react-helmet="lang,dir">            日本...]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ESP8266 で温湿度を測って、AWS IoT Core + Amplify でグラフにしてみる]]></title>
        <id>https://developer.feedforce.jp/entry/2021/04/28/110000</id>
        <link href="https://developer.feedforce.jp/entry/2021/04/28/110000"/>
        <updated>2021-04-28T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、ソーシャルPLUS チームの id:mashabow です。社内勉強会 FFTT のネタとして、ESP8266 で Wi-Fi 温湿度センサーを作り、グラフ化してみました。普段はフロントエンドの開発をしているんですが、ちょっと別のことをやってみようかと思いまして。電子工作に入門して、AWS IoT Core とか AWS Amplify を使ってみた記録です。グラフ：https://main.dt7p4lkfvt2c8.amplifyapp.com/リポジトリ：https://github.com/mashabow/uchino-sensors 詳しくはスライドをご覧ください。Q&A勉強会で出た質問です。Q. Wi-Fi のパスワードってどうしてるんですか？A. ESP8266 へ書き込むプログラムの中に、そのまま埋め込んでいます。Q. どれくらいのサイズのプログラムを書き込めるのかなA. 今回は小さいプログラムなのであまり気にしていませんでしたが、ユーザーが使えるのは 50 KB 程度らしいです。Q. すると timestamp は測定時点のものではないのか。数十ミリ秒くらいはズレているのかなA. IoT Core のルールエンジンに届いた時刻なので、正確ではないですね。10分間隔の測定で、かつセンサーの精度もそんなに無いので、まぁいいかなと。送信するメッセージに timestamp を入れるようにすればいい話ではあります（ESP8266 を書き変えるのが面倒だった 💦）。Q. AWS IoT Core とかってお高いんですか？A. 個人で使う規模なら激安ですね 👀Q. 温度が急上昇してたら 119 しとけば良いですか？A. よろしくおねがいします 🚒感想IoT っぽいものに初めて手を出してみたんですが、おもしろいですね。一番最初は用語も概念も調べ方もわからず、ネット上で情報収集しても「???」だったんですが、本を読んだら頭に入ってくるようになりました。グラフも単なるグラフだろと思っていたんですが、自宅の実データをいざ視覚化してみると、自分だけにしか見えない様子ががいろいろ見えてきます。「あ、これ夕食を準備してたときだ」とか、ついついじっくり眺めてしまいますね。今回はセンサーだけでしたが、アクチュエーターに指示を出せば、さらにおもしろいものができそうです。リンク集スライドで触れたページや、実装にあたって参考にしたページです。1. Wi-Fi 温湿度センサーを作るWi-Fiモジュール ESP-WROOM-02 DIP化キット: 秋月電子通商-電子部品・ネット通販超特急Web接続! ESPマイコン・プログラム全集 | 国野 亘 | Amazon組み立てや実装の参考にした本[PDF] IoT実習キット説明書・パーツリスト上記書籍の回路部分。基本的にはこれに倣ったHiLetgo 3個セット DHT11温度センサー | Amazon今回使った温湿度センサーArduinoで遊ぶページArduino 言語の参考にA professional collaborative platform for embedded development · PlatformIO使い慣れたエディタで Arduino の開発ができる。補完や依存管理ができて便利乾電池で本当に1年間動作するIoTセンサ ESP8266 - ボクにもわかる電子工作のブログ2. 測定結果を受け取ってためるAmbient – IoTデータ可視化サービスArduino ESP8266で温度・湿度を測定し、Ambientに送ってグラフ化する – AmbientMachinistAWS IoT Core 初級ハンズオン :: AWS IoT Core 初級 ハンズオンdebsahu/ESP-MQTT-AWS-IoT-Core: Arduino examples of connecting ESP8266/ESP32 to AWS IOT CoreTI DSP ソフトウェア設計のファームロジックス | ESP8266（Arduino環境）で AWS IoT（MQTT over TLS）にアクセスする最初 MQTT で接続できなくて悩んでいたが、NTP で時刻を合わせたら解決した3. 測定結果をグラフにするGetting started - Amplify Docs[Feedback] API.graphql return signature is hard to use in TypeScript · Issue #6369 · aws-amplify/amplify-js素の状態だと TS サポートがいまいちな件React-ApexChart - A React Chart wrapper for ApexCharts.jsAWS AppSync を使用して、外部データベースの更新をサブスクライバーにリアルタイムで通知する]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ふりかえりカンファレンスのスタッフをやりました！]]></title>
        <id>https://developer.feedforce.jp/entry/2021/04/19/141153</id>
        <link href="https://developer.feedforce.jp/entry/2021/04/19/141153"/>
        <updated>2021-04-19T05:11:53.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:pokotyamu です！最近は、モンハンライズにハマっています！ハンマー担いでブンブンしてます！4月16日(土)に行われた「ふりかえりカンファレンス」のスタッフをやりました！今回はそこでの学びや感じたことを社内勉強会で発表したので、スライドとコメントをまとめます。FFTT 発表資料勉強会の感想コメント なんのためにふりかえりやってるの？明日の自分やチームを1歩でも楽しくなるためにやってほしいですね！連続したサイクルの中にふりかえりを組み込むことで、安全に転んで、次の1歩を早く出せるようになると思います！オンラインセミナーは当日参加が多い結構人数集まったようなので準備とか大変そう場所の制約がなくなったのが非常に大きいですね〜！国内・国外関係なく、どこでもいけるのが本当に便利！振り返り手法ってあんなにたくさんあるのだなぁ振り返りの手法も多いようなのでどのタイミングで何を使うのが適切かを考えるの難しそう会社やチームよって向き不向きがありそうだけど、選ぶには知らないといけないので専門的な人がいる意味がよく分かるそーなんですよね。次の Action を決めたい時や、関係構築したい時など、用途に合わせてやるのがいいと思います！もちろん KPT も素晴らしい手法なので、たまに気分を変えてみるみたいな感じでどうぞ！振り返りとの因果関係を感じられる強い人やチームの実例を見たら、もう少しイメージが付くのかなと思っていたが、21卒の方の日報をザッピングしていたら、振り返りが役に立った、楽しいという風に書かれていた今年は特に楽しいにフォーカスしてふりかえりをしているのもあると思います！オンラインカンファレンスのスタッフの話って結構レアな気がするので興味深かった楽しいのでぜひぜひ！感想私は、初めてカンファレンスのスタッフをやらせてもらったのですが、非常に楽しかったです！他の人の感想やブログレポートを見るのも、それそれ！その言葉待ってた！という感じでいつもの一般参加とは違う感覚で聞くことができました！今回のスタッフを経験したことで、「楽しくふりかえる」の意味を体で感じることができたと思います。もちろん当日の発表もどれも素晴らしくてそれも含みで楽しかったところではありましたが ☺️気軽に試す、実験してみるを最近のふりかえりでも挑戦中です。また、オンラインカンファレンスということもあり、夜の2次会が3時ぐらいまで盛り上がっていたのも楽しかったポイントでした。新しいつながりも持てたので、社内の知見をどんどん外に発信して自分の魅力を高めていければと思います。改めて、スタッフに誘っていただいた @viva_tweet_x さんに改めて感謝です！ありがとうございました！これからもよろしくおねがいします！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ 夜間光データから土地価格を予測 コンペの参加記録]]></title>
        <id>https://developer.feedforce.jp/entry/2021/04/13/174808</id>
        <link href="https://developer.feedforce.jp/entry/2021/04/13/174808"/>
        <updated>2021-04-13T08:48:08.000Z</updated>
        <summary type="html"><![CDATA[こんにちは株式会社フィードフォース2020年入社の機械学習エンジニア八百　俊哉と申します。今回は、solafuneで開催された「夜間光データから土地価格を予測」という機械学習コンペに参加したので工夫した点や反省点などを紹介します。コンペ参加の目標設定としては、「賞金獲得！！（4位以内）」を設定していましたが、36位/201人中と目標達成できませんでした。残念な結果に終わってしまいましたが、多くのことを学ぶことができました。参加経緯私は、2020年10月から2021年2月ごろまで顧客の課題解決のために機械学習を応用する方法を学ぶためにAI Questというイベントに参加していました。そのイベントをきっかけに私は精度の高いモデルや良い特徴量を作成することに興味を持ちました。そこでより多くのコンペに参加することで精度を上げるためのノウハウを身に付けたいと思ったことが今回のコンペに参加したきっかけです。また、今回参加したコンペは与えられている特徴量が4つしかないので、初心者が参加しやすいコンペだったということも魅力的なポイントでした。課題と与えられているデータ課題としては、「夜間光データを元に土地価格を予測するアルゴリズムを開発する」というものです。使用可能なデータとしては、以下のものが与えられました。地域ごとのデータ・・・地域固有のID年代・・・1992~2013年まで土地の平均価格（目的変数）・・・1992~2013年まで夜間光量の平均値・・・0~63までのレンジでその地域の平均光量夜間光量の合計値・・・その地域の合計光量全体構成今回最終submitとして選択したモデルの全体構成は以下です。前処理に関して集約的特徴量について集約的特徴量の作成にあたってはmasato8823 (@mst_8823) | TwitterさんがBaseLineとして公開されていた以下のものを使用しました。zenn.dev作成した特徴量としては以下です。          面積    夜間光量の合計値/夜間光量の平均値を行い面積を算出した    PlaceID,Yearごとの統計情報   PlaceID,Yearをキーとして平均光量、合計光量、面積のmin,max,median,mean,std,max-min,q75-q25を算出した   PlaceID をキーにしたグループ内差分  平均光量、合計光量の年ごとの差分を算出したPlaceID をキーにしたグループ内シフト  平均光量、合計光量の年ごとの値をシフトしたピボットテーブルを用いた特徴量index=PlaceID,columns=Yearとして平均光量、合計光量、面積のピボットテーブルを作成し、PCAで次元削減したものを算出したPlaceIDをキーにしたグループ内相関係数PlaceIDごとにデータを集約しYearと平均光量、合計光量、面積との相関係数を算出した平均光量が63であった回数平均光量の最大値が63であることから平均光量が63である数を追加したArea特徴量について先ほど集約的特徴量についてで面積の求め方について書きました。面積=合計光量/平均光量で算出しています。ここで求められる土地の面積は、年が変化しようと変化しないと思われますが、実際のデータを確認すると年が変化すると面積も変化していました。そこで合計光量/平均光量より算出された面積をPlaceIDをキーとして平均を取ったものを新たな面積としました。新たな面積が求まると 新たな合計光量 =  平均光量×新たな面積,新たな平均面積 = 合計光量/新たな面積 が求まります。これらより求まる新たな合計光量、新たな平均光量、新たな面積を元々の合計光量、平均光量、面積と置き換えて集約的特徴量の作成を行いました。gplearnについて上で紹介した集約的特徴量とArea特徴量のそれぞれに対してgplearnというライブラリを用いて新たな特徴量を作成しました。このライブラリは遺伝的アルゴリズムにより目的変数をよく表している変数を作成してくれるものです。このライブラリを用いて新しい特徴量を10個,25個,50個作成し、元々の集約的特徴量、Area特徴量と組み合わせてそれぞれに対して予測を行いました。gplearnでの特徴量作成については以下のサイトが参考になります。qiita.comモデル構築に関してモデルの構築としてはgroup k fold(fold=5)でStackingのモデルを採用しました。1層目はrandom forest,lgb,multi regression,catboost,xgboostに加えてAutoMLのAutogluonを採用しました。Autogluonは以下のようにデータを渡すだけで、11個のモデルを検証し最後に出力結果を重量平均で作成してくれます。'label',                                problem_type='regression',                                 eval_metric='root_mean_squared_error', # 評価指標)X_train['label'] = y_trainX_test['label'] = y_testpredictor.fit(            train_data=X_train,            tuning_data=X_test, # これを渡さない場合はランダムスプリット            time_limit=None, # おおよその時間制限を設けられる)そして2層目は1層目でも採用しているAutogluonで出力を作成しました。感想・反省点Public Scoreの時点では6位と賞金獲得の可能性が十分にありましたが、Private Scoreでは36位と大幅にshake downしてしまいました。今回目標達成できなかった理由としては以下の2つが考えられます。1 CVの値とPublic ScoreからPrivate Scoreについて考えられなかった1つ目の要因としては、Public Scoreが下がることのみを考えてモデルの改善・特徴量の作成を行っていたということです。その時CVの値とPublic Scoreをどこかに記録しておけばよかったのですが、どこにも保存せずPublic Scoreが下がることが最も良いことであると捉えていました。実際は、CVが下がったモデル・特徴量においてPublic Scoreも同じように下がることが望ましく、その記録を取っておくべきでした。実際これまで提出していたファイルの中にPrivate Scoreが0.48774というものがあり、このファイルを最終提出としておけば3位に入ることができていました。しっかりとPrivate Scoreに効いているであろう提出ファイルが選べるようにCVとPublic Scoreに着目できるようにならないといけないと感じました。2 gplearnを行う位置が悪かった2つめは、group k foldを行う前にgplearnを行ったことによって、validationの目的変数が確認できる状態でgplearnが特徴量作成を行ってたことです。これは本来見ることができないデータを確認しながらデータ生成を行っていることになるので過学習を引き起こす可能性がありました。　　あるべき姿としては、group k foldでtrainをtrain,validationに分割した後にtrainのみのデータを用いてgplearnをfitさせるべきだったと思います。次回コンペでは今回のコンペを通じて集約的特徴量の作成方法、Stackingの実装方法、gplearnの実行位置、CVとPublic Scoreの関係性の重要度について学ぶことができました。テーブルコンペ において有効な手法を多く学ぶことができたので、次回参加するコンペでは賞金獲得を目標に頑張ります！！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[カスタマーサクセスエンジニアのお仕事]]></title>
        <id>https://developer.feedforce.jp/entry/2021/04/09/120453</id>
        <link href="https://developer.feedforce.jp/entry/2021/04/09/120453"/>
        <updated>2021-04-09T03:04:53.000Z</updated>
        <summary type="html"><![CDATA[ごきげんようございます、ソーシャルPLUS チームの id:tmd45 です。今回も社内の技術勉強会（通称 FFTT）で話した内容で書きたいと思います。毎回一貫性のないネタで（しかもあんまり Tech じゃない話を）話してますが、今回も自分のキャリアについての話をしました。2020/01/10 FFTT#381 認証認可情報の追い方のはなし2020/08/28 FFTT#407 認知（cognition）のはなし過去にこのブログで書いたこの記事↓の続編ということになります。2018/12/05 社内では開発チームマネージャーを名乗っています知識として役に立つ内容というよりは、自分に合った仕事ができると楽しいよね、とか、エンジニアでもこういう仕事あるよ、とか、ざっくりとした自分語りになります。では、よろしくお願いします！2020年10月からカスタマーサクセスエンジニアになりましたいきなり本題ですが、はい、なっておりました。対外的に自分のポジションを名乗ることがないので、ただの自称なんですが、最近だと元同僚の id:a-know さんが転職先で正式に（？）名乗ってらっしゃいます。自分ではあまり上手く言語化できていなかったんですが、a-know さんの記事に書かれているポジション名への思いを見て「あー！まさにそれ！」って思いました（あぁ！先に公に言われてしまった！😩というのも思いました。笑）。他にも Customer Support & Senior Developer とも自称しています。これは単純にいまやっていることを並べた感じの肩書きです。具体的に何をやってるのか１つは、メールでのカスタマーサポートを担当しています。現在 ソーシャルPLUS のカスタマーサポートへお問い合わせいただくと、8 〜 9 割がた私がお返事いたします。ソーシャルPLUS のカスタマーサポートは、メールでご連絡いただくことで対応する、受動的かつ突発的なタスクです。主に企業がソーシャルPLUS を導入するための技術的な事前調査や、導入開発に関する質問が多いです。この導入フェーズを乗り越えていただかないと、本来のソーシャルPLUS というサービスの価値を感じていただくことができないため、とても大事なサポートだと認識しています。もともと技術的なお問い合わせも多く、お客様側のシステムや利用方法を加味してご案内する必要がありました。またソーシャルログインプロバイダ側の契約や仕様についても把握している必要があります。技術面まで捉えてお返事するとなると、エンジニア以外では返答に窮することもありました。もちろん開発チームと連携して回答するというのは以前からやっていましたが、双方のコミュニケーションコストや、開発チームが本来の開発タスクから突発的な質問（しかもちょっと複雑）に対応するための脳のスイッチングコストなど気になることがありました。ビジネスチームにはもっとたくさんの企業に利用いただけるようなまさに "ビジネス的" な施策に時間を使ってほしかったし、開発チームには開発に集中する時間をできるだけ作りたいと思っていました。カスタマーサポートを担当しはじめたきっかけ先日、我らがチームリーダー、Tech Lead の佐藤の社内インタビュー記事が公開されました。media.feedforce.jp去年の夏頃、当時まだ開発チームリーダーだった私と 1on1 をしていた佐藤から、エンジニアとしてのステップアップとして、ソーシャルPLUS の開発チームリーダーとしてもっとコミットしていきたい、という話をもらいました。自分としては断る理由もなく、めっちゃいいじゃん！むしろぜひお願いします！という感じでした。しかし、そうなると 7 年もソーシャルPLUS にいる自分はどこに向かえばいいのか？という悩みがはっきりと表面化してきました。ポジションや肩書きに執着はないので、リーダーでもなんでもない、ただの（と言ったらいまチームにいるメンバーに失礼ですけど）バックエンドエンジニアに戻るという道もありました。でも本当にそんな言い方は失礼だなと感じるくらい、いま開発に携わってくれているメンバーは良い開発者たちです。もっと人手があれば、とは常々思っていますが（笑）私がそこに戻るだけで、私の経験や力を充分活かせるのかしらん？それでプロダクトはもっといい方向に進めるのかしらん？と。前の記事でも書きましたが、以前から自分はビジネス側と開発側の間に入って緩衝材をやったり、落ち穂拾いをしたりしていました。しかし自分が受ける側の 1on1 で「ここから先どうなりたいですか？」と問われて答えに詰まるような期間を過ごしていました。そんなとき、ビジネス側のいちメンバーから「（a-know さんみたいな）カスタマーサクセスにコミットするエンジニアというのが合ってるんじゃないか」と言ってもらう機会がありました。単純なんですが、それで「あ、そっか」と目からウロコがぽろり。いままで「担当する仕事じゃないから」と（はっきりそう言うわけじゃないですが）どこか伝言ゲームのようになっていた、問い合わせなどの仕事を、私自身が担当すれば手っ取り早いじゃないかと。それだけじゃなく、これまでのサポートメンバーが兼任で担当していたがゆえに、どうしてもサポート側の知見などを残す手間をかけづらかったところを、アウトプットに慣れた自分のようなエンジニアがやれば一石二鳥。同じように手が回らなかった仕組み化や、プロダクトへのフィードバックもやってやろうじゃないか、と。チーム内で互いに助け合う文化ができあがっていたので、自分ひとりが何でも知っていなければならないという過度なプレッシャーはなく、とはいえ自分がこのポジションを担当すればなんかいろいろすっきり組み立てられるのでは？というひらめきがありました。結局のところ、肩書きが変わっただけとも言えます。肩書きに執着しないと言いながら、行動が肩書きに囚われてしまっていたなぁと。チーム内外に「ポジションを変えます！エンジニアとしてカスタマーサクセスやります！」と大々的に宣言したことで、やれることの幅が広がりました。広がったというか、しっかりフォーカスする部分を切り替えることができた。そういう意味での肩書きって大切なんだなと思いました。やってみてどうだったか自分にはめちゃくちゃ合ってたなと、毎日楽しく働いてます。同僚にも、この仕事にシフトしてからの玉田さんめちゃくちゃ充実して楽しそうに見える、と言われました（笑）。これまでなんとなくもやもやしてたものが、自分の手で解決できていく気持ちよさを感じています。やってみて、いまの仕事にフィットしている自分の強みはこんなところだと思ってます。ソーシャルPLUS を 7 年もやってる（それなりの業務知識量）開発者なのでシステムの調査にスムーズにとりかかれるこう見えてお客様とのコミュニケーションは昔とった杵柄があるドキュメンテーションや情報整理にこだわりがある細かいことに気がつくほう前職（もう 7 年も前なわけですが）では頻度は高くないものの、企業のお客様のところへ営業さんといっしょにお話しにいくことがあったり、お客様と日々メールでやりとりしていたりという経験があって、いま役に立っています。SIer という立場で設計開発するときの心配事なんかも少しは知っているので、それも役に立ってるかもしれないなと思う場面があります。結果として、先に書いたような課題感はどうなったかというと、まだ数値にして改善されたことを確認するなどはできていないのですが技術的な調査・質問に以前より素早く答えられる形になったコミュニケーションコストや待ち時間などの削減ひとりの人間が対応できるので調査とお返事の文章作成を同時にできるなどナレッジを文章化して残すようになったものによって esa 記事にしたり GitHub Issue にしたりしています兼業していた他職種のひとが以前より本業に集中できるようになった確認依頼や私でもわからないことを質問して答えてもらったりというのは今もありますが、それでも以前より中断されない時間が作りやすくなったのではないかと思いますしかもこれがプロダクトマネージャだったりするので影響がでかい 😎などなど、いい効果が出てると感じます。ひとりで対応できてしまう＝属人的になってしまう、という問題も重々承知しているので、そうならないようにナレッジ化したり、やりとりを見える化したりというのは大事にしています。強みとして挙げたような "私だからできる仕事" というのも同じくらい大事にしています。他になにやってるのカスタマーサポートは私の仕事のうちの１つです。それ以外には Senior Developer として、プロダクトの新しい機能の検証や、機能として組み込むわけではないけどお客様が必要とするような仕様や仕組みを検証するなどしています。こちらはとくにビジネスメンバーと以前よりたくさん話す機会が増えました。セールスが普段お客様からどのような要望を伺うのか、マーケティングがどういった訴求をしているのか、プロダクトマネージャーはどんな未来を描いているのか。そういうものから何ができそうか、具体的な方法を考えて、開発チームにシステム化してもらうような仕事です。そこまで明確に切り分けて丸投げしているわけではないですし、開発チーム側からも活発に意見や案が出る環境でもあります。UI 面やお客様に "見せて" 提供する機能はデザイナーが具体案を詰めていくことが多いです。Senior Developer の仕事は、以前からやっていた落ち穂拾いに近く、それを「私の仕事としてコミットしますね！」と明言して取り組んでるようなイメージです。さらにこれからCustomer Support & Senior Developer をやることで、プロダクトの内情とお客様の困りごとの両方を見ることができています。そこから、これからもっと良く出来そうだなと確度高く考えられるものが増えました。いまのチームはまさに、この下の図のような感じでメンバーが良い関係で仕事していると思います。私が "望まない" チーム私が "望む" チーム複数の色を跨ぐような仕事ができているのが、自分の強みであり、プロダクトの役にも立てているなという実感もあります。溜まったナレッジや分析結果をもとにさらに良い改善、良い進化ができるように引き続きやっていきたいと思います！おわりにというわけで、他チームに自分の近況報告をするような発表となりました。これを読んで「おっ じゃあエンジニアにカスタマサポートやらせりゃいいんだ」と思われることは無いと思うんですが、そういう話ではないのでご留意ください。あくまでも私にはこの仕事が合っていて、ちょうどプロダクトのなかでそういう仕事があるといいなという状態に合致できたという話です。自分はとても恵まれていると思います。日々感謝感謝です。そんな話をすると、それだけじゃないよ、頑張ってるよ、と言ってくれる同僚も居ます。とてもありがたいことです。感謝の気持ちを力に変えて、ソーシャルPLUS をさらによいプロダクトにしていきたいと思いますので、今後ともよろしくお願いいたします。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[エンジニア向けミートアップを開催します！]]></title>
        <id>https://developer.feedforce.jp/entry/2021/03/15/113230</id>
        <link href="https://developer.feedforce.jp/entry/2021/03/15/113230"/>
        <updated>2021-03-15T02:32:30.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。人事チームからエンジニアミートアップについてお知らせです。3月26日（金）19：00から、エンジニア向けのミートアップを開催することになりました！選考とは関係ないので、純粋に「どんなエンジニアがいるかみてみたい」「会社の雰囲気を知りたい」という方もぜひご参加いただけたら嬉しいです。初回のLT登壇者は @daido1976です！media.feedforce.jpご参加お待ちしています！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Docker BuildKit の --secret フラグでイメージビルド時の秘匿情報を環境変数経由で渡せるようになっていた]]></title>
        <id>https://developer.feedforce.jp/entry/2021/03/15/102530</id>
        <link href="https://developer.feedforce.jp/entry/2021/03/15/102530"/>
        <updated>2021-03-15T01:25:30.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、id:daido1976 です。入社してもうすぐ 3 年が経ちます。Docker BuildKit の --secret フラグについて、公式ドキュメントでの説明や関連する日本語記事はすでにいくつかあるのですが、2020/12/08 にリリースされた Docker Engine 20.10.0 で「秘匿情報を環境変数経由で渡せるようになっていたこと」への言及が見当たらなかったので書かせていただきます。*1（個人的にはとても嬉しいアップデートでした…！）リリースノートの該当箇所は以下です。See. https://docs.docker.com/engine/release-notes/#20100buildkit: secrets: allow providing secrets with env moby/moby#41234 docker/cli#2656 moby/buildkit#1534Support --secret id=foo,env=MY_ENV as an alternative for storing a secret value to a file.--secret id=GIT_AUTH_TOKEN will load env if it exists and the file does not.要点Docker Engine 20.10.0 までは --secret id=my_secret,src=my_secret.txt のようにファイル経由でしか秘匿情報を渡せなかった20.10.0 からは --secret id=my_env,env=MY_ENV のように環境変数経由で秘匿情報を渡せるようになった上記の --secret id=my_env,env=MY_ENV にはショートハンドが用意されており --secret id=MY_ENV とも書ける（この場合 Dockerfile 側でも id は MY_ENV になる）使い方基本的な使い方は 公式ドキュメント に書いてあるのでそちらに説明を譲ります。以下は主に上記の 要点 と公式ドキュメントに記載のない仕様を説明するための最小のサンプルコードです。こんな Dockerfile があったとして、# syntax = docker/dockerfile:1.2FROM alpine# 1. docker build 時に `id` を明示的に指定する場合（例: `--secret id=my_env1,env=MY_ENV1` ）RUN --mount=type=secret,id=my_env1 cat /run/secrets/my_env1# 2. docker build 時に `id` を明示的に指定しない場合（例: `--secret id=MY_ENV2`） RUN --mount=type=secret,id=MY_ENV2 cat /run/secrets/MY_ENV2# 3. `dst` を指定した場合（`dst` を指定しないとデフォルトで `/run/secrets{id}` に秘匿情報が入ります）RUN --mount=type=secret,id=MY_ENV3,dst=/foobar cat /foobar# 補足: key 名の指定は実はいろいろできるらしい# https://github.com/moby/buildkit/blob/v0.8.2/frontend/dockerfile/instructions/commands_runmount.go#L154-L216以下のように docker build すると、環境変数経由で秘匿情報を渡すことができます。$ export MY_ENV1='my_env1!!!' && export MY_ENV2='my_env2!!!' && export MY_ENV3='my_env3!!!'# 複数の秘匿情報を渡すには --secret フラグを複数使えば良い$ DOCKER_BUILDKIT=1 docker build --secret id=my_env1,env=MY_ENV1 --secret id=MY_ENV2 --secret id=MY_ENV3 --no-cache --progress=plain .# ...#8 [2/4] RUN --mount=type=secret,id=my_env1 cat /run/secrets/my_env1#8 sha256:d32e4958843414fb006b5dbe7c259112f1bd481d17d84098c3d84f499793cb3b#8 0.252 my_env1!!!#8 DONE 0.3s#9 [3/4] RUN --mount=type=secret,id=MY_ENV2 cat /run/secrets/MY_ENV2#9 sha256:4cc22ee3d7f39180f4f4f8167deaa481e852b2fc577c89a234fe88b9852952ba#9 0.301 my_env2!!!#9 DONE 0.4s#10 [4/4] RUN --mount=type=secret,id=MY_ENV3,dst=/foobar cat /foobar#10 sha256:91f97f55e633c8ed675126abefa015110265a7fbd40e4aeaf88204dcb912cb26#10 0.283 my_env3!!!#10 DONE 0.3s# ...Docker Compose で --secret フラグが使えないことへの対策余談ですが、Docker Compose には BuildKit が統合されているものの、2021年3月現在 --secret フラグは利用できません。（対応中らしき PR はある）私のチームではワークアラウンドとして、開発環境でも docker-compose build ではなく docker build を使う方法を取りました。Docker Compose では docker-compose.yml で指定しているイメージと同名のイメージがローカルにあればそちらを使ってくれるので、以下のような docker-compose.yml を書いて、version: '3'services:  app:    build: .    image: my-app    # ...$ DOCKER_BUILDKIT=1 docker build -t my-app --secret id=MY_ENV .上記のように docker build で同名のイメージを事前に作っておいて docker-compose から使う、という流れになります。参考記事Docker 18.09 新機能 (イメージビルド&セキュリティ) | by Akihiro Suda | nttlabs | MediumDocker Engine 18.09 から使える Build-time secrets を試してみた | はったりエンジニアの備忘録*1:BuildKit や --secret フラグってそもそも何？という方は最下部の参考記事に詳しくまとまっておりますのでぜひご覧ください。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Attentionを用いてGoogle 無料リスティングの「拡張リスティングの不承認」に挑んだ話]]></title>
        <id>https://developer.feedforce.jp/entry/2021/03/11/101244</id>
        <link href="https://developer.feedforce.jp/entry/2021/03/11/101244"/>
        <updated>2021-03-11T01:12:44.000Z</updated>
        <summary type="html"><![CDATA[こんにちは株式会社フィードフォース2020年入社の機械学習エンジニア八百俊哉@Feedforce (@feed_yao) | Twitterと申します。最近はロードバイク にはまっており、ロードバイク購入後一ヶ月で一日100km走行に成功しました。今回、Google無料リスティングで不承認アカウントが発生する要因を調査する分析を行いました。Google 無料リスティングとは？なぜ今回分析が必要とされたのか？結果と考察Self-Attentionを採用した理由実装手順使用データのフォーマット必要ライブラリのインストール・インポートデータの前処理学習評価・出力まとめGoogle 無料リスティングとは？2020年10月にGoogleから公開されたGoogleショッピングタブに無料で商品掲載ができる「無料リスティング」のことです。Google 検索にサイトがインデックス登録されても料金が発生しないのと同様に、EC事業者は無料で利用可能になりました。Google 無料リスティングについての詳細は以下のサイトが参考になります。lab.ecbooster.jpなぜ今回分析が必要とされたのか？無料リスティングでは自社製品を無料でGoogleに掲載できます。しかしながら、課題として一部商品掲載が不承認となるケースが見受けられました。不承認となってしまうと自社商品の掲載ができていない状況が発生しています。不承認となる理由としては、「Googleが定める基準に対して、登録している商品データの属性数が足りない、内容が仕様に沿っていない場合、商品データの品質が低いため不承認になり、Googleの検索結果に表示させることができません。」とされています。これらを定量的に分析することで不承認となる理由を見つけ出す試みが始まりました。そのため今回の分析の目的は、商品の属性情報（title,description）から承認・不承認の要因を見つけ出し、不承認の商品を承認へと改善するための施策を考案することです。結果と考察今回の目的である「商品の属性情報（title,description）から承認・不承認の要因を見つけ出し、不承認の商品を承認へと改善するための施策を考案する」は、達成できませんでした。目的が達成できなかった理由として考えられる要因は、承認・不承認は商品のtitle,descriptionだけでは判断されていないということです。商品ごとのtitle,descriptionのみで承認・不承認が判断されているのではなく、商品データ全体またはアカウント全体のデータを総合的に見て、判断されている可能性が高いということがわかりました。承認・不承認予測のAccuracyとしては5割〜６割ほどで、承認・不承認を予測するという点でも低い精度となってしまいました。Self-Attentionを採用した理由今回はSelf-Attentionという手法を用いてこの課題解決を試みました。Self-Attentionとは、文章全体で重要とされるキーワードが予測結果と一緒に確認できるようになる手法です。Self-Attentionの仕組みについては詳しく書かれている方が多くいますので、ここでは割愛します。最初は、word2vecを用いて文章特徴量を作成し、承認・不承認を予測して終了という一連の流れを想定していました。しかし、今回の目的は承認・不承認を予測したいわけではなく、どの単語が承認・不承認と関わっているのかを確認し、不承認となっているアカウントを承認にすることです。もし仮にword2vecを用いた手法を採用すると予測結果の要因や理由が明確にならないので、不承認のアカウントを承認に改善する施策を考えることはできません。そのため今回は、Self-Attentionを用いて分類モデルを構築することで、承認・不承認の要因が文章内のどこにあるのかを分析するために、この手法を選択しました。実装手順本来の目的は達成できませんでしたが、Self-Attentionでの分類モデルの実装はできましたので、実装方法を記載します。今回はkerasを用いてSelf-Attention + LSTMで予測を行いました。検証環境はGoogle Colaboratoryを想定しています。使用データのフォーマット今回使用できるデータとしては以下のようなデータになっています。各アカウント・各商品ごとに商品IDが割り振られており、それぞれの商品にtitle,descriptionが割り振られています。承認・不承認のラベルは、アカウントごとに付加されています。必要ライブラリのインストール・インポート!pip install text_vectorian!pip install mojimoji!apt install aptitude!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y!pip install mecab-python3==0.7import pandas as pdimport numpy as npimport kerasimport osimport warningswarnings.simplefilter('ignore')import subprocessimport mojimojiimport reimport MeCabimport matplotlib.pyplot as pltfrom keras.layers import Dense, Dropout, LSTM, Embedding, BatchNormalizationfrom keras.layers.wrappers import Bidirectionalfrom keras.callbacks import EarlyStopping, ModelCheckpointfrom keras import Input, Model, utilsfrom keras.preprocessing.sequence import pad_sequencesfrom keras.callbacks import EarlyStoppingfrom text_vectorian import SentencePieceVectorianfrom keras_self_attention import SeqSelfAttentionfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_reportデータの前処理# データの読み込みapp = pd.read_csv('data/app.csv') # 承認データdisapp = pd.read_csv('data/disapp.csv') # 不承認データapp['target'] = 'app' # targetにlabelを代入するdisapp['target'] = 'disapp'# 今回は、titleとdescriptionを用いて予測するので、それら二つの変数を一つにまとめるapp['sentence'] = app['title'] + app['description'] disapp['sentence'] = disapp['title'] + disapp['description']# これまで別々に処理していたappとdisappをまとめてdfとするdf = app.append(disapp)今回のデータは特殊で、承認・不承認は商品ごとについているラベルではなくアカウントと紐づいたラベルとなっています。それらを各商品と承認・不承認が紐づいているとして各商品ごとに予測することを行ってます。ここで注意が必要なのは、データの分割方法です。アカウントを無視してデータを分割してしまうとリークを起こす可能性があります（リークとは、本来予測では使用できないデータが学習時に入ってしまっていることです）。そのため同じアカウントのデータが訓練データ、検証データ、テストデータに渡って存在しないようにしなければなりません。例えば、アカウントAの商品データは全て訓練データとする,アカウントBの商品データは全てテストデータにするといったようなことを意味しています。アカウントごとにデータを分割するには、各アカウントごとの商品数がある程度同じである方がlabelが不均衡にならないと考え、データ数を揃える処理を施しました。（これらはGroupKFoldを使用すれば解決できると考えられますが、分析実施時はGroupKFoldを知らなかった）# アカウントごとに商品数が異なるので50以上商品数がある場合は50までの商品を使用する# アカウントごとに商品数を揃えることで、labelが不均衡になることを緩和している# アカウントごとにlabelがふられるが、商品ごとに予測結果を出す時のみ実施cutted_df = pd.DataFrame([])for acc in df.account_name.unique():  data = df[df.account_name == acc]  if data.shape[0] > 50:     data = data[:50]  cutted_df = pd.concat([cutted_df,data],0)  df = cutted_df.sample(frac=1,random_state=1).reset_index(drop=True)次は、データの前処理についてです。自然言語処理の前処理で有効と言われている半角->全角、数字は全て0にする、スペース文字の消去を行いました。また、これまでlabelが'app'または'disapp'だったのでそれらを入力できる形式に変換しています。def PreprocessData(df,dirname):  # データの前処理関数  # 辞書型を返す  mecab = MeCab.Tagger('-Ochasen')  # textデータの前処理  df = TextPreprocess(df)  label2index = {k: i for i, k in enumerate(df.target.unique())}  index2label = {i: k for i, k in enumerate(df.target.unique())}  class_count = len(label2index)  labels = utils.to_categorical([label2index[label] for label in df.target], num_classes=class_count)  features,sentences,vectorian,account = MakeFeatures(df)  return {      'class_count': class_count,      'label2index': label2index,      'index2label': index2label,      'labels': labels,      'features': features,      'sentences':sentences,      'input_len': vectorian.max_tokens_len,      'vectorian':vectorian,      'account':account  }def TextPreprocess(df):  for i in df.index:    sen = df.loc[i,'sentence']    sen = mojimoji.han_to_zen(sen)    sen = re.sub(r'\d+','0',sen)    df.loc[i,'sentence'] = sen.replace('\u3000','')  return dfdef MakeFeatures(df):  vectorian = SentencePieceVectorian()  features = []  sentences = []  accounts = []  for feature,account in zip(df['sentence'],df['account_name']):    f = vectorian.fit(feature).indices    features.append(f)    sentences.append(feature)    accounts.append(account)  features = pad_sequences(features, maxlen=vectorian.max_tokens_len)  return features,sentences,vectorian,accountsでは、ここまでの前処理を流します。data = PreprocessData(df,dirname) # dirnameは、出力結果などを入れたいpath入れてください次はtrain_test_splitを行いますが、先ほども記述した通り通常の手法ではリークするので、以下のようにしました。（上述の通りGroupKFoldの実施で回避できる）def CollectData(data,account):  features = []  sentences = []  labels = []    for ac in account:    where_ = np.where(np.array(data['account']) == ac)    features.extend(np.array(data['features'])[where_])    sentences.extend(np.array(data['sentences'])[where_])    labels.extend(np.array(data['labels'])[where_])  return np.array(features),np.array(sentences),np.array(labels)train_account,test_account = train_test_split(list(set(data['account'])),test_size=0.2,random_state=1)train_account,val_account = train_test_split(train_account,test_size=0.25,random_state=1)train_features,train_sen,train_labels = CollectData(data,train_account)val_features,val_sen,val_labels = CollectData(data,val_account)test_features,test_sen,test_labels = CollectData(data,test_account)通常のデータセットであれば、以下のようにすることでデータの分割が行えます。(train_features,val_features, train_labels,val_labels, train_sen,val_sen) = train_test_split(data['features'], data['labels'], data['sentences'], test_size=0.2, random_state=1)(train_features,test_features, train_labels,test_labels, train_sen,test_sen) = train_test_split(train_features, train_labels, train_sen, test_size=0.25, random_state=1)ここまででデータの整形が完了です。学習次は、モデルの定義を行います。def _create_model(input_shape, hidden, class_count,vectorian):    input_tensor = Input(input_shape)    common_input = vectorian.get_keras_layer(trainable=True)(input_tensor)    x1 = SeqSelfAttention(name='attention')(common_input)    x1 = Bidirectional(LSTM(hidden))(x1)    x1 = Dropout(0.5)(x1)    x1 = Dense(32)(x1)    x1 = Dropout(0.5)(x1)    x1 = Dense(16)(x1)    x1 = Dropout(0.5)(x1)    output_tensor = Dense(class_count, activation='softmax', name='class')(x1)    model = Model(input_tensor, output_tensor)    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])    return modelhidden = 356model = _create_model(train_features[0].shape, hidden, data['class_count'],data['vectorian'])model.summary()作成したモデルにデータを流して学習を進めます。model_filename='{0}/model.h5'.format(dirname)history = model.fit(train_features, train_labels,                    epochs=50,                    batch_size=32,                    validation_data=(val_features, val_labels),                    shuffle=False,                    callbacks = [                        EarlyStopping(patience=5, monitor='val_acc', mode='max'),                        ModelCheckpoint(filepath=model_filename, monitor='val_acc', mode='max', save_best_only=True)                    ])評価・出力ModelCheckpointで保存したmodelを読み取り、さらにSelf-Attentionの結果を得られるようにします。from keras.models import load_modelmodel = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())model = Model(inputs=model.input, outputs=[model.output, model.get_layer('attention').output])modelにtest dataを入れて結果を取得します。out = model.predict(test_features)y = out[0] # 予測labelのsoftmaxが入っているweight = out[1] # Self-Attentionのweighが入っているpred = np.argmax(y,1) # 予測値max = np.max(y,1) # 信頼値df_y = pd.DataFrame(np.array([np.argmax(test_labels,1),pred,max*100]).T,columns=['true','pred','trust']) # 結果をまとめておくと精度確認に使える精度の確認を行います。ただ、testを入力し得られた結果を出力するだけでは精度が得られなかったので、信頼値が高いものだけを選別し、出力するようにしました。信頼値を90~55の間で出力し、最もAccuracyが高い時の信頼値以上のものを出力としました。 一方で信頼値を上げすぎるとわずかな出力しか得られないので、元のtestデータ数の1/3はデータ数が出力として確保できるような条件を加えました。report = classification_report(pred, np.argmax(test_labels,1),output_dict=True,target_names=[data['index2label'][i] for i in [0,1]])FirstReport_df = pd.DataFrame(report).Tprint(FirstReport_df)FirstReport_df.to_csv(dirname+'NotCutReport.csv')# 信頼値が高い予測だけを出力とすることで確からしいものだけをみるAppSupport = FirstReport_df.loc['app','support'] # 予測した数を取得DisappSupport = FirstReport_df.loc['disapp','support']for UpperLimit in range(90,55,-1):  max_acc = 0  for i in range(50,UpperLimit,1):    df_y_cut = df_y[df_y.trust > i]    report = classification_report(df_y_cut.pred, df_y_cut.true ,output_dict=True)    report_df = pd.DataFrame(report).T    acc = report_df.loc['accuracy','precision']    if max_acc < acc:      max_acc = acc      max_i = i  df_y_cut = df_y[df_y.trust > max_i]  report = classification_report(df_y_cut.pred, df_y_cut.true ,output_dict=True,target_names=[data['index2label'][i] for i in [0,1]])  report_df = pd.DataFrame(report).T  if (report_df.loc['app','support'] > HighSupport/3) and (report_df.loc['disapp','support'] > LowSupport/3):    # 元の予測値の1/3のデータ数が確保できていればクリア    print('UpperLimit:' + str(UpperLimit))    print('max_i:' + str(max_i))    print(report_df)    report_df.to_csv(dirname+'Report.csv')    break最後にSelf-AttentionのWeightをcsvで出力します。得られた出力結果は、予測値-承認と真値-承認、予測値-承認と真値-不承認、予測値-不承認と真値-承認、予測値-不承認と真値-不承認のように予測値と真値の結果に応じて4つに分けてcsvで出力するようになっています。app_app = pd.DataFrame([])app_disapp = pd.DataFrame([])disapp_app = pd.DataFrame([])disapp_disapp = pd.DataFrame([])for i in range(len(test_features)):  input_text = test_sen[i]  tokens = data['vectorian'].tokenizer._tokenizer.encode_as_pieces(input_text)  conf = out[0][i] * 100  wei = out[1][i]  if np.max(conf) <= max_i:    continue  pred = [data['index2label'][np.argmax(conf)]]  labels = [data['index2label'][np.argmax(test_labels[i])]]  weights = [w.max() for w in wei[-len(tokens):]]  df = pd.DataFrame([tokens, weights], index=['token', 'weight']).T  mean = np.asarray(weights).mean()  for j in df.index:    if df.loc[j,'weight'] - mean <= 0:      df.loc[j,'weight'] = 0    else:      df.loc[j,'weight'] = df.loc[j,'weight'] - mean    pred += df.token.values.tolist()  labels += df.weight.values.tolist()  final = pd.DataFrame(np.array([pred,labels]).T,columns=['pred',input_text])  if (pred[0] == 'app') & (labels[0] == 'app'):    app_app = pd.concat([app_app,final],1)  elif (pred[0]  == 'app') & (labels[0] == 'disapp'):    app_disapp = pd.concat([app_disapp,final],1)  elif (pred[0]  == 'disapp') & (labels[0] == 'app'):    disapp_app = pd.concat([disapp_app,final],1)  elif (pred[0]  == 'disapp') & (labels[0] == 'diaspp'):    disapp_disapp = pd.concat([disapp_disapp,final],1)app_app.to_csv(dirname+'app_app.csv',index=False)app_disapp.to_csv(dirname+'app_disapp.csv',index=False)disapp_app.to_csv(dirname+'disapp_app.csv',index=False)disapp_disapp.to_csv(dirname+'disapp_disapp.csv',index=False)まとめSelf-Attentionを用いて無料リスティングの不承認理由を解き明かそうと分析しました。しかし、title,descriptionのみからは承認と不承認を分類することができず、不承認理由の解明には貢献できせんでした。Self-Attentionとデータセットの相性が悪いという可能性も考えられるので、tfidf+lgbも試みましたが、こちらもうまくいきませんでした。やはりこちらの結果からもtitle,descriptionのみからは承認と不承認を分類することができないということが考えられます。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[自作の gem の名前を考えるのは難しい]]></title>
        <id>https://ryz310.hateblo.jp/entry/2021/03/07/202519</id>
        <link href="https://ryz310.hateblo.jp/entry/2021/03/07/202519"/>
        <updated>2021-03-07T11:25:19.000Z</updated>
        <summary type="html"><![CDATA[自作の gem の名前を変えたい。my_api_client という自作の gem がありまして、このブログでは何度も紹介している んですが、ニッチすぎるのか宣伝が下手すぎるのか、一向に使ってみた、という噂を聞きません (´・ω・｀)github.comまあ弊社のプロダクトの中ではガッツリ使ってるんで別にそれは良いんですが、もう少しまともな名前にならんのかね、というコメントを頂きます。いい機会だしちゃんと良い名前付けようと思って考えました。どうせなら自分が好きなゲームからいい名前付けたいな、と思って『Luida（ルイーダ）』という名前が浮かびました。ドラクエ III のあれです。ここはルイーダの店。旅人たちが仲間を求めてあつまる出会いと別れの酒場よ。my_api_client は API Client を簡単に作ったりテストしたりするための gem なので、冒険者を登録して一緒に旅するルイーダの店のイメージがピッタリだなーと思ったんですよね。とはいえ gem の名前を変えるのって面倒だしそのうちやろう、って思ってたら半年くらい経っちゃいましたけどね。上のルイーダの店の画像ってわざわざ iPhone 版のドラクエIII 買ってスクショ撮ったんですが、その日付が 2020/10/04 でした 😇多分そのうちやります（フラグ）my_api_client v0.20.0 をリリースしました 🚀gem の名前は変わらないけどアップデートはされていく。github.com元々 my_api_client には #pageable_get (alias: #pget) というメソッドがあり、REST API のレスポンス JSON に含まれる URL を順に辿ってリクエストする Enumerator を取得することが出来ます。機能自体は実相してテストもしてあるものの、実際のプロダクトで使う機会がなく長いこと日の目を見なかったんですが、この度ついに弊プロダクトで利用する機会が訪れたのでチームのエンジニアに使ってもらってるんですが、 RSpec を書く際のスタブ化が特にサポートされていなくてテストしづらい問題がありました。今回のアップデートでは、この #pageable_get に対応するスタブ化をサポートしています。 詳しい解説は README.jp.md にも書いた んですが、せっかくなのでブログにも転記しておきます。まあ文章で説明されても実際に使ってみないとピンと来ないのはわかってますけどね。。。#pageable_get  (#pget) を使った実装用に pageable というオプションが利用できます。pageable に設定する値は Enumerable である必要があります。stub_api_client_all(  MyPaginationApiClient,  pagination: {    pageable: [      { page: 1 },      { page: 2 },      { page: 3 },    ],  })MyPaginationApiClient.new.pagination.each do |response|  response.page #=> 1, 2, 3endなお、 Enumerable の各値にはここまで紹介した response, raise, Proc など全てのオプションが利用可能です。stub_api_client_all(  MyPaginationApiClient,  pagination: {    pageable: [      { response: { page: 1 } },      { page: 2 },      ->(params) { { page: 3, user_id: params[:user_id] } },      { raise: MyApiClient::ClientError::IamTeapot },    ],  })また、 Enumerator を使えば無限に続くページネーションを定義することもできます。stub_api_client_all(  MyPaginationApiClient,  pagination: {    pageable: Enumerator.new do |y|      loop.with_index(1) do |_, i|        y << { page: i }      end    end,  })]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[データ指向アプリケーションデザイン 第三章 旅行記]]></title>
        <id>https://developer.feedforce.jp/entry/2021/03/04/180304</id>
        <link href="https://developer.feedforce.jp/entry/2021/03/04/180304"/>
        <updated>2021-03-04T09:03:04.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。id:kano-e です。先日 データ指向アプリケーションデザイン 第二章 旅行記 という記事を公開しました。第三章も第二章と同じく、地図への書き込みという形で読書記録を作りました。第三章の章題は「ストレージと抽出」です。データ指向アプリケーションデザイン 第三章 旅行記第三章では分散データ洋に浮かぶ島を舞台に、トランザクション共和国のログストラクチャ、B-Tree領、そして分析王国のデータウェアハウス領まで旅をしました。この旅の記録が、ツールの選定やチューニングの際の道標になるのでは、と思います。データ指向アプリケーションデザイン ―信頼性、拡張性、保守性の高い分散システム設計の原理作者:Martin Kleppmann発売日: 2019/07/18メディア: 単行本（ソフトカバー）よろしければ第二章の記録もどうぞ。developer.feedforce.jpそれでは、みなさま、良い旅を！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[データ指向アプリケーションデザイン 第二章 旅行記]]></title>
        <id>https://developer.feedforce.jp/entry/2021/03/02/172959</id>
        <link href="https://developer.feedforce.jp/entry/2021/03/02/172959"/>
        <updated>2021-03-02T08:29:59.000Z</updated>
        <summary type="html"><![CDATA[こんにちは！id:kano-e です。今日は『データ指向アプリケーションデザイン』という本の読書記録を公開しにきました。こちらの本、章毎にその章を象徴するような地図が挿絵として挿入されています。というわけで、この『データ指向アプリケーションデザイン』でも、そのように「今はこの地図のこの辺りだなあ」なんて思いながら読み進めていました。その読書記録、あるいは旅の記録が以下の画像です。データ指向アプリケーションデザイン 第二章 旅行記第二章の章題は「データモデルとクエリ言語」。データ指向アプリケーションデザイン ―信頼性、拡張性、保守性の高い分散システム設計の原理作者:Martin Kleppmann発売日: 2019/07/18メディア: 単行本（ソフトカバー）アプリケーションは常に変化し続けます。その中で、どのようにデータというものと向き合ってゆくのか。そんな内容が書かれている面白い本です。旅の記録を書きとめるのに使ったホワイトボードはこちら。nu board (ヌーボード) A4判 NGA403FN08発売日: 2015/04/01メディア: オフィス用品このタイプの nu board には透明シートがあるので、そのシートの下に地図のコピーを置いて、文字や旅路を辿った線は透明シートの上に書いています。それではみなさん、良い旅を！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ActiveJob の retry_on に jitter というオプションがあるの知ってますか？]]></title>
        <id>https://ryz310.hateblo.jp/entry/2021/03/02/113927</id>
        <link href="https://ryz310.hateblo.jp/entry/2021/03/02/113927"/>
        <updated>2021-03-02T02:39:27.000Z</updated>
        <summary type="html"><![CDATA[僕は知らなかったです (・∀・)Rails で ActiveJob の retry_on が同時に発火されるの何とかしたいなーと思って調べてたら、Rails 6 からは jitter というオプションが指定できるようになってて、デフォルトでリトライ間隔を 15% ランダマイズしてくれるとの事。Rate Limit の回避とかで便利。 https://t.co/kpIx6YVFEq— サトウリョウスケ (@ryosuke_sato) February 27, 2021 jitter とはRails 6.1 から追加されたオプション で、 retry_on の待ち時間に対して任意の割合でバラけさせてくれるようになります（デフォルト 15% ）。ActiveJob::Exceptions::ClassMethods - retry_on:jitter - A random delay of wait time used when calculating backoff. The default is 15% (0.15) which represents the upper bound of possible wait time (expressed as a percentage)実装を確認してみたこのバラけさせ方が待ち時間に対して増えるのか減るのかが気になって実装を見てみました。増える方向でバラけるようです。例えば wait: 60.seconds で jitter: 0.5 の場合だと、最大 90 秒の待ち時間となります。うっかり 100.0 とか指定すると最大 10000% 待ち時間が加算されちゃいそうです。間違えて指定しないようにご注意下さい🙏delay = seconds_or_duration_or_algorithm.to_idelay_jitter = determine_jitter_for_delay(delay, jitter)delay + delay_jitter📝 jitter の計算処理は このあたりどういう場面で使うの？この機能が無かった従来だと、リトライ処理が一斉に起動してしまうという問題がありました。例えば外部のサービスに API リクエストするような Job を作ったとします。API Rate Limit 超過の例外をハンドリングして、 retry_on で N 分後にリトライするように実装します。この時、 1000 件の Job が同時に実行され 900 件が Rate Limit 超過となった場合、N 分後に 900 件の Job が一斉にリトライされてしまい、再び 800 件がエラーとなってしまう。これが何度も繰り返される、という現象が起こってました。Rails 6.1 以前だと回避できないの？従来の ActiveJob でも一応回避策はあって、 wait に Proc を与えてランダムな待ち時間を返すような処理を書くことで似たような動作は可能です。retry_on SomeError, wait: -> { rand(60..90).seconds }iPad Air (第4世代) 買っちゃったiPad Air (第4世代)近日中に iPad Pro の新型が出るとの噂があるが、軽い Air がほしかったので問題 🍆 （知らなかったけどね）🔗 新型iPad Pro搭載A14X/A14Zチップの処理能力はM1チップに匹敵か - iPhone Mania]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[RuboCop で違反してるコードを自動的に修正する PR 作ってくれたら嬉しいやろ。できるでそれ。]]></title>
        <id>https://ryz310.hateblo.jp/entry/2021/02/23/222720</id>
        <link href="https://ryz310.hateblo.jp/entry/2021/02/23/222720"/>
        <updated>2021-02-23T13:27:20.000Z</updated>
        <summary type="html"><![CDATA[久々の更新は Rubocop Challenger の話です。このブログでは触れたこと無かったですが、 そういう gem も作ってます。自動的に .rubocop_todo.yml から  Cop supports --auto-correct. になってる Cop を拾ってきて PR 作ってくれるやつです。ちょうど hey 社の CTO の藤村さんが同じ事をやっていてちょっとバズってたんですが、自分の gem 使ってほしかったなーと地味に思ってたりします。自分が世の中に対してアピールが足りてなさすぎましたね 😇tech.hey.jp何年か前に会社のブログで書いたりはしてたのでリンク貼っておく ✍developer.feedforce.jpRubocop Challenger v2.3.0 をリリースしました 🚀github.comとはいえ新機能は一つだけで、 auto-correct された Cop が SafeAutocorrect かどうかを教えてくれる、というものです。Add description whether the challenge is created by safe autocorrect or not by ryz310 · Pull Request #465 · ryz310/rubocop_challenger · GitHubこんな感じの PR が作成されます。（画像は動作確認で作ったものなので、実際の Style/Alias は常に SafeAutocorrect: true になります）ぜひお試し下さい 👍つぎやりたいこと先日 GitHub が auto-merge 機能をリリースしましたね。自分は即全部のリポジトリで有効化しました ✅github.blog次やりたい機能としては SafeAutocorrect: true  の場合とかは auto-merge を有効化した PR を作るようにしたいんですよね。GitHub の GraphQL API でこの機能を有効化できるらしいです。GraphQL APIs will be rolling out later this week. The pull request webhook event also now includes actions that indicate when auto-merge is enabled or disabled.また時間ある時にでも調べます。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[CircleCI で docker build するときの Empty continuation lines will become errors in a future release. という warning への対処方法]]></title>
        <id>https://developer.feedforce.jp/entry/2021/02/17/110000</id>
        <link href="https://developer.feedforce.jp/entry/2021/02/17/110000"/>
        <updated>2021-02-17T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、id:tsub511 です。Dockerfile を読みやすくするために \ とコメントを駆使してみたら CircleCI で warning が出て一瞬焦ったので記事を書いてみました。CircleCI で docker build する時の warningwarning が出たのは Docker のバグCircleCI の Docker のデフォルトバージョンは 17.09.0-ceまとめCircleCI で docker build する時の warning例えば以下のような Dockerfile があったとします。FROM amazonlinux:2ENV RUBY_VERSION=2.7.2 \    BUNDLER_VERSION=2.2.9 \    TZ=/usr/share/zoneinfo/Asia/TokyoRUN \    # Install mysql-community-devel    yum install -y yum-utils && \    yum localinstall -y https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm && \    yum-config-manager --enable mysql57-community && \    yum-config-manager --disable mysql80-community && \    yum install -y mysql-community-devel && \    yum remove -y mysql80-community-release yum-utils && \    \    # Install ruby    yum install -y "https://github.com/feedforce/ruby-rpm/releases/download/${RUBY_VERSION}/ruby-${RUBY_VERSION}-1.el7.centos.x86_64.rpm" && \    printf "install: --no-document\nupdate: --no-document\n" > /etc/gemrc && \    gem install -v "$BUNDLER_VERSION" bundlerこれを使って CircleCI で docker build すると、Empty continuation lines will become errors in a future release. という warning が出てしまいます。docker build 時の warning普通に読むと、何もコマンドを実行していない \ だけの行やコメントだけの行を消した方が良いのかな？と受け取ってしまいがちですが、これは実は Docker 側のバグでした。warning が出たのは Docker のバグ本来はただの空行だけの場合に warning を出したかったようですが、コメントが書かれた行も warning が出てしまっているようです。以下に書かれているように、Docker 17.10 で修正済みとのことです。Thanks for reporting; this issue was resolved through #35004, which is included in Docker 17.10 and up.I'll close this issue because this was resolved, but feel free to continue the conversation 👍github.comただ、Docker 17.10 というと 2017/10 リリースのバージョンですので、2021/02 現在でまだバグが残っているのはおかしいです。CircleCI の Docker のバージョンを確認してみましょう。CircleCI の Docker のデフォルトバージョンは 17.09.0-ceCircleCI 内で docker build を実行するためには setup_remote_docker が必要です。setup_remote_docker によって CircleCI のジョブのホスト VM で Docker Engine が起動しますが、そこで使われている Docker Engine のバージョンは 17.09.0-ce でした。CircleCI の Docker Engine バージョンDocker のバグが修正されたのは 17.10 ですので、確かにまだバグが残っているバージョンです。さて CircleCI の setup_remote_docker ですが、実はデフォルトでは 17.09.0-ce が使われるようです。https://circleci.com/docs/2.0/building-docker-images/#docker-versionまとめDockerfile で \ だけの行やコメントだけの行がある時に CircleCI で docker build すると Empty continuation lines will become errors in a future release. という warning が出るのは Docker のバグと CircleCI の setup_remote_docker のデフォルトバージョンが古い、という合わせ技によって起きていました。warning 自体は Docker のバグだったので無視で良いですが、古いバージョンを使い続けるのはあまり良くない気がします。基本的にはデフォルトを使いたいところですが、17.09.0-ce だと色々な機能が使えないですし、上述したバグもあるので setup_remote_docker を使うときは version を指定することをおすすめします。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[docker-compose での MySQL の疎通確認で telnet を使う時に自動でコネクションを切る]]></title>
        <id>https://developer.feedforce.jp/entry/2021/02/16/110000</id>
        <link href="https://developer.feedforce.jp/entry/2021/02/16/110000"/>
        <updated>2021-02-16T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、id:tsub511 です。TELNET プロトコルには全く馴染みがないのですが、今回たまたま使う機会があり、かつ調べても割と見つけられない情報だったので記事を書いてみました。curl で TELNET プロトコルを使うユースケース解説curl で TELNET プロトコルを使うcurl は HTTP/HTTPS 以外のプロトコルも使うことができます。curl のドキュメントを確認すると、サポートしているプロコトルは DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMB, SMBS, SMTP, SMTPS, TELNET and TFTP のようです。$ man curl # in macOScurl(1)                                                                                                                    Curl Manual                                                                                                                    curl(1)NAME       curl - transfer a URLSYNOPSIS       curl [options / URLs]DESCRIPTION       curl is a tool to transfer data from or to a server, using one of the supported protocols (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMB, SMBS, SMTP, SMTPS, TELNET and TFTP). The command       is designed to work without user interaction.       curl offers a busload of useful tricks like proxy support, user authentication, FTP upload, HTTP post, SSL connections, cookies, file transfer resume, Metalink, and more. As you will see below, the number of features will make your head spin!       curl is powered by libcurl for all transfer-related features. See libcurl(3) for details....例えば以下のように指定することで TELNET プロトコルで対象のサーバーに接続することが可能です。$ curl telnet://localhost:3306ユースケースdocker-compose ではコンテナ間の依存関係を depends_on で定義できますが、コンテナ内でサーバーなどが立ち上がるまでは待ってくれません。そこで、以下のドキュメントに書いてあるような方法でコンテナ間で依存しているサーバーに対するヘルスチェックを行うことで解決できます。docs.docker.com実際には以下のスクリプトで MySQL サーバーの起動を待ってから別のコンテナを実行するような仕組みにしていました。#!/bin/bashhost="$1"shiftcmd="$@"until mysql -h "$host" -u root -e 'show databases' > /dev/null 2>&1; do  >&2 echo "MySQL is unavailable - sleeping"  sleep 1done>&2 echo "MySQL is up - executing"exec $cmdただ、この方法だと mysql コマンドがコンテナ内にインストールされている必要があります。本番環境でのコンテナの実行を考慮すると、mysql のクライアントはインストールする必要がなかったので mysql コマンド以外の方法で MySQL サーバーの起動を確認する必要がありました。そこで、ベースイメージの都合でたまたま curl がインストールされていたので curl の TELNET プロトコルを使うことにしました。変更後のスクリプトが以下になります。#!/bin/bashhost="$1"shiftcmd="$@"until echo 'quit' | curl telnet://$host:3306 > /dev/null 2>&1; do  >&2 echo "MySQL is unavailable - sleeping"  sleep 1done>&2 echo "MySQL is up - executing"exec $cmd変更したのは 7 行目のみで、mysql コマンドを curl に置き換えています。こうすることで mysql のクライアントをインストールせずに MySQL サーバーが起動するのを待ってから別のコンテナを実行することができるようになりました。解説例えば以下のように実行すると、TELNET プロトコルを使って疎通確認ができます。$ docker run -d -p 3306:3306 -e MYSQL_ALLOW_EMPTY_PASSWORD=1 mysql$ curl -s -o /dev/null telnet://localhost:3306ただしこのままだと Ctrl+C などでコネクションを切るまで curl が実行されたままになります。TELNET プロトコルは対話型であるため、コネクションを張りっぱなしになるという認識です。Ctrl+C が必要ということは、上述したスクリプトでは使えません。ではどうすれば疎通確認後に自動でコネクションを切れるでしょうか。実は以下のように quit を curl に標準入力で渡すことで解決できます。$ docker run -d -p 3306:3306 -e MYSQL_ALLOW_EMPTY_PASSWORD=1 mysql$ echo 'quit' | curl -s -o /dev/null telnet://localhost:3306quit とは何かというと、telnet コマンドでは quit というコマンドを指定することで telnet の接続を切ることができます。$ man telnet # in macOS with brew install telnetTELNET(1)                 BSD General Commands Manual                TELNET(1)NAME     telnet -- user interface to the TELNET protocolSYNOPSIS     telnet [-468EFKLNacdfruxy] [-S tos] [-X authtype] [-e escapechar] [-k realm] [-l user] [-n tracefile] [-s src_addr] [host [port]]DESCRIPTION     The telnet command is used to communicate with another host using the TELNET protocol.  If telnet is invoked without the host argument, it enters command mode, indicated by its prompt (``telnet>'').  In this mode, it accepts and executes the commands     listed below.  If it is invoked with arguments, it performs an open command with those arguments.     Options:...quit       Close any open TELNET session and exit telnet.  An end of file (in command mode) will also close a session and exit.そして quit を使った telnet コマンドを自動的に終了するための方法が以下の記事で紹介されていました。qiita.com上記の記事を参考に、curl でも同様の方法を試してみたら動いた、ということになります。ただし、curl に標準入力を渡すことで TELNET プロトコルにコマンドを渡すことができる、という挙動自体は curl のドキュメントを確認しても見つけられませんでした。公式の情報で裏が取れない限りは当記事の事例のように開発環境でのみ使った方が良いかもしれません。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[全ての esa 新着記事を読みきれないので、最近はこの方法を使ってる]]></title>
        <id>https://developer.feedforce.jp/entry/2021/02/15/110000</id>
        <link href="https://developer.feedforce.jp/entry/2021/02/15/110000"/>
        <updated>2021-02-15T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:masutaka26 です。最近好きな YouTuber はシバターです。シバターかわいいよ...。シバター...フィードフォースでは先日紹介したとおり、ドキュメント共有ツールに esa を使っています。メンバー数は 90、デイリーアクティブメンバーは 69 くらいです。2/10 の新着記事数は 106 でした。情報共有が活発な会社で、体感でもとても多いです。全ての新着記事と記事へのコメントは Slack の #esa-notify に通知されます。そこを全部読めば追えはしますが、いかんせんこの数です。真面目に全部読むと、私の場合はかける時間に見合わないなと感じたので、最近は esa の検索を利用した方法を使っています。使っている方法フォローした人の新着記事を読むエゴサーチする気になるキーワードに言及した新着記事を読む日報以外の最近更新された記事を読む直近の記事を読む公式ドキュメントちょっとした悩み所感おまけ: ボツにした方法使っている方法フォローした人の新着記事を読む検索クエリの例:@alice OR @bob OR @sapp sort:created-desc🔖 ブラウザのブックマーク: Follow上の例では alice, bob, sapp をフォローしているイメージです。社長やプロダクトオーナーをフォローして、最近の会社の状況を把握するようにしています。他、自分が関心がある人もフォローしています。メンバーは固定ではなくたまに入れ替えます。現在 16 人をフォローしています。エゴサーチする検索クエリの例:マスタカ -@me OR ますたか -@me OR masutaka -@me OR 増田 -@me sort:created-desc🔖 ブラウザのブックマーク: Egosearch「マスタカ」「ますたか」「masutaka」「増田」が含まれ、且つ自分の記事を除いた新着記事を全部読んでいます。空メンション masutaka やアイコン絵文字 :@masutaka:、<!-- Boku ha masutaka nari. --> のような HTML コメントも捕捉出来ます。👁気になるキーワードに言及した新着記事を読む検索クエリの例:looker -@me sort:created-desc🔖 ブラウザのブックマーク: Search Looker「looker」が含まれ、且つ自分の記事を除いた新着記事を全部読んでいます。日報以外の最近更新された記事を読む検索クエリの例:-category:日報 sort:updated-desc🔖 ブラウザのブックマーク: 日報以外日報/ カテゴリ以外で、最近更新された記事をたまに読んでいます。新着記事でないことがポイントです。良さそうな WIP 記事を先取りできることがあります。🤗直近の記事を読む退勤後とかは、新着記事一覧 /posts も見ます。公式ドキュメントここまで使った検索クエリは、公式ドキュメント「help/記事の検索方法」に書かれています。OR 検索よりも AND 検索のほうが優先度が高いとは書かれていないので、行間を読む必要があります。括弧は使えないので、前述の「エゴサーチする」は自分にとって難易度が高かったです。esa の皆様、その節はサポートありがとうございました。🙏ちょっとした悩みスマホだと「フォローした人の新着記事を読む」が検索対象のメンバーアイコンで埋まるので、結構スクロールしないと記事にたどり着けません。メンバーアイコンと記事の境界も曖昧で、スクロールしすぎることがあります。スマホだと数スクロールがメンバーアイコンで埋まるもう少しメンバーアイコンのサイズが小さいと良いと思います。ﾁﾗｯﾁﾗｯ所感全ての新着記事を読む代わりに、テーマを決めた複数の方法で esa 記事を読み、かける時間を減らしました。２ヶ月ほど使っており、なかなか満足しています。ただ、関心外のことが入って来づらくなることは、特にこのフルリモートワーク下では怖さを感じます。カッチリやらずに、たまに #esa-notify を覗けばよいのですけどね。記事へのコメントだけを通知する Slack チャンネルでも作れば良いのかしら？おまけ: ボツにした方法コメントもエゴサーチしようとしましたが、なんか期待通りの振る舞いではなかったです。comment:マスタカ OR comment:ますたか OR comment:masutaka OR comment:増田 sort:created-descコメント本文はきちんと検索出来ているようですが、検索結果はあくまで（コメント単位ではなく）記事単位なので、すでに読んだ記事のことが多いからかな？他の方法で網羅できているようなので、まあいいかあと思いました。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Terraform の terraform-provider-datadog で古い source から新しい source に更新する際の Warning を解消する方法]]></title>
        <id>https://developer.feedforce.jp/entry/2021/02/09/114110</id>
        <link href="https://developer.feedforce.jp/entry/2021/02/09/114110"/>
        <updated>2021-02-09T02:41:10.000Z</updated>
        <summary type="html"><![CDATA[ソーシャルPLUS の開発チーム でインフラエンジニア をしている id:mayuki123 です。ソーシャルPLUS のインフラ環境は基本的に Terraform を利用して管理をしています。今回は、Terraform の terraform-provider-datadog で terraform init を実行時の Warning を解消するのに頭を悩ませたので書き残しておきます。発生していた事象Datadog Provider を利用している所で terraform init を実行すると、下記のWarning が表示されるようになりました。Warning: Additional provider information from registryThe remote registry returned warnings forregistry.terraform.io/terraform-providers/datadog:- For users on Terraform 0.13 or greater, this provider has moved toDataDog/datadog. Please update your source in required_providers.Datadog Provider を導入した時は、 Source に terraform-providers/datadog を指定する必要がありましたが、現在は DataDog/datadog が推奨との事で変更する必要があるようです。Terraform のコードで Source を変更すればいとも簡単に解決するだろうとこの時の私は思っていました。Terraform の コードを修正後に terraform init を実行して Provider を更新した後に、 terraform providers を実行してみると State では registry.terraform.io/terraform-providers/datadog を使い続ける事象に陥りました。% terraform providersProviders required by configuration:.├── provider[registry.terraform.io/hashicorp/aws] 3.27.0├── provider[registry.terraform.io/datadog/datadog] 2.20.0/// 中略 ///Providers required by state:    provider[registry.terraform.io/hashicorp/aws]    provider[registry.terraform.io/terraform-providers/datadog]    provider[terraform.io/builtin/terraform] Terraform v0.14.x で追加されるようになった .terraform.lock.hcl には Provider として registry.terraform.io/datadog/datadog が新しく追加はされますが、 registry.terraform.io/terraform-providers/datadog が残り続けていました。亡霊なのでしょうか。解決方法terraform state replace-provider コマンドを実行する事で、State 上の既存のリソースに対して Terraform の Datadog provider の Source として DataDog/datadog を利用するように変更する必要がありました。terraform state replace-provider \'registry.terraform.io/terraform-providers/datadog' \'registry.terraform.io/datadog/datadog'実行すると下記のような確認が表示されるので、 yes を入力すると更新されます。% terraform state replace-provider 'registry.terraform.io/terraform-providers/datadog' 'registry.terraform.io/datadog/datadog' Terraform will perform the following actions:  ~ Updating provider:    - registry.terraform.io/terraform-providers/datadog    + registry.terraform.io/datadog/datadogChanging 1 resources:  datadog_integration_aws.xxxxDo you want to make these changes?Only 'yes' will be accepted to continue.Enter a value: yesterraform state replace-provider コマンドを実行後に terraform init を実行すると Warning は解消されました。Terraform provider の Source の変更はあまり発生しない気はしますが、同じような事象にハマった時にはご参考になればと思います。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[変化に耐え得る esa のカテゴリ設計を徹底的に考えてみた]]></title>
        <id>https://developer.feedforce.jp/entry/2021/02/03/110000</id>
        <link href="https://developer.feedforce.jp/entry/2021/02/03/110000"/>
        <updated>2021-02-03T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:masutaka26 です。夜の散歩（意味深）に勤しむ毎日です。フィードフォースではドキュメント共有ツールには esa と Google ドキュメント1を、コミュニケーションツールには Slack を採用しています。情報共有はかなり活発で、2021/2/1 現在の esa 記事数は 81,324 です2。現在のカテゴリ構成と課題チームのスピードを上げるための大原則チームのスピードを上げるための情報整理1. Flow 型と Stock 型の記事を理解する2. 基本は Flow 型の記事にする3. 議事録カテゴリは出来るだけ作らない4. Slack に流れていく情報も Flow 型の記事にする5. 使い続けられる情報を Stock 記事として引き上げる6. 整理を頑張らないことで整理の難易度が低くなった7. esa を全ての情報の起点にするそれをチームでやるのは難しすぎない？esa は難しいesa の本当の正体esa への要望まとめesa 公式アカウントからのアドバイスコラム記事のカテゴリ整理を頑張らない理由Flow カテゴリをどこまで許容するか現在のカテゴリ構成と課題トップカテゴリは厳密にルール化されていて、これらの種類以外のカテゴリが増えることはありません。日報/2021/02/01 (月)/masutaka といった記事が置かれる曜日がないなど、型から外れた日報は小人さんによって速やかに修正される。Bot 並に早いwプロダクト1/実際には Feedmatic などのプロダクト名が入る︙プロダクトN/プロジェクト/期間が決まっている系のプロジェクトカテゴリが並ぶチーム/人事や情報セキュリティなどのチーム系カテゴリが並ぶコミュニティ/技術系や読書会などのコミュニティ系カテゴリが並ぶノウハウ/各種ツールのノウハウ系カテゴリが並ぶFeedforce Inc./会社全体に関係する記事が置かれるUsers/Templates/Archived/例えば「プロダクト1」の下は、議事録系を除き基本的に Stock 型の記事になっており、「マニュアル」カテゴリの下にはさらに 15 の子カテゴリが生えているとします。プロダクト1/議事録/開発/コンサルティング/マーケティング/インシデント/ナレッジ/マニュアル/子カテゴリ数は 15︙その他/※ 複数のプロダクトから作った架空カテゴリです。ここまでならよく整理されているように見えますが、実際は古くてメンテナンスされていない記事が多く、言ってしまえばノイズが多い状況です。情報共有されればされるほど、ノイズが増えてしまう悪循環です。Slack に目を向けると、こちらもオープンではありますが、時にブログ記事並みのメッセージが投稿されることがあります。未読メッセージを読み進めると手が止まり、読み切るのが難しいと想像されます。ブログ記事並みのメッセージ例。これで半分です。細切れの情報も多く川の流れのようで、断片化した情報をつなぎ合わせるのは難しいと思います。世の中には情報共有が足りないことに悩む組織は多いと思いますが、活発になったとしても、適切に整理されていなければその量に溺れてしまいます。一方で、最近のコロナ禍もあって 10 年前と今とでは変化のスピードは上がっています。変化に強いチーム設計、つまりは変化に強い情報整理設計が重要です。これをやらなければ、チームのスピードは徐々に下がっていくでしょう。チームのスピードを上げるための大原則私が社内で様々なプロダクトを渡り歩いた経験上、チームのスピードを上げるためには以下の 2 点が重要だと感じています。ノイズを減らす情報を一ヶ所に集めるマネージャーはこれらの阻害要因を減らす仕掛けを作る必要があります。ただ、そういったことが得意な方ばかりではありません。むしろ情報量が一番多く、一番困っているのは彼らかもしれません。マネージャーに余裕がなければ、それがボトルネックとなりチームのスピードが下がります。これからその対策を考察していきます。チームのスピードを上げるための情報整理はじめに書いておくと、出来るだけ整理を頑張らないことが重要です。1. Flow 型と Stock 型の記事を理解するまずは公式ドキュメント「記事のストック・フローの分類と検索」を読んで、Flow 型と Stock 型の記事の存在を知り、理解します。2. 基本は Flow 型の記事にする全て Flow 型にするくらいの気持ちで記事を作ります。記事が多くなっても視界に入りづらいし、整理する必要もないためです。Flow 型記事の例書き捨ての記事であれば プロダクト1/2021/02/01/タイトル、今月中は更新する記事であれば プロダクト1/2021/02/タイトル で良いと思います。ただ、さすがに今年中更新する記事は Stock 型の記事にしたほうが良いと思います。3. 議事録カテゴリは出来るだけ作らないプロダクト1/議事録/ のような議事録カテゴリはできるだけ作らず、プロダクト1/2021/02/01/〇〇会議 議事録 のようなカテゴリとタイトルにします。議事録のような使い続けない記事を視界に入れるのは、どちらかと言えばノイズだからです。社内で以下のような階層をよく見かけますが3、一回限りの会議の置き場所に悩んでしまいます。プロダクト1/議事録/〇〇会議/YYYY/MM/DD/〇〇会議プランニング/YYYY/MM/DD/プランニングプロダクト1/議事録/20210201 〇×会議 議事録 のような自由なパターンも現れたりして、さらに悩ましくなります。過去の議事録をたどる目的でカテゴリを分けたいのであれば、記事の最初に「議事録一覧はこちら」みたいな検索リンクがあれば十分でしょう。「議事録一覧はこちら」の一例それでも作りたい場合は プロダクト1/議事録/〇〇会議/2021/02/01/〇〇会議 議事録 のような記事名にして、後から プロダクト1/2021/02/01/〇〇会議 議事録 に一括変換して視界から消せるように、設計するのが良いと思います。一括変換の仕方は公式ドキュメント「記事のカテゴリを一括変更」が分かりやすいです。4. Slack に流れていく情報も Flow 型の記事にするSlack にはブログ記事並みのメッセージが投稿されることがあります。そうなりそうになったら迷わず プロダクト1/2021/02/01/○○のお願い といった Flow 型の記事を作り、その URL を Slack で共有します。他の場面で引用したい時は意外とあるものですし、内容をあとで更新したいこともあります。Slack だと特にあとからの更新には不向きです。何より当該記事をブラウザで開き、残りの Slack 未読メッセージをスイスイと読むことが出来ます。Slack に投稿されてしまうと、その長いメッセージを読みながら、他の channel に移動するのは億劫です。5. 使い続けられる情報を Stock 記事として引き上げる結局のところ、ほとんどの情報は使い続けることはありません。具体的すぎるからです。そのような具体的な情報は寝かすことで、抽象度の高い情報、例えばカテゴリ名を炙り出せることがあります。良いカテゴリ名が思いつかない時は、まだ抽象度が高くないと思うので、引き上げないほうが良いでしょう。いくつかの Flow 記事をしばらく寝かしたら、あとから汎用的な抽象度の高い Stock 記事のアイディアが湧くこともあります。6. 整理を頑張らないことで整理の難易度が低くなったここまででほとんどの記事は Flow 型の記事のはずです。冒頭に述べたノイズになるような記事は少なく、情報に溺れる確率は低いと思います。使い回しが効かない具体的な情報を Flow 型の記事に追いやることで、整理の難易度を低くすることが出来ました。7. esa を全ての情報の起点にするでは、Google ドキュメントも同じようにしましょう！とは思っていません。esa を全ての情報の起点にする勢いで、適宜 Google ドキュメントにリンクを張ると良いでしょう。Google ドキュメントのフォルダ整理は出来るだけやらないほうが良いです。時間がいくらあっても足りません。Google Drive はただの情報プールです。Google Drive 内検索も優秀です。それをチームでやるのは難しすぎない？そう思った方、正しい感覚です。「チームのスピードを上げるための情報整理」は「私が考える esa 原理主義」に振り切って書きました。情報整理のスキルに関して少数精鋭チームでないと、運用するのは難しいと思います。現在私は開発者 1 人、ビジネスメンバー 2 人のチームに所属しています。私主導でカテゴリを決めているので、これまで書いた方法で整理し、うまくいっている実感があります。Flow 型の記事に対して、Stock 型の記事を少なく保てている。そういえば初期は Flow 型の記事しか作りませんでした。esa は難しいフィードフォースでは 1 年半ほど前まで Qiita:Team を使っていました。Qiita:Team はほぼ Flow 型の記事しか書けないため、これまで話した問題は出てきませんでした。いや「表面化しなかった」が正確でしょう。esa は Stock 型の記事も書けます。ブログと wiki が合体したようなツールなので、難しくないわけがありません。wiki を書くためには抽象的思考が必要です。私の観測範囲では、半分以上の方は具体的思考に寄り過ぎているため、得意ではないという実感です。意識したことがないかもしれません。esa の本当の正体正直言って現在の esa は「情報整理のスキルに関して少数精鋭チーム」でないと、乗りこなすのは難しいと思います。これに気づいた時、Ruby という言語に似ていると思いました。Ruby は esa で採用されているプログラミング言語です。初心者はにこやかに迎えてくれますが、本番環境で使い続けるためには、コードで表現されていない振る舞いを読み解く必要があるなど、実は少数精鋭チーム向けの言語です。esa LLC は少数精鋭チームのようなので、良くも悪くも「コンウェイの法則」が働いて、そのようなサービスになったのだと勝手に想像します。esa への要望社員数 100 人オーバーが見えてくると、型（制約）の必要性を感じます。新規作成時にデフォルトカテゴリが Flow 型になるような、型を設定できると良い？例: プロダクト1/ 以下での新規作成は、デフォルトカテゴリが プロダクト1/YYYY/MM/DD/ になるフレームワーク的に、何らかのパターン以外のカテゴリを作れないようにする？第一階層カテゴリ以下で、そのようなパターンを数種類から選べるようにする？どれも難しい話ですかね...？現在社内で抱えている課題として「記事整理にハードルがある」は間違いなくあると思います。たくさんの記事を移動すると、/posts がその情報で埋まる4移動しただけなのに、記事に自分の小さなアイコンが付くそもそも記事を 1 つ 1 つ移動するのが大変タイトルに入れてしまった日付をカテゴリにして...とかまですると、API を使わざるを得ない出来れば Windows のエクスプローラのようなカジュアルさで、且つ履歴が残るとうれしいです。あと、要望したことはありますが、現在の「カテゴリ以下の記事全て」に加えて、「カテゴリ直下の記事」「アーカイブした記事」を切り替えるような機能が欲しいです。イメージとしては、GitHub の UI に「カテゴリ直下の記事」を加えたものです。In, On, Archived を切り替えられるイメージ「カテゴリ直下の記事」は on:カテゴリ で検索できますが、知っているユーザーはごく僅かです。アーカイブの認知も怪しくて、古い記事が残る要因かもしれません。一部カテゴリではこんな工夫をしています。アーカイブという手段を認知させている例難しいと思いますが、情報整理のスキルがそれほど高くなくても使える UI 設計をお願いしたいところです。🙏もし、もっと詳しく私にヒアリングしたいなどあれば、masutaka@feedforce.jp または @masutaka の DM までお知らせ下さい。まとめ私から見える、フィードフォースで抱えている情報整理の課題をまとめ、チームのスピードを上げるための整理方法を提案しました。ただ、それは「私が考える esa 原理主義」に振り切っているため、チームで採用するのは難しいと思います。カテゴリがデフォルト Flow 型になるなど、情報整理を頑張らず済む使い勝手になると、とてもうれしいです。esa 公式アカウントからのアドバイス新規作成時にデフォルトカテゴリが Flow 型になるような、型を設定できると良い？こちらのアドバイスを頂いたので早速試しました。良さそうです！あるカテゴリ配下で "Create a new post" ボタンで記事を新規作成するときに、カテゴリが一致するFlowのテンプレートがあれば、そのテンプレートが自動的にプリセットされるという機能があります。（続く→）— esa_io (@esa_io) 2021年2月3日  例えば以下のテンプレートがあったとして- Templates/カテゴリ1/YYYY/MM/DD/テンプレートA- Templates/カテゴリ1/サブカテゴリ1/YYYY/MM/DD/テンプレートB「カテゴリ1」配下で記事作成→ テンプレートA 「カテゴリ1/サブカテゴリ2」配下で記事作成 →テンプレートB がそれぞれ自動適用されます— esa_io (@esa_io) 2021年2月3日  後日アンサー記事も頂きました！「所感」にこの記事の感想が書いてあります。ProTip/2021/02/03/特定のカテゴリ配下の記事作成時に、テンプレートを適用する - docs.esa.ioコラム自分の思いを書ききれないので、コラムに逃しました。(^^;記事のカテゴリ整理を頑張らない理由こんな理由からです。歴史が証明しているesa の検索がそこそこ優秀今では信じられないかもしれませんが、1990 年代の Yahoo! JAPAN は、人間のスタッフがウェブサイトの情報を収集してカテゴリ分類して登録する、ディレクトリ型検索サービスでした5。今はロボット型検索が使われているのは周知のとおりです。そのため Stock 型の記事だけを作り続けるといずれ破綻することは、1 歴史が証明していると言えます（やや大げさ）。2 については、公式ドキュメント「記事の検索方法」にある検索クエリで、それなりに検索できます（諸説あり）。「キーワード検索しやすい記事にするコツ」も参考にすると良いでしょう。Flow カテゴリをどこまで許容するかFlow カテゴリをどこまで許容するかは、悩ましいところです。(a) 制約をつけずに自由に作ることを許容するか。プロダクト1セールス...YYYYMMDD開発○○機能YYYYMMDDYYYYMMDDYYYYMMDD(b) 原理主義っぽく、１つの Flow カテゴリしか許容しないか。プロダクト1セールス...開発...YYYYMMDD(a) は職種横断的な情報共有を重視しないチームに合うと思います。中規模以上のチームです。(b) は職種横断的な情報共有を重視するチームに合うと思います。小さなチームです。はじめは (b) で作ってみて、やりづらくなってきたら (a) にすると良いでしょう。ただし、(a) の プロダクト1/開発/○○機能/YYYY/MM/DD/ まで細かく Flow カテゴリを作るのは、把握が難しくなりそうですので、オススメはしません。Google Workspace（旧称 G Suite）を契約しています。↩現在のメンバー数は 90、最古の記事は 2014/4/3 です。↩過去に推奨したことあります。ｽﾏﾇｽﾏﾇ…。↩Archived/ 以下への移動は更新日時が変わらず /posts に現れないので、カテゴリ一括変更と組み合わせれば、一応回避はできます。↩今回の件で調べたところ、その「Yahoo!カテゴリ」は2018年3月29日まで動いていたことを知りびっくりしました↩]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ruby 3.0 の「キーワード引数の分離」で委譲用のメソッドが壊れた場合の対応方法]]></title>
        <id>https://developer.feedforce.jp/entry/2021/01/29/121252</id>
        <link href="https://developer.feedforce.jp/entry/2021/01/29/121252"/>
        <updated>2021-01-29T03:12:52.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、id:daido1976 です。Ruby 3.0 の「キーワード引数の分離」が原因で委譲用のメソッドが壊れた場合、特に Ruby 2.6 ~ 3.0 で互換性を保ちながら対応する場合の日本語記事が見当たらなかったので、書かせていただきます。インスタンスメソッドが壊れた場合以下のコードは Ruby 3.0 より前のバージョンでは動きますが、3.0 からは MyClass#other_method の実行時に ArgumentError が起きるようになります。（2.7 の時も警告が出ます）class MyClass  def delegating_method(*args)    other_method(*args)  end  def other_method(value:)    p value  endendMyClass.new.delegating_method(value: 'value')こちらは公式の Separation of positional and keyword arguments in Ruby 3.0#a-compatible-delegation や ruby2_keywords gem や Module#ruby2_keywords を参考に以下のようなコードで対応できます。class MyClass  def delegating_method(*args)    other_method(*args)  end  def other_method(value:)    p value  end  # この 1 行を追加する  ruby2_keywords :delegating_method if respond_to?(:ruby2_keywords, true)endMyClass.new.delegating_method(value: 'value')ポイントは Ruby 2.7 や 3.0 でも旧来のスタイルで実行できるようにするための Module#ruby2_keywords が Ruby 2.6 以前では定義されてないので、Object#respond_to? などでそのケアをしてあげる必要があることです。クラスメソッドが壊れた場合こちらがこの記事を書いた目的です。サンプルコードに現実味を持たせるためにインスタンスメソッドの例とは少し変えています。クラスメソッドで引数を受けて、そのまま委譲して初期化するようなコードです。このコードも Ruby 3.0 からは ArgumentError が発生します。class MyClass  def self.delegating_method(*args)    new(*args)  end  def initialize(value:)    p value  endendMyClass.delegating_method(value: 'value')こちらはインスタンスメソッドの場合と違い、以下のように class << self で特異クラスをオープンしてから、Module#ruby2_keywords の対応をしてあげる必要があります。class MyClass  def self.delegating_method(*args)    new(*args)  end  def initialize(value:)    p value  end  # この 3 行を追加する  class << self    ruby2_keywords :delegating_method if respond_to?(:ruby2_keywords, true)  endendMyClass.delegating_method(value: 'value')定義されたクラスメソッドの居場所が分からなくて、久しぶりに『メタプログラミングRuby』の 5 章を開きました。Also see. https://docs.ruby-lang.org/ja/latest/doc/spec=2fdef.html#singleton_class【おまけ】Ruby 2.6 ~ 3.0 で互換性を保たなくて良い場合公式の Separation of positional and keyword arguments in Ruby 3.0#delegation-ruby-3 の通りにやれば OK です。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google MyBusiness APIのGoクライアントを生成する]]></title>
        <id>https://developer.feedforce.jp/entry/2021/01/12/120000</id>
        <link href="https://developer.feedforce.jp/entry/2021/01/12/120000"/>
        <updated>2021-01-12T03:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、 id:kogainotdan です。現在私は新規事業開発のチームで、飲食店支援のサービスの可能性を探索しています。飲食店、ひいてはローカルビジネスの運営において、集客施策として重要とされるのがGoogleマイビジネスの活用です。ナレッジパネルやローカル検索において表示されている情報は、ユーザーとしても見たことがある方が多いと思います。喫緊の話題としては、緊急事態宣言の前後における営業時間や営業形態の変更の正確な反映が、見込み顧客の取りこぼしを防ぐと共に店舗自体への信頼感の醸成にも役立つものと考えられます。そんなGoogleマイビジネスですが、Googleの他のサービス同様にAPIが存在します。ただし、まだGeneral Availableではない様子で、フォームによる利用申請が必要となるのですが、Googleマイビジネスの管理画面で出来る操作の多くをサポートしており、ローカルビジネス関連のサービスを開発する際に重要なパーツとなり得ると思われます。さて、そんなGoogle MyBusiness APIですが、公式に提供されているクライアントライブラリがJava, PHP, C#の3種となっています。他のGoogleのサービスと同様にDiscoveryドキュメントが提供されていますので、この3種以外の任意の言語のクライアントライブラリを生成することは可能と思われます。そこで本稿では、Google MyBusiness APIのGolangクライアントライブラリの生成までのステップを記録したいと思います。Google API Discovery Service公式ガイドの導入ドキュメントに沿ってAPIを有効化し、OAuth 2.0 の認証情報を取得すると、APIリクエストが発行出来るようになります。(認証情報の取得は他のサービスと同様なので割愛します)なお、APIの利用申請はorganization毎に1つだけ許可されるようですのでご注意ください。さて、前述のようにGoogle MyBusiness APIのクライアントライブラリは、Java, PHP, C#の3種のみ提供されています。この3種のいずれかでアプリケーションを開発する手も無くはないですが、すでに稼働している別の言語のコードがある場合など、APIの利用のためだけに別の言語のランタイムを導入するのは大仰に過ぎるというケースもあるかと思います。(白状すると、私はこの3種のどれも書いたことがありません)ところで上記のクライアントライブラリの紹介ページには、以下のようなことも書かれています。Google My Business API Discovery ドキュメントは、この API の個々のバージョンのインターフェースについて解説しています。このドキュメントは Google API Discovery Service とともにご利用ください。Swagger/Open API SpecificationやGraphQLなど、Web APIのクライアントは形式化された仕様から生成するのが一般的になっています。Google API Discovery Serviceのサイトに飛ぶと、Use the Google API Discovery Service to build client librariesThe Discovery API provides a list of Google APIs and a machine-readable "Discovery Document" for each API.とあり、これもそういった仕様の一つであることが伺えます。Google関連のAPIクライアントライブラリここでGoogleが提供する各種サービスのAPIクライアントライブラリが集積されたレポジトリを見てみましょう。何らかの形でこのDiscoveryドキュメントが利用されていることが見て取れます。Goほど分かりやすい形ではないですが、Rubyや.NETでも、同様に利用されていることが伺われます。Google MyBusiness APIのドキュメントに示されている通り、このレポジトリにはGoogle MyBusiness APIのクライアントライブラリは置かれていません。ただしSupport for Bussiness Messaging private API? #597にIf you want to try out the generator and see if it works for you can.You would need to clone this repo and generate from within it as the code it generates will rely upon some internal packages here.という回答が付いています。internal packageを利用する必要があるものの、Discoveryドキュメントさえあれば、個々の開発者がクライアントライブラリを生成する道はありそうです。Google MyBusiness API クライアントライブラリの生成上述のIssueから参照されているAdd instructions to generate services yourself #283にあるように、同じレポジトリにコードジェネレータも置いてあるようです。実態としてはGoのmain packageのようですので、forkしてGoogle MyBusiness APIのDiscoveryドキュメントを食わせて上げれば良さそうです。公式に公開されたツールというわけではないようなので、ドキュメントなどは見当たりませんが、--helpオプションで実行してあげるとサポートされているオプションが確認出来ます。  -api string        The API ID to generate, like 'tasks:v1'. A value of '*' means all. (default "*")  -api_json_file string        If non-empty, the path to a local file on disk containing the API to generate. Exclusive with setting --api.  -api_pkg_base string        Go package prefix to use for all generated APIs. (default "google.golang.org/api")  -base_url string        (optional) Override the default service API URL. If empty, the service's root URL will be used.  -build        Compile generated packages.  -cache        Use cache of discovered Google API discovery documents. (default true)  -copyright_year string        Year for copyright. (default "2021")  -discoveryurl string        URL to root discovery document (default "https://www.googleapis.com/discovery/v1/apis")  -gendir string        Directory to use to write out generated Go files (default "/Users/kogai/.gvm/pkgsets/go1.14/global/src/google.golang.org/api")  -gensupport_pkg string        Go package path of the 'api/internal/gensupport' support package. (default "google.golang.org/api/internal/gensupport")  -googleapi_pkg string        Go package path of the 'api/googleapi' support package. (default "google.golang.org/api/googleapi")  -header_path string        If non-empty, prepend the contents of this file to generated services.  -htransport_pkg string        Go package path of the 'api/transport/http' support package. (default "google.golang.org/api/transport/http")  -install        Install generated packages.  -internaloption_pkg string        Go package path of the 'api/option/internaloption' support package. (default "google.golang.org/api/option/internaloption")  -option_pkg string        Go package path of the 'api/option' support package. (default "google.golang.org/api/option")  -output string        (optional) Path to source output file. If not specified, the API name and version are used to construct an output path (e.g. tasks/v1).  -publiconly        Only build public, released APIs. Only applicable for Google employees. (default true)Add instructions to generate services yourself #283で指摘されているapi_jsonなどのオプションも確認出来ます。GitHub Actionsを使ったShopify テーマの自動デプロイ環境構築という記事でも触れたのですが、私の管理しているmonorepoレポジトリはBazelでビルドターゲットを管理していますので、Google MyBusiness APIのクライアントライブラリも、ビルド単位の一つとして扱っています。# WORKSPACE# Discoveryドキュメントを取得するhttp_file(    name = "mybusiness_api_go_schema",    downloaded_file_path = "mybusiness.json",    sha256 = "084e68d0fc746fe9d0ba105f6878b8eb208181f5ee1288e79c35a11684ec4a13",    urls = ["https://developers.google.com/my-business/samples/mybusiness_google_rest_v4p7.json"],)# forkしてくる代わりにcloneしておくnew_git_repository(    name = "google_api_go_client",    # non-bazelレポジトリなので、カスタムのビルドファイルを作っておく    build_file = "//:google_api_go_client.bazel",    commit = "074c16e73361434fc3d1f6ef62585d57b70a9d1b",    remote = "https://github.com/googleapis/google-api-go-client.git",    shallow_since = "1608641643 +0000",)# google_api_go_client.bazel# ビルドに必要なのはinternal packageだけexports_files(    [] + glob(["internal/**/*.go"]),)load("@io_bazel_rules_go//go:def.bzl", "go_binary")load("@build_bazel_rules_nodejs//:index.bzl", "generated_file_test")filegroup(    name = "gensupport",    srcs = [        "@google_api_go_client//:internal/gensupport/buffer.go",        "@google_api_go_client//:internal/gensupport/doc.go",        "@google_api_go_client//:internal/gensupport/json.go",        "@google_api_go_client//:internal/gensupport/jsonfloat.go",        "@google_api_go_client//:internal/gensupport/media.go",        "@google_api_go_client//:internal/gensupport/params.go",        "@google_api_go_client//:internal/gensupport/resumable.go",        "@google_api_go_client//:internal/gensupport/retryable_linux.go",        "@google_api_go_client//:internal/gensupport/send.go",        "@google_api_go_client//:internal/gensupport/version.go",    ],)# gazelle:ignorego_binary(    name = "codegen",    args = ["-api_json_file=$(location @mybusiness_api_go_schema//file:mybusiness.json)"],    data = [        ":gensupport",        "@mybusiness_api_go_schema//file:mybusiness.json",    ],    embed = [        "@org_golang_google_api//google-api-go-generator",    ],    visibility = ["//visibility:public"],)generated_file_test(    name = "json",    src = "//packages/google-mybusiness-api-go/mybusiness/v4:mybusiness-api.json",    generated = "@mybusiness_api_go_schema//file:mybusiness.json",)当初は単純にMakefileにcurlの実行などを書いていたのですが、Discoveryドキュメントを都度取得・クライアントライブラリを生成しているとCIが重くなるので、で適宜キャッシュさせるためにもBazelが効いています。mybusiness/v4:    npx bazelisk run -- //google-mybusiness-api-go:codegen \        -gendir="$(CURDIR)" \                # internal以下に置かれたpackageではなく、手元に移したpackageを使う        -gensupport_pkg="github.com/your-organization/your-repository/packages/google-mybusiness-api-go/mybusiness/gensupport"    cp -r $(BZL_BIN)/$(PKG)/codegen_/codegen.runfiles/google_api_go_client_internal/internal/gensupport $(CURDIR)/mybusiness最後のgenerated_file_testはクライアントライブラリの生成には直接関係しないのですが、同じレポジトリでTypeScriptのコードも管理している関係でJavaScript関係のBazel Ruleが導入されています。ついでだったので、Discoveryドキュメントのスナップショットテストのようなこともしています。まとめGoogle MyBusiness APIのGoクライアントライブラリの生成について書いてみました。何かの参考になれば幸いです。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[半期に1度の Engineer’s Principles Award 受賞者を紹介します]]></title>
        <id>https://developer.feedforce.jp/entry/2020/12/28/131042</id>
        <link href="https://developer.feedforce.jp/entry/2020/12/28/131042"/>
        <updated>2020-12-28T04:10:42.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。人事の今岡と申します。Developer Blog への投稿は初めてで若干緊張しています。先日行われたオンライン忘年会にて、半期に一度の「Engineer’s Principles Award 2020 Winter」の受賞者が発表されました。アワードが始まって3回目となり、そろそろ社外にも発信していきたいなと思いまして、せっかくなのでこちらのブログでアワードを受賞したメンバーと表彰内容をご紹介しようと思います。Engineer’s Principles Award とはEngineer’s Principles とは、フィードフォースの開発メンバー向けに現場が主体となって設定した、5つの行動指針です。半期に一度、開発メンバー同士で投票を行い、行動指針の項目ごとに最も体現しているメンバーが選ばれ、開発本部長から表彰者が発表されます。ちなみに、下記の5項目はつい先日アップデートされたばかりで、今年導入されたフルリモートワークという働き方も考慮した内容になりました。この話はまた別の機会に改めて発信しようと思います。Stay Humble; 常に謙虚であるべしBe Positive & Proactive; 常に肯定的・主体的であるべしBe Prepared; 常に来るべき機会に備えるべしShare All; 己の知見、試行、失敗、遍く共有すべしJust Do It; 全力でやりきるべし受賞者紹介※表彰コメントは本来社内向けのものであるため一部変更させていただいています。人によって各種アカウントを載せています。🏆「Stay Humble; 常に謙虚であるべし」受賞者@tmd45 さん表彰コメント：＠azmin さん表彰コメント：🏆「Be Positive & Proactive; 常に肯定的・主体的であるべし」受賞者@kogai さん表彰コメント： 茶碗 (@iamchawan) | Twitter,  Blog@sukechannnn さん表彰コメント： sukechannnn (@sukechannnn) | Twitter🏆「Be Prepared; 常に来るべき機会に備えるべし」受賞者@kogai さん表彰コメント： 茶碗 (@iamchawan) | Twitter,  Blog@namikingsoftさん表彰コメント：🏆「Share All; 己の知見、試行、失敗、遍く共有すべし」受賞者@masutaka さん表彰コメント： Takashi Masuda (@masutaka) | Twitter,  Blog@kogai さん表彰コメント： 茶碗 (@iamchawan) | Twitter,  Blog🏆「Just Do It; 全力でやりきるべし」受賞者@ryz310さん表彰コメント： サトウリョウスケ (@ryosuke_sato) | Twitter,  Blog@koshigoe さん表彰コメント：周囲の賞賛・承認を共有するよい機会に以上、延べ10名の受賞者でした。なんと3つの項目で表彰されている人もいましたね！表彰コメントは、開発メンバー同士の投票時に自由記述できるコメントがもとになっているので、周囲からの賞賛・承認の声を全社で共有できるよい機会となっています。なかなか近くで仕事ぶりを見ることがないビジネスサイドのメンバーにとっても開発メンバーの活躍を知ることができ、よい刺激をもらうことができました。このような形で開発チームメンバーの活躍や勉強会の様子なども発信していきたいと思います。気づけば2020年もあと3日ほど。今年も1年間ありがとうございました。そして受賞者のみなさん、おめでとうございました！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shopify アプリのリスト設定ポイント]]></title>
        <id>https://developer.feedforce.jp/entry/2020/12/23/093000</id>
        <link href="https://developer.feedforce.jp/entry/2020/12/23/093000"/>
        <updated>2020-12-23T00:30:00.000Z</updated>
        <summary type="html"><![CDATA[この記事は Shopify開発を盛り上げる（Liquid, React, Node.js, Graph QL） Advent Calendar 2020 の 23 日目の記事です。昨日は @hal_256 さんの『Next.js+GrapqhQLでShopifyアプリを開発する』でした！型が用意できるっていいですよね。ごきげんよう！ id:tmd45 と申します。よろしくお願いします。同僚たちが 4 日目、8 日目と記事を書き、こっそり（？）11 日目も書かれていたので、これで 4 本目の弊社記事になります。がんばります。とはいえ、これまでとても濃い記事がたくさん書かれていたので、ここらで軽めに、アプリのリスト設定で引っかかったポイントなんぞを共有したいと思います。アプリのリストとはShopify の公開アプリをアプリストアに掲載するための情報を、設定する場所です。アプリのリスト設定公開アプリを実際に公開するには、このリストの設定を行ったうえで、Shopify からのレビューを通らなければいけません。項目もたくさんあるので、ここではひとつひとつの説明はしません。アプリ名初っ端ですが、エンジニアが対応しているとうっかりしがちなポイント その１。A. Listing information、1. App information の「App name」ここを書き忘れることはまずありませんが、この値は、基本である「アプリ設定」の「アプリ名」と同じ値になっている必要があります。アプリ設定の「アプリ名」アプリ名に "本番環境" と入れているとか、ストア情報（リストの内容）でかっこいいアプリ名を付け直すとかして、名前が一致していないとレビューの自動チェックで弾かれます。アプリアイコン初っ端ですが、エンジニアが対応しているとうっかりしがちなポイント その２（笑）A. Listing information の「2. App icon」リストの設定はしたものの…アプリ設定の「アプリのアイコン」こちらも「アプリ設定」の「アプリのアイコン」を設定し忘れていた…なんてことがありました。また、この２つの画像は指定サイズは異なるものの、基本的には同じ内容の画像を設定する必要があります。開発中に適当なアイコンを設定していて、ストア情報にだけリッチなアイコンを後から作った場合なども、差し替えをお忘れなく。スクリーンショット必須項目のなかにスクリーンショットの添付があります。ただ画面のスクリーンショットを貼ればいいというものではなく、かなり細かいガイドラインが定められています。リストの設定ページからもリンクされているので確認しましょう。画像サイズや縦横比率はわかりやすいですね。代替テキストが必要というのも、そもそもリスト設定のページに項目があるのでまず入れ忘れることは無いと思います。画像そのものの注意点ですと、以下のようなものが引っかかりやすいかなと思います。スクリーンショットにデスクトップの背景や ブラウザウィンドウを含めないでください 。画像が乱雑になったり、利用者の気を散らしたりしないように、それらをトリミングしてください「埋め込みアプリ」（Embedded App）のスクリーンショットに、ストア名、検索バー、ユーザ情報など Shopify 管理画面（ストア管理画面）のヘッダーコンテンツを含めないでください。左側のナビゲーションは含めることができますストアのヘッダー部分スクリーンショットを丁寧に撮っていても、後者のストアのヘッダー部分は含めてしまいがちだと思いますので、気をつけましょう。詳細な説明B. App details の「5. Detailed description」の部分です。アプリストアのアプリ詳細ページで、「（アプリ名）の詳細情報」として表示される項目です。アプリの詳細情報こちらでついやってしまいがちなのが、とにかく「アプリの機能の説明」以外のことを書いてしまうこと。ガイドラインでも使用してはいけない内容が事細かに書かれていますのでチェックしましょう。"このアプリで何ができるのか" を書くようにすれば問題ないのですが、日本人気質なのか「困ったことがあったらサポートします」みたいな一文を入れるのも NG です（やりました）。それはこのアプリの "機能" の話ではないですよね！スクリーンショットにも見えるように、サポートに関してはストアの定型で項目が用意されているので、そちらで我慢（？）しましょう。おわりにというわけで、リストの設定のなかでもストアに表示される基本的に部分についてポイントをあげてみました。項目が多くて大変ですが、区分けがしっかりしているおかげで適切な粒度で、アプリ利用者へ必要な情報を届けられるこういった仕組みが、とてもエコだなと思います（エコシステム的な意味で）。アプリレビューを乗り越えたらついにアプリ公開です！ﾌｧｲﾄｫ･.*:+:(n'∀')η:+:*.･]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[2020年 買ってよかったもの]]></title>
        <id>https://blog.tmd45.jp/entry/2020/12/14/133631</id>
        <link href="https://blog.tmd45.jp/entry/2020/12/14/133631"/>
        <updated>2020-12-14T04:36:31.000Z</updated>
        <summary type="html"><![CDATA[この記事は feedforce Advent Calendar 2020 の 14 日目の記事です。昨日は同じプロダクトチームでマーケティングを担当してくれている、ねこにしさんによる『その文章に、読者はいるか？』でした。お仕事でもねこにしさんらしい自然体の文章と思っていましたが、その裏ではいろいろ考えることがあるんですね。そんな素敵な記事のあとでナニですが、わたくし @tmd45 からは個人的な散財の記録をお届けします。通称、散財部活動報告です。2020年は生活が激変した年もはやテンプレのような話ですが、自分も例にもれず新型コロナウィルス対策の影響で、現在はフルリモート環境で仕事しています。とはいえ、自宅のパソコン環境はもともと整っていたので、そういう意味で新しく買い揃えたものはありませんでした。過去の散財記録を見るともう椅子とかディスプレイとか買ってあって、それそのものが「買っておいてよかったなー」と思いました。とはいえ本格的にミーティングなどを通話でするようになって、マイクとイヤホンは買いました。単一指向性USBコンデンサマイクFIFINE USBマイク コンデンサーマイク 単一指向性 マイクスタンド(アームスタンドと三脚スタンド付き) ポップガード付き マイクセット ABタイプ USBケーブル PC用マイク Skype 録音 ゲーミング ライブ配信 ゲーム実況 在宅勤務 Windows Mac PS4対応 高音質マイク T669メディア: エレクトロニクスいきなりゴツいですが、快適なフルリモートワークにはコミュニケーションツールへの投資が欠かせません。単一指向性で雑音を減らし、アームスタンドで取り回しや位置取りが柔軟にできるようになって、コスパ良かったと思います。ポップガード（風防）は使っていません。会社でも同僚におすすめしまくって 2, 3 人に買ってもらいました（笑）特別なドライバインストールも不要で、Mac と Windows、あと PS4 にも接続して使ってみましたが、どれもいい感じでした。骨伝導イヤホンAfterShokz Aeropex コスミックブラック 骨伝導ワイヤレスヘッドホン AFT-EP-メディア:イヤホンはインイヤー（カナル型）派なんですが、嘘か真か長時間使用で難聴とかいう噂を聞いて、実際、長時間付けていると疲れを感じていたので、社内で話題になっていた骨伝導イヤホンを買ってみました。いまのところ、いい感じです。どんなイヤホンでも長時間つけっぱなしだと疲れるのは変わりないので、こまめに電源切って外してます。接地面に髪の毛がはさまってると聞こえづらかったり、大きな音がするとちょっと頭がクラっとする気もします。お安くもないので、機会があったら試してみてから買ったほうがいいと思います（このご時世だとそれも難しいですけど…）。足置き・足枕足置き 足枕 低反発 足まくら 足休め 足用クッション 体圧分散 半円形 足楽 足むくみ 車用 新幹線 飛行機用 旅行用 オフィス エコノミークラス座席をフラット 敬老の最適プレゼントメディア:そういえば地味ですが、リモートワークになってから足置きを買いました。これまで足元の電源ボックス（電源タップとかをまとめてしまっている箱）に足を乗せていたのですが、通話中に蓋を踏み抜いて大騒ぎすることが何度かあったので（すみません…）これも買ってよかったなと思います。キーボードカバーmoshi ClearGuard MB for MacBook Pro 13/16 キーボードカバー (欧州EUキーボード用) ※13インチは2020年発売の型のみ対応 静かな打音 Touch Barも保護 ぴったりフィットの極薄0.1mm 洗って貼り直せる 無害素材 製品登録で10年グローバル保証メディア:日本国内で EU キーボード対応商品買うのなかなか選択肢が少なくて大変です。会社で使ってる MacBook が諸事情で EU キーボードでして(；・∀・) ※通常は US か JIS で選べます！また、いままで幸いやらかしたことはないんですが、自宅で Mac 使っててコーヒーこぼしたりするのが怖いので、キーボードカバーを買いました。ちゃんとサイズが合ってるので、いい感じに使えてます。ぺたっとくっ付くわけではなく、ふわっと被ってるだけなので、指が汗ばんでくるとちょっと浮いたりはしますが、慣れですね。Mac そのものをぶっ壊す恐怖からは守ってくれるのでこれも買ってよかったです。あとはコロナとかリモートとか関係ないやつほんとにただ気が向いて買っただけのもの。電動歯ブラシフィリップス ソニッケアー ダイヤモンドクリーン スマート 電動歯ブラシ ホワイト HX9934/05発売日: 2017/09/01メディア: ホーム&キッチンなにかの折に友人が「電動歯ブラシ使ってるよー」という話をしていて、ふと調べてみたら、ソニッケアーがスマホアプリと連動してる…！というネタ半分で買いました。結果的に（スマホアプリも面白かったですが）（普通に）買ってよかったなと思ったものランクインです。振動で磨き時間をコントロールしてくれたりして、キレイに磨けて、磨き時間は短縮されました。マッサージクッションアテックス ルルド マッサージクッションダブルもみスリム ブラウン AX-HCL258brメディア: ホーム&キッチンしゃれおつなファブリックでお部屋に置いてもおっさんな雰囲気漂わないマッサージクッションで有名なルルドのやつです。古い型のものがお釈迦になったので買いました。モミ玉が 2 → 4 つに増えてごりごりマッサージしてくれます。小型化されたので普段座ってる椅子でも無理なく使えるようにもなりました。オフィスにいたときより、休憩時に身体を動かさなくなったので、これでほぐしてます。コーヒー淹れる系BRUNO 電動ミルコーヒーメーカー BOE080 [ カーキ ]メディア:Kslong コーヒーポットコーヒー ケトルステンレス 細口ハンドパンチポットドリップih対応長い口ポット ファイン口ポット グースネックポット (ブラック, 350ml)メディア: ホーム&キッチンこれまで会社ではインスタントで済ませていたコーヒーですが、自宅にいるようになって PostCoffee® のサブスクリプションなんかも始めました。だんだん慣れてきたので、最近コーヒーバッグから豆に切り替えて、自分で挽いて淹れるようになりまして。そのために買ったのがこの２つです（計量スプーンとかミル掃除用のブラシとかも買いました）。朝のおいしいコーヒーで目を覚ましてから仕事にとりかかるのが最近のルーチンです ☕スマートリモコン Nature Remo 3Nature スマートリモコン Nature Remo 3 ネイチャーリモ Remo-1W3 Alexa/Google Home/Siri対応メディア: Tools & Hardware先日 AWS 関連の障害に巻き込まれていた Nature Remo ですが、その障害きっかけで新しい 3 が出たことを知りました。ずっと mini とか買おうか迷ってたんですが、3 でセンサー類が増えたりしたみたいなので、これも面白そうだと思って勢いで買いました。いまは "寝る前に部屋の電気とエアコンを消して、Relax Melodies アプリを起動する" みたいなショートカットを活用してます。部屋から出ないのと、猫がいるので人感センサーは扱いようが難しいです。旧来のリモコンでもまったく困ってなかったですが、同僚曰く「こういうのはロマンですから」とのことで、私もそう思います。おわりさてさて、雑多にご紹介しましたが、なにかピンとくるものがあったりしたでしょうか。自宅にいる時間が伸びたからこそ、生活を豊かにできるものを揃えて楽しく過ごしたいですね。ちなみにこの記事を書くために Amazon の購入履歴をみたら 2020 年の注文件数が 259 件もあって腰抜かしました。よく見たら Kindle で漫画を買うようになったからでしたね。やー、びっくりびっくり（棒読み）明日はアジャイルお兄さん pokotyamu がきっとエモい話をしてくれると思います。では、皆様良いお年を。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[私の好きなプログラミング動画 10 選]]></title>
        <id>https://developer.feedforce.jp/entry/2020/12/13/003657</id>
        <link href="https://developer.feedforce.jp/entry/2020/12/13/003657"/>
        <updated>2020-12-12T15:36:57.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。id:daido1976 です。この記事は Feedforce Advent Calendar 2020 の 12 日目の記事です！昨日は @sukechannnn の モブプロしてたらチームが大きく成長した話 でした。社内の技術勉強会でも発表してくれたのですが、みんなの関心が高い話題だったので発表後の議論が大変盛り上がっていました！ちなみにこの記事は YouTubeにあるライブコーディング動画を見て Rust を学ぼう という記事にインスパイアされて書いています。はじめに好きな動画 10 選George Hotz のライブコーディング低レイヤーガールHigh Engineermayah 氏の「実践！コーディング面接」メルカリ社の開発ライブ実況The Coding Train のソートアルゴリズム解説AWS Black Belt Online SeminarAuthlete 社による OAuth & OIDC 入門予備校のノリで学ぶ「大学の数学・物理」t-kihira 氏の「テトリスを1時間強で作ってみた」最後にはじめに私はプログラミングに関する動画を見るのがとても好きです。集中できる時は手を動かしたり本を読んだりした方が学習効率は良いと思うのですが、動画はご飯食べながら、お酒を飲みながら見られるのがとても良いですね。また、新しい技術を触る時などは手を動かす・本を読む前に動画でさらっと概要を掴んでから始めると、立ち上がりがスムーズになる気がします。好きな動画 10 選George Hotz のライブコーディングComma.ai のハッカー George Hotz のライブコーディング動画Vim でプラグインほぼ使わずに print デバッグしながら書き殴る、それでも並の人を凌駕するコーディングの速さハマりまくるけど全部力技でガンガン解決していくのが見ていて気持ちいい、そして公式ドキュメントより Stack Overflow ガンガン見るのが意外でした動画が 10 時間もあってその間ずっとコーディングしてるのがすごすぎます…！低レイヤーガールwww.youtube.com@d0iasm 氏、@hikalium 氏、@saho_bofffff 氏によるライブコーディング & 雑談動画ゆるふわな語り口でコーディングを進めていますが、内容はゴリゴリの低レイヤープログラミング少しでも気を抜けば置いてかれてしまうのでいつも正座して見ていますHigh Engineerwww.youtube.com@dowanna6 氏による Web エンジニア経験 2 年目未満の方に向けた各技術の解説動画「Cookie」や「CORS」など Web プログラミングを行う上で必須となる知識を解説しています既知の内容もありますが、基本的な概念を他の人にわかりやすく伝えるにはどうすれば良いか？の参考にしたくて見ていますmayah 氏の「実践！コーディング面接」@mayahjp 氏によるコーディング面接の実践動画mayah 氏のヒントの出し方と追加質問がとてもリアルで参考になります今はまだ一つしか実践動画がアップされてないので、今後の更新をとても楽しみにしているチャンネルの一つですメルカリ社の開発ライブ実況お題は「Go で Todo アプリ用の API を作ってみましょう」と、大変シンプルカスタマイズしまくりの Vim で高速コーディングする人、Dockerfile から書き始める人、gqlgen 使って GraphQL API を作り始める人、Go Playground で書き始める人など様々で楽しいです他の実況動画は こちら から見られますThe Coding Train のソートアルゴリズム解説テンション高めにソートアルゴリズムを解説してくれる動画（他にも色んな種類の解説動画があります）上記の動画ではバブルソート、クイックソートのアルゴリズムを概要〜実際の実装に至るまで大変わかりやすく解説してくれています丁寧に図を描いて説明してくれるので英語がわからなくても雰囲気で理解できると思いますAWS Black Belt Online SeminarAWS のサービス別の説明動画（上記は CloudFront の例）AWS はドキュメントも充実していますが、動画も併用するとさらに理解度が深まりますちなみにサービスと動画の一覧は AWS サービス別資料 から探すと良いですAuthlete 社による OAuth & OIDC 入門今まで見た中で一番わかりやすい OAuth & OIDC（OpenID Connect）の解説動画ですOAuth を学ぶにはこの動画を見た後に OAuth 徹底入門 の 3 章を参考に OAuth クライアントを自分で実装してみるのが一番手っ取り早いと思っています予備校のノリで学ぶ「大学の数学・物理」数学が苦手なのにプログラマをやっているのがずっとコンプレックスで、少しずつ学習を始めています高校数学で挫折した自分にもこの動画はものすごく分かりやすくて感動しました…！t-kihira 氏の「テトリスを1時間強で作ってみた」【プログラミング】テトリスを1時間強で作ってみた【実況解説】10 年前の動画ですがライブコーディング系の動画の中ではこれが一番好きですWindows プログラミングも C++ も全然わからないんですが、そんなの関係ないぐらいテンションの上がる動画です。多分今まで 20 回ぐらい見ていますエディタのインストールから始めるところやペイントでテトリスのブロックを自作する辺り、最高にプログラマだなって感じがします最後に途中から「これはプログラミング動画なのか？」というものも含まれていますが、お気に入りなので外せませんでした。ご容赦ください。会社の技術ブログなのを忘れて好き勝手に書いてしまいましたが、この記事を読んでくださった方にとってお気に入りの動画が見つかれば幸いです。最後までお読みいただきありがとうございました＾＾次回は去年のアドベントカレンダーで 新卒でベンチャーに行くのは「もったいない」のか という名文を書いてくれた ねこにしさん の出番です。お楽しみに！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[半年モブプロしたらチームが大きく成長した話]]></title>
        <id>https://developer.feedforce.jp/entry/2020/12/11/172338</id>
        <link href="https://developer.feedforce.jp/entry/2020/12/11/172338"/>
        <updated>2020-12-11T08:23:38.000Z</updated>
        <summary type="html"><![CDATA[こんにちは！フィードフォースで EC Booster というプロダクトを作っている @sukechannnn です。この記事は Feedforce Advent Calendar 2020 の 11日目の記事です。趣味の本屋を始めました でした。実際に自分でECサイトを立ち上げて運営するのって、言うは易く行うは難しですよね。すごいです。さて、内容に入っていきます。EC Booster チームではメイン開発をモブプログラミングで行っています！EC Booster はEC事業者様の集客を支援するサービスで、主に Google ショッピング広告を扱っています。また、今年１１月にフリープランをリリースし、より多くのEC担当者様をご支援できるよう機能開発を進めています。アプリケーションの構成は、フロントエンドが React + Flow (TypeScript 移行中)、サーバーサイドが Ruby on Rails で、API が GraphQL です。また、データフィード広告を扱う関係上、Ruby で書かれたバッチ処理がたくさん動いています。広告領域はなかなかに複雑で、深いドメイン知識が必要です。そんなプロダクトですが、現在はモブプログラミング（以下モブプロ）で全員がフロントエンド/バックエンドの垣根なく開発をしています。モブプロを導入した当初は試行錯誤の連続でしたが、最近はとても良い感じです。そして、気づいたらチームそのものが成長してきた...と感じています。この記事では、僕ら開発チームがどのようにモブプロの課題を乗り越え、モブプロの利点を生かして開発できるようになったのかを共有できればと思います。少し長いですが、お付き合いください 🙏RSGT でモブプロを知る僕がモブプログラミングという概念を知ったきっかけは Regional Scrum Gathering Tokyo 2018  というスクラム開発を題材にしたカンファレンスでした。モブプロは、ドライバーというコードを書く人が１人、それ以外のナビゲーターと呼ばれる複数の人で行います。ナビゲーターはドライバーに対して、実装する内容を指示します。ドライバーは支持された内容をコードに書いていく、という感じです。ドライバーは適宜交代していきます。RSGT で発表を聞いた時は、話している内容には説得力があり楽しそうだったものの、「それぞれ個人で開発した方が集中できて進捗も出るんじゃないかなぁ」と半信半疑でした。まさか自分がモブプロをやることになるとは、この時は思ってもいませんでした。時は流れ〜開発チームのメンバーが入れ替わることにそれから２年くらい経ち、今年の４月に EC Booster 開発チームのメンバーが入れ替わることになりました。デザイナー含め８人いた開発メンバーは僕含め４人になり、そのうち２人（当時フロントエンド担当）は入社して日が浅いという状況でした。その時リーダー的な役割を果たせと谷垣さん（この記事の右側の髪の長い人）に言い渡された僕は、どうしようかなと考え始めました。プロダクトの複雑なドメインをお二人にもしっかりキャッチアップして欲しい広告運用やデータフィードなどのドメイン知識があった方が、主体的に開発できると思ったためバッチ処理が多いアプリケーションなので、どういう処理をしているかを把握してもらった方がフロントエンドの開発をするにも役立ちそうフロントエンドだけじゃなくてバックエンドも開発してもらいたいな（彼らも意欲的だった）その時ちょうど、コロナで出社できない日常がスタートし、会社で直に話ができなくなってしまいました。そうだ、モブプロしよう！そこで思い出したのがモブプロです。以下のような理由から、オフィスで話ができないならずっとZoomを繋いでいればいいのでは？と思い、提案してみました。都度 zoom を繋ぐのだと、どうしても会話するハードルが上がってしまうドメインの説明は、直接話せば５分で伝えられることが文字だと３０分かかる、みたいなことが多い相手が理解してそうか？をリアクションから読み取ることで、補足説明できる（文字だとそれが分からない）以前にも少しモブプロを試していたが、リモートワークとの親和性が高そうだった上記を説明したら、開発メンバーのみんなも「せやな」と同意してくれたので、やってみることになりました。モブプロを始めたら情報の非対称性がなくなっていった最初は小さな改修をしながらドメイン知識をキャッチアップ最初は大きな開発ではなく、小さな改修を繰り返しながらドメイン知識のキャッチアップを進めていきました。ドメイン知識が詰まってるのが主にバックエンド（というかバッチ処理）だったので、最初はバックエンドの改修から始めました。キャッチアップして欲しい人にドライバーをお願いし、ナビゲーターが解説しながらコードを書いてもらう感じです。やってみると、実際に手を動かしてもらいながら説明できたので、口頭で説明するよりも知識が染み込んで行く感覚がありました。日々の運用作業も皆に担当してもらい、よく発生するアラートや必要な対応を、裏側の事情も含めて理解してもらうのに役立てました。ただ、その時はバックエンド担当⇢フロントエンド担当に教える感じが続いてしまいました。アプリケーションの全部を全員で開発しよう！しばらくやってみて、フロントエンド担当にバックエンドをやってもらうなら、バックエンド担当もフロントエンドやらなあかんやろ、ということでバックエンドとフロントエンドの垣根をなくしました。それにより、フロントエンド担当⇢バックエンド担当の説明も生まれ、お互いの話すバランスがちょうど良い感じになり、双方向のコミュニケーションになりました。その結果、ドメイン知識だけでなく、技術的にも情報の非対称性が減っていきました。今では技術的な課題をフロントエンド/バックエンド横断で全員が把握してるので、議論がとてもスムーズです。メインの開発もモブプロでやってみることに最初はドメイン知識のキャッチアップがある程度できたら非同期な開発に戻ろうと思っていましたが、モブプロが予想以上に良かったので新機能のメイン開発も引き続きモブプロでやってみることにしました。その時は、以下の２つを利点と感じていました。"何をどう作るか" という議論がノータイムでできることの価値が思ったより高かった（コンテキストの共有がしやすい）プランニングでは話しきれない、実際のコードを元にした議論がしやすかったその当時に実装しようとしていたのが冒頭で紹介した「フリープラン」という完全に新規の開発だったので、メンバー間の認識を合わせることが特に重要でした。それに加え、モブプロでも開発スピードが思っていたより落ちなかったのも導入を後押ししてくれました。モブプロは良いことずくめ！に感じたが...こうしてモブプロでのメイン開発がスタートしました。めっちゃええやん！と思って始めたモブプロでしたが、やっていく中でいくつか課題も見えてきました。めちゃくちゃ疲れる（最初は「これは本当に毎日続けられるのか？」と途方に暮れるレベルで疲れた）ずっと Zoom 繋いでるけど何を話そう...開発の進み具合が良くない議論を進める人に偏りが出る個人で集中して調査したり開発したりする時間も必要では？特に課題だったのは、思っていたより "疲れる" ということでした。そのせいで一時期は開発スピードが落ち、朝起きるのがつらい状態になってしまいました...。このままではアカン！ということで、これらの課題を解決するために、色々な工夫をしてきました。モブプロを上手くやるための工夫① 休憩する② 雑談する③ なるべく毎日同じ時間モブプロする④ その日やることを朝会で話し合う⑤ 個人タスクを用意する⑥ プランニングと開発キックオフ⑦ 自分から話さない人にも明示的に意見を聞くこれらの工夫を繰り返した結果、モブプロが上手くできるようになっただけでなく、チームそのものが成長していきました。それぞれ詳しく説明していきます。工夫① - 休憩する休憩するって当たり前のことなんですが、集中してしまうと忘れがちになるし、モブプロをやり始めた最初はみんな探り探りなので「休憩しましょう」とか気軽に言いにくい部分もあったように思います。そのため、まずは休憩する習慣を付けるために、あえて Zoom の無料プランを使って "40分で強制的に休憩" するようにしました。ポモドーロテクニックとか色々ありますが、これが一番確実でした。本当に強制的に切れるので、タイミングによっては笑いが起きて、和やかな雰囲気が副次的に醸成されていきました。最近は zoom の有料プランを使ってますが、"適度に休憩を取る" という習慣が根付いているので、適当なタイミングで誰かが休憩入れてくれます。休憩が上手になったので、今は明確なルールは定めてません。ガッと60分やりたい時もあれば、30分で区切りが良い時もありますからね。それでも以前より格段に疲れなくなりました。工夫② - 雑談する一見すると業務に関係のないような雑談もして良いよ！という雰囲気作りに務めました。特別 "雑談の時間" を作っているわけではなく、突発的な雑談を盛り上げるイメージです。これには、発言のハードルを徹底的に下げて、ちょっとした懸念やアイディアをポンと出せるようなチームにしたいという思いがありました。また、単に "話す" ことで疲れないようにしたいなとも思っていました。結果、チームメンバーがお互いのことを知るきっかけになり、それぞれの考え方や得手不得手を把握した上で議論ができるようになってきました。時には、エンジニアリングと全く関係ない雑談から実装のアイディアが浮かんだりして面白いです。開発メンバーが打ち解けている様子エビチリ🍤 をきっかけに、モブプロが解散するとエビのリアクションが飛び交うようになりました。良いですね。エビチリ工夫③ - なるべく毎日同じ時間モブプロする以前は、金曜日にまとめてプランニングを行っていたので、モブプロできるのが週４日でした。１日の中で５時間モブプロをやる日と、全くやらない日が混在してる状態です。集中して作業する時間をまとめて確保するためにそうしていたのですが、モブプロは長時間やるとめちゃくちゃ疲れるんですよね。逆に言うと、クリアな脳なら短時間でもめちゃくちゃ進捗します。ということで、なるべく同じ時間で毎日モブプロした方が良いことが分かったので、プランニングを他の日に移動して、最低２時間はモブプロの時間を確保するようにしました。今では無理なく週の最後まで開発をすることができています。また、上記の工夫により毎日の進捗が安定するようになったので、プロジェクトマネージメントの難易度がぐっと下がりました。この半年間、ほぼ事前の見積もり通りに開発が進んでいるのは自分でもびっくりです（ガントチャートで記録してます）。余談 - モブプロではレビューが要らないモブプロを始めてから、以前よりも見積もりが正確になっていることに気づきました。これには "モブプロではレビューがほとんど不要" という特性も関係しているように思います。非同期な開発だと「開発⇢レビュー⇢ (修正⇢レビュー) * n回⇢リリース」だったので、(修正⇢レビュー) * n回 の部分も見積もる必要があったが難しかったモブプロだと「開発⇢リリース」と単純なので、正確に見積もりやすいやってみて分かったことですが、モブプロだと本当にレビューが最低限で済みます。実装の時点で複数人の目が入ってますからね。今のところ大きなバグもありませんし、これに関しては "コストが下がった" と言って良いでしょう。工夫④ - その日やることを朝会で話し合う朝会は元々、開発メンバーがそれぞれ「昨日やったこと/今日やること」を報告するという、アジャイル開発ではオーソドックスなやり方でした。非同期な開発では、それぞれやってることが違うので報告し合う必要があったのですが、具体的な内容までは分からないので朝会で何か議論が発生することは稀でした。モブプロでは、昨日やったことは全員が知っているので、個々人が別々に報告する必要がありません。とはいえ昨日やったことの確認はしたいので、日替わりで１人がやったことを振り返り、その後に今日やることを話し合う感じにしてます。この、"今日やることを話し合う" 時間がとても良くて、昨日の実装の不安な点を話し合ったり（寝て起きると人は閃く）、これから実装するアプリケーションの設計を話し合ったりと、お互いの認識を同期する場になっています。毎日ちょっとしたプランニングをしてる感じですね。特に話すことがない日でも、すぐに区切るのではなく積極的に雑談するようにしてます。雑談することでそれぞれの体調や気持ち的な浮き沈みもなんとなく分かり、今日のモブプロの進め方とかどこまでやるかを確認できます。あと、単純に雑談は楽しいので、その日１日頑張ろう！という気持ちになります。工夫⑤ - 個人タスクを用意するモブプロが中心ではあるものの、個人で開発した方が効率的なことも多々ありますよね。新しい技術の調査リファクタリングある程度形になった後の細かい機能/テストの追加ライブラリーのアップデートetc...これらは明示的に "個人タスク" と切り出して、個人をアサインして進めるようにしています。当たり前ですが、個人タスクをやる時間は設けていて、良い息抜きになってるみたいです。何を個人タスクにするかは、モブプロの中で話し合ってサッと決まってしまうことが多いです。個人タスクをやってて詰まってしまった時にはいつでも聞いて良いし、場合によってはモブプロに戻ることもあります。また、実装していて「気になるから調べてたらできちゃった」みたいな事もエンジニアならあると思うんですが、そういう時は共有してもらった上でマージしちゃいます。ガチガチにモブプロじゃないとダメ！というルールにしているわけではなく、きちんとコンテキストが共有できているなら良いのです。工夫⑥ - プランニングと開発キックオフ以前は週次のプランニングは重たい行事でした。来週やることがよく分からないまま、各自で実装をしていたからです。プランニングでしっかり決めないと...というプレッシャーが、チーム全体の空気を重くしていたように思います。今では、朝会 + モブプロにより毎日やることを確認しているので、開発タスクについて週次のプランニングで話し合うことがかなり減りました。今はビジネスサイドへの進捗状況の共有と、メインタスク以外に発生する細かいタスクについて相談することが主になっています。代わり...というわけではないのですが、月曜の朝に "プチプランニング" と称して、その週に何をどこまで開発するか確認する場を設けています。その後にチーム全体のミーティングがあるので、プチプランニングで話したことを共有し、チーム全体での認識を合わせるようにしています。そして、大きな開発を始める前には "開発キックオフ" を開催するようになりました。UI/UXデザインとアプリケーション設計の大まかな叩き台を僕が用意しておき、それを元に具体的な実装について話し合うミーティングです。これは１日で終わることもあるし、数日かかることもあります。今開発している機能の開発キックオフは数日かかりましたが、とても良い感じのアーキテクチャになりました。具体的な設計まで行えたので見通しが良く、楽しく開発を進めることができています。工夫⑦ - 自分から話さない人にも明示的に意見を聞く最後に、これは個人的に意識していたことですが、自分から話さない人にも積極的に意見を聞くようにしてました。モブプロを始めた当初は議論を進める人に偏りが出やすかったのですが、改めて意見を聞いてみると、頭の中で別の意見を持ってることも多かったのです。"話さない == 意見がない" ではないんですよね。なので、雑談を通じて話しやすい雰囲気を作るのとセットで、積極的に全員の意見を明示的に聞くようにしていました。その結果、見落としていた懸念点に気づけたり、実は不要な実装だったことに気づけたりして、実利を得ることができました。最近は、特に僕が聞かなくてもみんな意見を言ってくれますし、大丈夫なときは「大丈夫です」って明示的に言ってくれます（リアクションだいじ）。意見を求める側も、気になる時は全員の意見を明示的に聞く流れができてきました。すごく良いことだなと思っています。モブプロを上手くやるための工夫を繰り返していたら開発チームそのものが成長したこんな感じで、モブプロを上手くやるための工夫を繰り返してしたら、チームそのものが良い感じになり、大きく成長することができました。モブプロの何が良かったのでしょうか。僕は、チームメンバー全員が "自分が考えていること/実際にやったこと" を共有することの大切さを理解し、積極的に共有できる状態になったこと、なのかなと思っています。以前は喋る前に緊張してめちゃくちゃ準備してたメンバーが、今は頭の中にあるものをそのまま話してくれるようになりました。特に取り繕うこともなく、自然とです。最近では、今後のやりたいことや構想をチームメンバーと話すと、それぞれで良いやり方や設計を考えてモブプロに持ち寄ってくれます。モブプロで各自が考えたことを元に話し合う時間がとても楽しくて、結果としてめっちゃ良いアーキテクチャが生まれています。今ならモブプロでなくても、非同期で開発を進めることもできると思います。実際に、開発する内容によっては、メイン開発であっても非同期で実装することもあります。それでも今、メイン開発をモブプロで進めているのは、このままモブプロで行くか？を改めて全員で確認し、その方が良いと皆が明示的に選んでいるからです。全員がコンテキストを共有した状態で考え工夫することで、チームとして成長できたのかなと思っています。ここまで大変なこともありましたが、諦めずに続けてきて良かった！一緒に工夫しながら開発してくれているメンバーには感謝しかありません。これからもよろしくお願いします！！！明日は daido1976 の「好きな動画の話」です！お楽しみに！（ちなみに僕が好きなユーチューバーは かねこ さんです。）]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[『4月から取り組んできたLookerの導入から実装までのお話（Redashとも比較）』という発表をした]]></title>
        <id>https://developer.feedforce.jp/entry/2020/10/23/190000</id>
        <link href="https://developer.feedforce.jp/entry/2020/10/23/190000"/>
        <updated>2020-10-23T10:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:masutaka26 です。最近の趣味はリハビリと YouTube 鑑賞です。本日、週次の社内勉強会 FFTT で『4月から取り組んできたLookerの導入から実装までのお話（Redashとも比較）』という発表をしました。4 月から Feedmatic という弊社フィードフォースの広告運用サービスに BI ツールである Looker を導入すべく、検討と実装をしてきました。関わっているエンジニアは私一人で、社内の他のエンジニアからは何やってきたか分からないと思っていたので、今までのまとめ的な発表をしました。ネット上を探しても導入時の具体的な話は見つけられなかったので、誰かの参考になることも期待して、外向けも意識しています。Looker ユーザ向けには P30 から「Symmetric 集計」を頑張って解説しています。説明が雑なのは認めます。🙏Looker は学習コストは高いですが、かなりパワフルなツールなので、個人的には非常にやる気を出して燃えています。🔥それでは！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next.js の Static HTML Export で生成したファイルを Lambda@Edge を使わずに CloudFront + S3 にデプロイする]]></title>
        <id>https://developer.feedforce.jp/entry/2020/10/21/111537</id>
        <link href="https://developer.feedforce.jp/entry/2020/10/21/111537"/>
        <updated>2020-10-21T02:15:37.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、id:daido1976 です。今回は Next.js の Static HTML Export で生成したファイルを Lambda@Edge を使わずに CloudFront + S3 にデプロイする方法を紹介します。前提Next.jsCloudFront + S3やり方1. S3 側での静的ウェブサイトホスティングの設定2. CloudFront の Origin Domain Name の設定3. Referer を使って CloudFront から S3 への直接アクセスを禁止する4. Next.js 側で各ページのファイル名を index.html にする設定参考前提Next.jsNext.js のアプリで SSR を行う場合は Vercel やその他の Node.js 実行環境のあるサーバにデプロイする必要がありますが、そうでない場合は Static HTML Export 機能を使って静的ページを出力し、任意の Web サーバにデプロイすることもできます。Static HTML Export 機能では通常 page/about.tsx というファイルから /about.html が生成されますが、これをそのまま S3 にアップロードして CloudFront 経由で /about にアクセスしても上手く動きません。Next.js 側で生成されるファイルを /about.html から /about/index.html に変える trailingSlash: true という設定があるのでこれで一件落着と思いきや、CloudFront + S3 側でも課題がありました…。CloudFront + S3Next.js の Static HTML Export 機能（& trailingSlash: true）でビルドされた /index.html と /about/index.html のように、ルートとサブディレクトリにインデックスドキュメントを含むファイル群をデプロイする際には以下の方法が考えられますが、それぞれに問題があります。CloudFront の Default Root Object と オリジンアクセスアイデンティティ を使うルートの index.html は表示できるがサブディレクトリの index.html が表示できないS3 の 静的ウェブサイトホスティング 機能を有効にし、インデックスドキュメント を設定するルート、サブディレクトリともに index.html 表示できるが、S3への直接アクセスが禁止できない👆 の解決策として、以下の記事のように Lambda@Edge などを使って「末尾が "/" で終わっているアクセスのパスに index.html を付与する」というものが多いです。dev.classmethod.jpが、管理するリソースはできるだけ増やしたくないのと、CloudFront の Behavior を使ってマルチオリジン構成にしている時にはパス書き換えのロジックが複雑になってしまうので、できれば Lambda@Edge を使わず CloudFront + S3 だけでやる方法を見つけたいと考えていました。結論としては、2. S3 の 静的ウェブサイトホスティング 機能を有効にし、インデックスドキュメント を設定する の方法を使いながら S3 への直接アクセスを禁止することができたので、その方法をご紹介します。やり方以下の AWS ナレッジセンターの記事を参考にしました。CloudFront を使用して、Amazon S3 でホストされた静的ウェブサイトを公開するにはどうすればよいですか?#アクセスが Referer ヘッダーで制限されたオリジンとして、ウェブサイトのエンドポイントを使用するざっくりしたアーキテクチャ図は以下になります。1. S3 側での静的ウェブサイトホスティングの設定S3 の静的ウェブサイトを有効にして、インデックスドキュメントを index.html にします。以下のエンドポイントは後ほど CloudFront の Origin Domain Name に設定する必要があるので控えておきましょう。2. CloudFront の Origin Domain Name の設定CloudFront の Origins and Origin Groups タブから Origin の Edit を押下し、Origin Domain Name を S3 の Origin ではなく、1 で取得した S3 の静的ウェブサイトのエンドポイント（ドメインの指定なので http:// は不要）にします。*1ここまでは通常の静的ウェブサイトホスティングの設定と同じです。3. Referer を使って CloudFront から S3 への直接アクセスを禁止する[ヘッダー名] の [Origin カスタムヘッダー] に、[Referer] と入力します。[値] には、オリジンに転送するカスタムヘッダーを入力します (S3 バケット)。オリジンへのアクセスを制限するために、ランダムな値または他の人は知らない秘密の値を入力することができます。ドキュメントに従ってそのまま Origin の編集画面でカスタムヘッダを設定します。「ランダムな値または他の人は知らない秘密の値を入力することができます」とある通り、この Referer の値は URL ではなく適当な値でも構いません。加えて、S3 コンソールから上記のカスタム Referer ヘッダーがリクエストに含まれることを条件として、s3:GetObject を許可するバケットポリシーを追加します。バケットポリシーの例#特定の HTTP Referer へのアクセスの制限 - Amazon Simple Storage Serviceこれで Referer ヘッダーに指定の値が含まれていないアクセスを禁止することができます。*24. Next.js 側で各ページのファイル名を index.html にする設定最後に Next.js の設定で trailingSlash: true を指定して、各ページのファイル名を index.html にします。https://nextjs.org/docs/api-reference/next.config.js/exportPathMap#adding-a-trailing-slash参考CloudFront を使用して、Amazon S3 でホストされた静的ウェブサイトを公開するにはどうすればよいですか?#アクセスが Referer ヘッダーで制限されたオリジンとして、ウェブサイトのエンドポイントを使用するバケットポリシーの例#特定の HTTP Referer へのアクセスの制限 - Amazon Simple Storage Serviceオリジンリクエストへのカスタムヘッダーの追加 - Amazon CloudFront*1:S3 の Origin は {bucket}.s3.amazonaws.com のような形式で、S3 の静的ウェブサイトのエンドポイントは {bucket}.s3-website-{region}.amazonaws.com のような形式です。間違えやすいので注意。*2:実際は curl などでこの Referer の値をヘッダにセットすると CloudFront 経由でなくともアクセスできるのですが、今回は一般ユーザからの URL 直打ちによるアクセスが禁止できれば良いので、この方法で問題ないと判断しました。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ソーシャルPLUS を Rails 5.0 にアップグレードしました]]></title>
        <id>https://developer.feedforce.jp/entry/2020/09/08/142913</id>
        <link href="https://developer.feedforce.jp/entry/2020/09/08/142913"/>
        <updated>2020-09-08T05:29:13.000Z</updated>
        <summary type="html"><![CDATA[[2021/08 追記]現在は Rails 5.2 にアップグレード済みです。こんにちは。今年の 4 月に EC Booster チームからソーシャルPLUS チームに異動してきた id:daido1976 です。今回はソーシャルPLUS の核となるソーシャルログインサービスの Rails アプリを 4.2 から 5.0 にアップグレードした話を書きます。ソーシャルPLUS のサービスや技術スタックについては以下の記事が詳しいのでご参照ください。developer.feedforce.jpはじめにどんな状態の Rails アプリかどれぐらいの期間かかったかレビューの仕方アップグレード時の Tips便利な gem たちStrong Parameters 対応に使える be_strong gemDEPRECATION WARNING のテストを変換する rails5-spec-converter gemカナリアリリース時のセッションデータ取得の不具合最後にはじめにRails 5.0 がリリースされたのは 2016 年、ということでアップグレードガイド的な記事はすでに素晴らしいものがたくさんあり、「rails 5 アップグレード」というキーワードやエラーメッセージをコピペして検索すれば大体の事象が解決できるはずです。なので、今回はどんな状態の Rails アプリをどれぐらいの期間かけてどのように進めたかに焦点を当てて書いていきたいと思います。（あとは Tips も少々）どんな状態の Rails アプリかアップグレード前の環境: Ruby 2.5.7, Rails 4.2.11.3 2012 年に Rails 3 系からスタートし、2015 年の Rails 4 系へのアップグレードを経て、5 年ぶりにメジャーバージョンのアップグレードを行う Rails アプリです。rails stats コマンドの実行結果は以下の通りで、simplecov によるテストカバレッジは 92 % です。技術的負債はそれなりに溜まっているものの、テストカバレッジが高いのが幸いでした。どれぐらいの期間かかったか約 1.5 ヶ月かかりました。担当者の私が、今回アップグレードした Rails アプリをほぼ初めて触る状態ということもあり、コードリーディングや動作確認に手間取り、時間がかかりました。また、このアプリの技術的負債の一つとして gem をやたらとバージョン固定していたこともあり、Rails のアップグレードを行う前準備として固定された gem をアップデートする必要もありました。レビューの仕方コードレビューは Zoom を用いて同期的に行い、私がレビュアーに変更点を説明、動作確認項目を箇条書きにし、抜け漏れがないかチェックしてもらいました。また他の方がみて、変更の意図がわかりやすいように Commit メッセージの 2 行目以降に補足コメントをつけるなどの工夫をしました。アップグレード時の Tipsここからはアップグレード前にこれ知りたかった、という Tips を軽く紹介します。便利な gem たちStrong Parameters 対応に使える be_strong gem今回のアップグレードの懸念の一つとしてモデルにある大量の attr_accessible を Strong Parameters に移行するという作業があったのですが、その際に以下の gem が役立ちました。github.com使い方は以下の記事にわかりやすくまとまっています。tech.pepabo.comDEPRECATION WARNING のテストを変換する rails5-spec-converter gemDEPRECATION WARNING: Using positional arguments in functional tests has been deprecated, ...Deprecated style:get :show, { id: 1 }, nil, { notice: "This is a flash message" }New keyword style:get :show, params: { id: 1 }, flash: { notice: "This is a flash message" }, ...数ある DEPRECATION WARNING の中でも 👆 は警告文が長く、デバッグの邪魔になるので、Rails 5.0 へのアップグレードと同じ PR で修正しました。その際に以下の gem が大変便利でした。github.comカナリアリリース時のセッションデータ取得の不具合ソーシャルPLUS では大量のアクセスを捌く必要があるため、常時複数台のアプリケーションサーバが稼働しています。そのためインパクトのある変更は可能な限りカナリアリリース*1 を行い、バグ混入時の被害を最小限に留めるようにしています。今回、Rails と一緒にアップデートされた rack gem にて、セッション ID を用いたセッションデータの取得ロジックに前方互換性がなく、カナリアリリース環境の時のみセッションデータが取得できなくなる不具合が見つかりました。長くなってしまったので、詳細は以下の記事に書きました。qiita.com最後に時間はかかりましたが、結果的に移行時のエラーも無く Rails 5.0 へのアップグレードを完遂することができました。開発チームリーダーからお褒めの Unipos もいただけて嬉しかったです。この勢いで Rails 5.2 系までは一気に上げてしまいたいと思っているので引き続き頑張ります！ではまたお会いしましょう＾＾*1:複数サーバのうちの一部にだけ新しいバージョンのアプリケーションをリリースするデプロイ手法]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[いろんな本でつながった「認知（cognition）のはなし」]]></title>
        <id>https://developer.feedforce.jp/entry/2020/08/28/183637</id>
        <link href="https://developer.feedforce.jp/entry/2020/08/28/183637"/>
        <updated>2020-08-28T09:36:37.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、入社７年目の今年は自分が光の戦士になってから７年目にもなります、id:tmd45 です。社内の技術勉強会（FFTT）での発表内容をざっとお送りします。今回は、自分の中ではよく考えることがあったけどなかなかアウトプット出来なかった「認知」に関する発表をしました。ちなみにフルリモートが選択可能になった弊社では、週次で続けている FFTT も Zoom でのオンライン開催をしています。スライドspeakerdeck.com内容についてもともと心理学（というかメンタルヘルス）に興味があって知っていた「認知」と、組織づくりやマネジメント、コーチングといった文脈で読んだ本、それから心理的安全性といった話題が自分のなかで噛み合って面白かったので、つらつらと紹介してみました。『エンジニアリング組織論への招待』という本は以前にも別のエンジニアが FFTT の発表で引用していました（2018年）。いまでも良い書籍だと思います。スライドの中で紹介した書籍はこちら。エンジニアリング組織論への招待　～不確実性に向き合う思考と組織のリファクタリング作者:広木 大地発売日: 2018/02/22メディア: Kindle版他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一発売日: 2019/10/02メディア: Kindle版「やさしさ」という技術――賢い利己主義者になるための7講作者:ステファン・アインホルン発売日: 2016/02/04メディア: Kindle版認知行動療法に関する書籍と、おすすめ漫画については省きます😇どれもおすすめですが、やはり『エンジニアリング組織論への招待』は網羅性と読みやすさがあって一番読んでいただきたい本です。またこの発表ネタを決めたきっかけになり、スライド中にも記載した２つの記事もご紹介したいと思います。発表のなかで「コーチャブルなひととは、認知のリフレーミングができるひとである」という話をしました。私もそろそろ年齢的には中堅おじさんですが、いつまでもそのように、人の間で学びながら成長できる人間でありたいものです。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[個人的に実践している、Slack に振り回されない方法]]></title>
        <id>https://developer.feedforce.jp/entry/2020/06/29/140000</id>
        <link href="https://developer.feedforce.jp/entry/2020/06/29/140000"/>
        <updated>2020-06-29T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:masutaka26 です。足底筋膜炎は誤診のようでした。リハビリに勤しむ日々です。今回は社内向けに書いた esa 記事が好評でしたので、少し改変してお届けします。フィードフォースでは Slack を使っており、リモートワークにおけるコミュニケーションの手段も Slack がメインです。Slack はチャットツールであるため、ある程度のリアルタイム性を求められますが、ただそれに従っていると振り回されてしまいます。今回は私が実践しているいくつかの工夫を紹介します。他に良い方法があれば、是非教えてください。今すぐできないこと（あとでやる＆あとで確認する）は自分にリマインドする自分が他の人に依頼した時もリマインドを設定するチャンネルは優先度を付ける見なくなったチャンネルからはこまめに抜ける未読チャンネルはショートカットキーで移動する今すぐできないこと（あとでやる＆あとで確認する）は自分にリマインドする自分にリマインドすることで忘れることが出来る。同期的コミュニケーションを非同期に逃がす感じ。👇 slackbot からの DM で通知される。すぐ出来なくてさらに先延ばししたければ、Snooze する。出来ない時はゆるふわで良いので Snooze することが大事。放置しないようにしている。👇 たまに slackbot の DM とかで /remind list して確認している。Snooze し忘れたりして、過去のリマインドが残っているとこんな感じになる。→ Past and Incomplete🌀 本当は Gmail の Inbox（受信トレイ）でリマインド管理したい。Snooze が使いやすいし。自分が他の人に依頼した時もリマインドを設定する相手が忘れても大丈夫になる。これも同期コミュニケーションから非同期への変換。チャンネルは優先度を付ける私の場合は Starred > Low Priority > Channels という優先度にしてます。（優先度が分からなくなるので、全未読機能は使っていません）Starred と Channels は最初からあるグルーピングです。Low Priority は自分で作りました。公式ヘルプ: チャンネルやダイレクトメッセージにスターを付ける | Slack💡 Channels の中には mute しているチャンネルもあります。見なくなったチャンネルからはこまめに抜ける抜けたいチャンネルで /leave と発言すると、一瞬で抜けられるよ。未読チャンネルはショートカットキーで移動するWindows前の未読チャンネルや DM に移動: Alt Shift ↑次の未読チャンネルや DM に移動: Alt Shift ↓Mac前の未読チャンネルや DM に移動: Option Shift ↑次の未読チャンネルや DM に移動: Option Shift ↓公式ヘルプ: Slack のキーボードショートカット | Slack]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[iOS Safari を Mac Safari でデバッグするときの注意点]]></title>
        <id>https://developer.feedforce.jp/entry/2020/04/20/201356</id>
        <link href="https://developer.feedforce.jp/entry/2020/04/20/201356"/>
        <updated>2020-04-20T11:13:56.000Z</updated>
        <summary type="html"><![CDATA[ごきげんようございます、id:tmd45 です。業務上の必要があって、タイトルのとおり iOS 端末（今回は iPhone XR）の Safari でのデバッグのため、Mac につないで Mac Safari の Web インスペクタを使いました。このやり方については一般的によく情報が公開されてるので、ここでは割愛します。今回ハマったことと解決法iPhone と Mac を USB ケーブルで繋ぎ、iPhone 側で「この Mac を信頼する」許可を行いました。が、その途端 Mac Safari の開発メニューから、繋げた iPhone デバイスが見つからなくなってしまいました。どこいったの私の iPhone ちゃん... Mac の Finder にはいるのに… 🤔結果的に、iPhone 側の Wifi を OFF にしたらデバイス表示されるようになりました。ちなみにこの解にたどり着くまでに、iOS を最新にしたり、macOS を最新にしたり、双方を再起動したりもしています。最新バージョン同士になるように気をつけるのは基本でしょうね…（古いバージョンで確認したい場合は難儀ですが…）またこの方法、今回の iPhone が Cellular 対応であったため事なきを得ていますが、Cellular 対応でない場合は Safari からインターネットにアクセスできなくなるので、あまり有効な方法とは思えません…これに関する公式あるいは公式に近しいソース情報を見つけられていません。なにか情報をお持ちの方がいたらぜひ @tmd45 に教えてください…普段から iOS 開発に精通しているわけではないので、超基本的なことだったらすみません！！！ 🙈]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS SDK for Ruby で S3 Select を使って S3 にある CSV ファイルからデータを抽出する]]></title>
        <id>https://developer.feedforce.jp/entry/2020/04/06/173701</id>
        <link href="https://developer.feedforce.jp/entry/2020/04/06/173701"/>
        <updated>2020-04-06T08:37:01.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、id:daido1976 です。つい先日 30 歳になりました。S3 に置いてある大きめの CSV ファイルから特定のデータだけ抽出して取得するのに S3 Select は大変便利です。Amazon Athena とは違い単一のファイルのみしか対象にできませんが、その分手軽に利用することができます。Python や Go の日本語記事はあるものの、Ruby の記事がなかったので書かせていただきます。実行環境Ruby 2.7.0aws-sdk-s3 1.60.2コード例今回は複数の id をキーに CSV ファイルからデータを抽出する例を見てみましょう。少し長いですが、以下のコードで動きます。Aws::S3::Client#select_object_content が Ruby から S3 Select を使うためのメソッドです。require 'aws-sdk-s3'require 'csv's3_client = Aws::S3::Client.new(region: 'ap-northeast-1')bucket = 's3-select-sample'key = 'target_sample_users.csv'# サンプル用のデータを準備するs3_client.put_object(bucket: bucket, key: key, body: File.read('sample_users.csv'))# 与えられた ids 配列の id で検索し、id と name だけ抽出するクエリを作成するdef build_query(ids)  <<~QUERY    SELECT    s.id    , s.name    FROM S3Object s    WHERE s.id    IN ('#{ids.join("', '")}')  QUERYend# `#select_object_content` に渡すパラメータを作成するdef build_params(bucket, key, query)  {    bucket: bucket,    key: key,    expression_type: 'SQL',    expression: query,    input_serialization: {      # field_delimiter を "\t" にすることでタブ区切りのファイルにも対応可能      csv: { file_header_info: 'USE', allow_quoted_record_delimiter: true, record_delimiter: "\n", field_delimiter: ',' }    },    output_serialization: {      csv: { record_delimiter: "\n", field_delimiter: ',' }    }  }end# id が 1 と 50 のユーザを検索するquery = build_query([1, 50])params = build_params(bucket, key, query)# S3 Select を使ってレスポンスを取得するresponse = s3_client.select_object_content(params)# `#event_type == :records` の中にレコード（抽出されたデータ）が含まれるcsv_list = response.payload.select { |p| p.event_type == :records }.map(&:payload).map(&:read)csv = CSV.parse(csv_list.join)# マルチバイト文字列を扱う場合csv.map do |row|  row.map { |r| r.force_encoding('UTF-8') }endp csvsample_users.csv は以下のような CSV を想定しています。（値は ffaker gem を使って作成しました）id,name,email,address,job0,小出整,idell@boyle.us,773-9409 群馬県大府市安芸区福山市0丁目4番4号,運転手1,坂口昭三,clifford.steuber@walshkutch.name,663-8507 東京都豊島区裾野市842,管理栄養士2,池間悠希,taunya.kerluke@hellerweimann.ca,898-7832 東京都港区鯖江市8丁目5番3号,理学療法士3,北野真美,rick.nitzsche@harvey.info,411-8019 静岡県大野市中村区駒ヶ根市9丁目4番9号,テレビディレクター4,関根京子,francene@cummings.com,809-0176 東京都練馬区夕張市524,クラブDJ5,熊本とうこ,bo.oconner@bradtkemiller.biz,423-3182 青森県柏原市佐渡市2丁目5番9号,指揮者...50,酒向彩香,seema.denesik@fahey.us,481-5386 宮崎県上高井郡にかほ市9丁目1番1号,クラブDJ上記コードを実行すると、出力は以下のようになります。[["1", "坂口昭三"], ["50", "酒向彩香"]]補足レスポンスのサイズが不明な場合、S3 Select はデータを複数のイベントに分割し、ストリームとして返す仕様になっています。今回は全てのデータが到着するのを待ってから処理するようなコードになっていますが、AWS Developer Blog にはストリームを活かして複数のイベントを非同期に処理する方法も記載されていますので、ご興味のある方はぜひご覧になってください。ちなみに S3 Select を S3 Management Console から試したい場合は以下の箇所から利用できます。ご参考まで。最初この位置にあるの気づかなかった…結論S3 Select めっちゃ便利。参考Introducing support for Amazon S3 Select in the AWS SDK for Ruby | AWS Developer BlogClass: Aws::S3::Client — AWS SDK for Ruby V3Amazon S3 Select が一般公開されたので使ってみた（Python） | Developers.IOAWS SDK for Go で Amazon S3 Select を試してみた | Developers.IO]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[my_api_client v0.16.0 をリリースしました🚀]]></title>
        <id>https://ryz310.hateblo.jp/entry/2020/03/29/220805</id>
        <link href="https://ryz310.hateblo.jp/entry/2020/03/29/220805"/>
        <updated>2020-03-29T13:08:05.000Z</updated>
        <summary type="html"><![CDATA[前回のリリースから 1 週間ほどですが、今日予定していたライブがコロナウイルスの影響で中止になったので暇を持て余しました 😷github.comv0.16.0 の新機能2 つありますが、どちらも若干の Breaking Change です。とはいえ普通に使っていたら全く影響を受けないと思います。新機能 1. エラーハンドラがエラーを検出した際は常に例外を raise するようになりましたmy_api_client では JSON API からのレスポンス内容に応じて例外を発生させる error_handling というメソッドが利用できます。以下に error_handling を利用した例を示します。class ExampleApiClient < ApplicationApiClient  endpoint 'https://example.com'  error_handling json: { '$.errors.code': 10 }  error_handling json: { '$.errors.code': 20 }, raise: MyErrorClass  error_handling json: { '$.errors.code': 30 }, with: :my_error_handling  error_handling json: { '$.errors.code': 40 } do |params, logger|    # Do something.  end  # GET error/:code  def request    get 'path/to/resouce'  end  private  def my_error_handling(params, logger)    # Do something.  endendこの例の場合、 ExampleApiClient#request を実行すると GET https://example.com/path/to/resouce に対してリクエストが実行され、レスポンスボディが JSON 形式だった場合、JSONPath $.error.code の値に応じて以下の処理を実行します。10 だった場合 MyApiClient::Error を発生させる20 だった場合 MyErrorClass を発生させる30 だった場合 #my_error_handling を実行する （例外は発生しない）40 だった場合 do ~ end を実行する （例外は発生しない）MyApiClient::Error は raise オプションで例外クラスを指定しなかった場合のデフォルトの例外クラスです。この時、従来は 30 と 40 のように with や block を利用した場合は、処理の中で明示的に raise を実行しない限り、例外は発生しませんでした。エラー検出時に例外を発生させるかどうかは、 my_api_client の利用者に委ねられていた形になります。しかしながら、ここに自由度を持たせるよりも、 エラー検出時には必ず raise させて rescue で異常時の処理を記述する 、という方式に統一した方が my_api_client の利用方法としても理解しやすく、特に困るケースも想定されなかったことから、以下のように変更することにしました。10 だった場合 MyApiClient::Error を発生させる （変更なし）20 だった場合 MyErrorClass を発生させる （変更なし）30 だった場合 #my_error_handling を実行し、 MyApiClient::Error を発生させる40 だった場合 do ~ end を実行し、 MyApiClient::Error を発生させる今後はエラー検出時には常に何らかの例外が raise されるようになります。上記の例では MyApiClient::Error が発生しますが、 with や block と同時に raise を指定すれば、任意の例外クラスが発生するようになります。これにより、 with や block は例外の前処理という位置付けになります。ユースケースとしてはログ出力や slack への通知などが考えられます。新機能 2. 標準のエラーハンドラが用意されましたmy_api_client では generator 機能 が用意されており、 $ rails g api_client path/to/resource get:path/to/resource を実行すると以下のファイルが作成されます。create  app/api_clients/application_api_client.rbcreate  app/api_clients/path/to/resource_api_client.rbinvoke  rspeccreate    spec/api_clients/path/to/resource_api_client_spec.rb` この時、 application_api_client.rb に標準のエラーハンドラの例がいくつか記載されるのですが、例というより必須のエラーハンドラだよね、ということで、 my_api_client の内部で標準実装するようにしました。これにより、ステータスコード 4xx と 5xx のレスポンスに対しては標準で例外が発生するようなります。また、ネットワーク系のエラーに対しても標準で 300 msec 間隔を空けて 3 回リトライが試行されるようになります。（リトライ処理も従来は明示的な定義が必須でした）# 従来の `application_api_client.rb` に出力されていた標準のエラーハンドラ例error_handling status_code: 400..499, raise: MyApiClient::ClientErrorerror_handling status_code: 500..599, raise: MyApiClient::ServerError# 従来の `application_api_client.rb` に出力されていた標準のリトライ処理例retry_on MyApiClient::NetworkError, wait: 5.seconds, attempts: 3標準で定義されているエラーハンドラは my_api_client/default_error_handlers.rb から参照できます。error_handling は後から定義した物が優先されますので、例えばステータスコード 400 に対しては独自の例外クラスを発生させるようにしたい場合、継承先のクラスで error_handling status_code: 400, raise: MyErrorClass のように定義すれば、 MyErrorClass が例外として発生するようになります。所感社内のプロダクト用に作った gem ですが、少しずつ自分以外のエンジニアも利用してくれるようになってきました。一方で、自由度が高過ぎると熟知していないと使えない機能が増えてしまう点を課題感として感じるようになってきました。なるべく自由度の高い gem を意識しつつ、標準の状態でも高度な機能の恩恵を受けられる状態を目指していきたいと思います。恐らく次の新機能は async/await っぽい機能、または sawyer gem の依存からの脱却なると思います。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[my_api_client v0.15.0 をリリースしました🚀]]></title>
        <id>https://ryz310.hateblo.jp/entry/2020/03/21/163849</id>
        <link href="https://ryz310.hateblo.jp/entry/2020/03/21/163849"/>
        <updated>2020-03-21T07:38:49.000Z</updated>
        <summary type="html"><![CDATA[その前に v0.14.0 もリリースしているのですが、こちらはリファクタリングと Integration Test の実装だけで新機能はありませんでした。差分が +2,799 -1,246 もあるので中身は結構書き換わっています。Release v0.14.0 · ryz310/my_api_client · GitHubIntegration Test では Ruby on Jets を使って AWS Lambda でサーバーを建てて、CI でのテストで my_api_client を使って実際に HTTP リクエストが成功することを確認しているので、デグレの心配が随分と緩和されました 😌新機能: Pagination API のサポートここからは v0.15.0 の話になります。Release v0.15.0 · ryz310/my_api_client · GitHubv0.15.0 のメイン機能が Pagination API のサポートになります。JSON:API というしっかりとした仕様もあるようですが、 my_api_client ではそこまで厳密な仕様に則っている訳ではなく、レスポンスに含まれる URL を認識して enumerable に HTTP リクエストを実行する、というざっくりした機能になります。レスポンスヘッダの Link などで次のページの URL を返すケースもあるようですが、そちらは現時点では未対応です 🙏Pagination API という単語は Django REST Framework の Pagination 機能の説明で出てきます。www.django-rest-framework.org要するに一度のリクエストで結果を全件取得させるのではなく、一定の件数を返却し、続きを取得できる Link を一緒に返却する API のことですね。Request:GET https://api.example.org/accounts/?page=4Response:HTTP 200 OK{    "count": 1023    "next": "https://api.example.org/accounts/?page=5",    "previous": "https://api.example.org/accounts/?page=3",    "results": [       …    ]}使い方my_api_client での使い方は以下のようになります。class MyPaginationApiClient < ApplicationApiClient  endpoint 'https://example.com/v1'  # GET pagination?page=1  def pagination    pageable_get 'pagination', paging: '$.links.next', headers: headers, query: { page: 1 }  end  private  def headers    { 'Content-Type': 'application/json;charset=UTF-8' }  endend通常であれば #get を使って HTTP リクエストを実行させるのですが、ここでは #pageable_get というメソッドを使用しています。 #pageable_get だと長いので #pget というエイリアスも用意しています。また、 paging というキーワード引数も新たに出てきました。 paging ではレスポンスのどの部分に次のページの URL が含まれるかを JSONPath expression で指定します。goessner.net以下のような JSON であれば、 $.links.next という JSONPath expression は "https://example.com/pagination?page=3" を取得します。{  "links": {    "next": "https://example.com/pagination?page=3",    "previous": "https://example.com/pagination?page=1",  },  "page": 2}作成した API Client は以下のように使用できます。api_clinet = MyPaginationApiClient.newapi_clinet.pagination.each do |response|  # Do something.endp = api_clinet.paginationp.next # => 1st page resultp.next # => 2nd page resultp.next # => 3rd page result結果は Enumerator::Lazy で返却される#pageable_get は Enumerator::Lazy を返却するので、 Enumerable で定義されているメソッドは一通り利用可能です。docs.ruby-lang.orgEnumerator で返してしまうと #take で 100 ページ目まで結果を取得するような処理を記述したときに、100 ページ分の HTTP リクエストを実行結果を #each で回すという動きになり、 100 回分の HTTP リクエストが完了するまで次の処理に移ることができません。Enumerator::Lazy であれば、1 ページ目の HTTP リクエストを実行結果を処理する2ページ目の HTTP リクエストを実行結果を処理する...という動きになってくれます。便利ですね ✨Enumerator と Enumerator::Lazy の違いは以下の記事が参考になると思います。qiita.com]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[『HerokuでSidekiqを監視する方法を確立した』という発表をした]]></title>
        <id>https://developer.feedforce.jp/entry/2020/03/02/140000</id>
        <link href="https://developer.feedforce.jp/entry/2020/03/02/140000"/>
        <updated>2020-03-02T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:masutaka26 です。最近はドラクエウォークと足底筋膜炎との両立に悩んでいます。先週の金曜日、週次の社内勉強会 FFTT で『HerokuでSidekiqを監視する方法を確立した』という発表をしました。Heroku で Sidekiq を監視する知見を見つけられなかったのが、このお題を選んだ経緯です。どなたかが参考になるとうれしいです。今回は自宅からのリモート発表でした。物理発表と違って、話した内容よりもリアクションの分からなさによる戸惑いが気になってしまいました。（オイ※ リアクションはしてくれてたみたいです。🙏複数モニター必須とか、いろいろ知見がありそうなので、今後蓄積出来ると良さそうです。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[駆け込みで Chrome 80 の SameSite=None; Secure の対応をやった🍪]]></title>
        <id>https://ryz310.hateblo.jp/entry/2020/02/20/235548</id>
        <link href="https://ryz310.hateblo.jp/entry/2020/02/20/235548"/>
        <updated>2020-02-20T14:55:48.000Z</updated>
        <summary type="html"><![CDATA[ご存知の方も多いかと思いますが、 Chrome 80 から 3rd Party Cookie の取り扱いが厳しくなり、特に指定がないと外部サイトの Cookie は POST・iframe・XHR 等のリクエスト  で送られなくなります。developers-jp.googleblog.com2 月の Chrome 80 以降、SameSite 値が宣言されていない Cookie は SameSite=Lax として扱われます。外部アクセスは、SameSite=None; Secure 設定のある Cookie のみ可能になります。ただし、これらが安全な接続からアクセスされることが条件です。とはいえ完全に無効になるわけではなく、 SameSite という属性が宣言されていない Cookie は Lax という区分がデフォルトで適用されるという物なので、サーバーから返す Cookie に対して明示的に SameSite を None に指定して、かつ Secure という属性を付与すれば、従来どおり Cookie が送信されます。去年の秋くらいにもこの Cookie の対応が必要かどうか、会社で調査していたんですが、その時点では影響を受ける箇所が無くて、特に何も対応せずにスルーしていたんですが、現在開発してる新機能がたまたまこの影響を受ける機能だったため、急遽対応することになりました。結構厳しいな、と思ったのは、これが原因で動かないことに気付くのが結構難しいんですよね。シングルサインオンみたいな機能を作ってるとドメインが異なるので 3rd Party Cookie の扱いになります。そして Cookie に入っているはずの Session ID が送られてこないので、サーバー側で Session が見つからずにエラー。自分の手元の Chrome は 79 で、まだ上記の制限が入っていなかったんですが、以前影響範囲を調査した時に chrome://flags から有効にするフラグを ON にしていたので、他のエンジニアの環境では動作するけど、自分だけ動かないということになり、もしかして、と思って気付いた感じです。リリース後に気付いてたらヤバかったですね。。で、こういう問題は Rails みたいなフレームワークで対応してくれよって気持ちになるんですが、ちゃんと対応する PR は作られていて、すでに merge もされています。github.comですが、 2020/2/20 現在、これを反映した Rails はまだリリースされていないみたいですね。（最新が 2019/12/19 にリリースされた 6.0.2.1 ）また、 Rails 5.2 に反映されるかどうかは微妙な感じになっています。https://github.com/rails/rails/pull/28297#issuecomment-577414543We will backport to 6.0 as a bug fix, but I don't know this warrants a backport to a security only release like 5.2. Rails 6.0 was released 6 months ago, and upgrading applications could be high, high priority if that problem is so important.バグ修正として6.0にバックポートしますが、これが5.2のようなセキュリティのみのリリースへのバックポートを保証するかどうかわかりません。 Rails 6.0は6か月前にリリースされました。その問題が非常に重要な場合、アプリケーションのアップグレードは優先度が高くなる可能性があります。自分の開発環境は恥ずかしながら Rails 4.2 （今年中にアップデートします！）なので、当然反映されるはずもないので、自前で Rack を作成して対応しました。参考まで以下のようなコードになります。# config/initializers/custom_rack_middleware.rb# NOTE: Rails 6.0.x であれば以下の処理は不要となる。Rails.application.config.middleware.insert_before(  ActionDispatch::Cookies,  CustomRackMiddleware::SetSameSiteOptionOnCookie)# lib/custom_rack_middleware/set_same_site_option_on_cookie.rb# TODO: Rails 6.0.x にアップデートしたら削除するmodule CustomRackMiddleware  class SetSameSiteOptionOnCookie    def initialize(app)      @app = app    end    def call(env)      status, headers, body = @app.call(env)      cookies = headers['Set-Cookie']      if cookies.present?        processed_cookies = cookies.split("\n").map do |cookie|          "#{cookie}; SameSite=None; Secure"        end        headers['Set-Cookie'] = processed_cookies.join("\n")      end      [status, headers, body]    end  endend2020/02/22 追記ローカルの開発環境など HTTP リクエストをする環境だと、 SameSite=None の設定を入れると Cookie が送られなくなるようです。ローカルでは上述の Rack を読み込ませないようにするなどの工夫が必要だと思います。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heroku Ruby Language Metrics のメトリクス収集の仕組みを調べた]]></title>
        <id>https://developer.feedforce.jp/entry/2020/02/13/150000</id>
        <link href="https://developer.feedforce.jp/entry/2020/02/13/150000"/>
        <updated>2020-02-13T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。id:masutaka26 です。半年くらい前から Heroku の Ruby Language Metrics (Public Beta) を使い始めました。その時、どのようにメトリクスを収集しているか調べたので、この記事にまとめます。（なぜこのタイミング...）有効にする方法は前述の公式ドキュメントをどうぞ。id:sho7650 の記事も併せて読むと分かりやすいと思います。qiita.comところで、いつから Public Beta だったのですかね？ GA はいつになるのかな...？概要今回は Rails 前提で説明します。このような仕組みでメトリクスの収集と送信が行われます。heroku/metrics buildpack により、Dyno 起動時に agentmon がインストールされ、常駐するRails が起動すると、barnes gem がメトリクスを収集し始め、定期的に localhost の agentmon に送信するagentmon は https://app.metrics.heroku.com/<dyno id> にメトリクスを送信するエンジニアは Heroku Dashboard から Ruby のメトリクスを閲覧できるagentmon のインストールと起動heroku/metrics buildpack によって、agentmon というデーモンがインストールされます。heroku/agentmonagentmon は後述する barnes gem から受け取ったメトリクスを https://app.metrics.heroku.com/<dyno id> に送信します。この URL は環境変数 HEROKU_METRICS_URL の値で、Heroku Dashboard から Enhanced Language Metrics を有効にすると、各 Dyno で定義されます。面白いのは、Slug 生成時には /app/.profile.d/heroku-metrics-daemon.sh がインストールされるだけで、Dyno 起動時にこのスクリプトが実行され、agentmon のインストールと起動が行われることです。Dyno では通常 Puma などのプロセスしか起動されませんが、このような方法を使えばデーモンも起動できることを初めて知りました。ちなみに Dyno が起動する時 /etc/profile によって /app/.profile.d/*.sh がすべて実行されます。barnes gem によるメトリクス収集と送信heroku/barnesRails が起動すると、Barnes.start から呼ばれる Barnes::Periodic.new で Ruby のスレッドが作られます。Barnes.start はすぐに終了しますが、作られたスレッドは非同期で実行されます。無限ループのスレッドなので、Rails が起動している間、起動し続けます。この無限ループでは 10 秒に 1 回、後述するメトリクスを収集し、localhost の agentmon に送信します。送信するのはスレッド中のインスタンス変数 @reporter です。これは Barnes::Reporter のインスタンスで、statsd_client を介して agentmon に送信します。statsd_client は Barnes::Reporter.new 時に渡されます。https://github.com/heroku/barnes/blob/v0.0.8/lib/barnes.rb#L51収集されるメトリクスBarnes::ResourceUsage では収集されるメトリクスを一望できます。https://github.com/heroku/barnes/blob/v0.0.8/lib/barnes/resource_usage.rb#L28-L63収集されるメトリクスと、収集方法です。Puma のメトリクス（ Barnes::Instruments::PumaInstrument ）https://github.com/heroku/barnes/blob/v0.0.8/lib/barnes/instruments/puma_instrument.rb#L33-L48CPU 実行時間等のメトリクス（ Barnes::Instruments::Stopwatch ）https://github.com/heroku/barnes/blob/v0.0.8/lib/barnes/instruments/stopwatch.rb#L36-L58ObjectSpace.count_objects の結果（ Barnes::Instruments::ObjectSpaceCounter ）https://github.com/heroku/barnes/blob/v0.0.8/lib/barnes/instruments/object_space_counter.rb#L27-L30GC.stat の結果（ Barnes::Instruments::RubyGC ）https://github.com/heroku/barnes/blob/v0.0.8/lib/barnes/instruments/ruby_gc.rb#L62-L84まとめHeroku Ruby Language Metrics のメトリクス収集の仕組みを調べました。ただ有効にするだけだと、他のエンジニアに説明することができなかったことが、今回調べた動機です。今回に限りませんが、Heroku は完全なブラックボックスではなく、調べると実装が透けて見えるのがとても良いと思います。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pusher の Private channels と認証周りの処理を追いかけた]]></title>
        <id>https://developer.feedforce.jp/entry/2020/02/05/172927</id>
        <link href="https://developer.feedforce.jp/entry/2020/02/05/172927"/>
        <updated>2020-02-05T08:29:27.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、id:daido1976 です。もうすぐ 30 歳になります。Pusher の Private channels と認証周りの処理が公式ドキュメントを読んだだけでは全然わからなかったので、ライブラリの実装を追いかけてみました。結論コード例クライアント（JavaScript）サーバ（Rails）ライブラリの実装を追いかけた1. WebSocket のコネクションを確立する2. App Server の /pusher/auth に POST リクエストして認証トークンを取得3. 認証トークンをライブラリ内でセットして当該チャンネルを Subscribe するリクエストを Pusher に送る参考結論実装を追っていくと公式ドキュメントに載っている以下の図の通りになっていました。（当たり前）https://pusher.com/docs/channels/server_api/authenticating-usersコード例以下のコード例は抜粋ですが、Rails サーバとの通信を仮定しているので、ライブラリは pusher-js と pusher-http-ruby を利用する前提です。クライアント（JavaScript）const pusher = new Pusher("app_key", {  auth: {    headers: {      // トークンベースで認証を行う場合、ここに何らかの認証情報を載せる      // ...    }  }});// User ごとの Private チャンネルを作成し、Subscribe する場合const channel = pusher.subscribe(`private-${userId}`);// 以下イベントに bind する処理などが続く...サーバ（Rails）# RoutingRails.application.routes.draw do  # ...  post '/pusher/auth', to: 'pusher_auth#create'end# Controllerclass PusherAuthController < ApplicationController  def create    # クライアントから渡ってきた認証情報を使ってユーザを識別するコードは省略    if current_user      # Pusher のクライアント作成のコードは省略      response = pusher_client.authenticate(params[:channel_name], params[:socket_id])      render json: response    else      render json: {}, status: 403    end  endendライブラリの実装を追いかけた以下、ライブラリのコードを読む際に少しハマったので先にお伝えしておきます。pusher-js は Web、 React Native など複数のランタイムのビルドを生成できるようになっています。（ビルド時に webpack によって依存関係の解決が切り替わるようになっている）1プラットフォームに依存しない共通部分の実装は core/ 以下に、プラットフォームに依存する実装は runtimes/ 以下に置いてあるのでご留意ください。それでは、公式ドキュメントの図に沿ってライブラリの実装を追っていきましょう。1. WebSocket のコネクションを確立する公式ドキュメントの図でいうと 1~3 まで。コネクションの確立は Runtime.setup の中でやっています。（これは実装者が Pusher オブジェクトを new した時に呼ばれる）具体的には上記の Runtime.setup の中で Pusher.ready が呼ばれ（実装はここ）、その中で Pusher#connect が呼ばれてコネクションが確立されます。※ 内部の処理は深かったので省略しますが、最終的には公式ドキュメントの図の通り pusher:connection_established イベントが発火して  connected な状態になります。（Web のための Runtime.setup は こちら から始まりますので、興味のある方は追ってみてください）2. App Server の /pusher/auth に POST リクエストして認証トークンを取得公式ドキュメントの図でいうと 4~6 まで。コネクションが確立されたら Pusher#subscribe の ここの条件分岐 に入り、*.Channel#authorize が呼ばれて App Server （コード例でいう Rails サーバ）の /pusher/auth2に POST リクエスト3して、App Server 側では ここ で認証トークン（authentication_string） を詰めて、 *.Channel#authorize の コールバック に返ってきます。ちなみにチャンネル名を見て Private かどうかを調べてるのは ここ です。※ Public channels の場合は ここ で認証をスキップしています。3. 認証トークンをライブラリ内でセットして当該チャンネルを Subscribe するリクエストを Pusher に送る公式ドキュメントの図でいうと 7。認証トークンは ここ で勝手にセットされ、 pusher:subscribe のイベントが Pusher に送られます。（最終的には ここ で Pusher にデータが送られている）参考Authenticating users | Pusher docsGitHub - pusher/pusher-js: Pusher Javascript library | owner=@leesioGitHub - pusher/pusher-http-ruby: Ruby library for Pusher Channels HTTP API | owner=@annzenkinahttps://github.com/pusher/pusher-js#core-vs-platform-specific-code↩authEndpoint は何も指定しなければ /pusher/auth になります。（ここでセットされてる）↩App Server 側に POST /pusher/auth してるのは次のような流れ。Private チャンネルなので、#authorize したら PrivateChannel#authorize が呼ばれます。PusherAuthorizer#authorize の中で Runtime#getAuthorizers が呼ばれ、 この ajax 関数 が実行されます。↩]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[my_api_client v0.13.0 をリリースしました🚀]]></title>
        <id>https://ryz310.hateblo.jp/entry/2020/01/21/230012</id>
        <link href="https://ryz310.hateblo.jp/entry/2020/01/21/230012"/>
        <updated>2020-01-21T14:00:12.000Z</updated>
        <summary type="html"><![CDATA[つい先日、v0.12.0 をリリースしたばかり ですが、 v0.13.0 をリリースしましたので、含まれる PR の内容について解説していきます。 より詳しい使い方は README.jp.md をご参照ください。github.com#180 Stub response on raising error (@ryz310)今回はこの PR のみの更新です。仕事で spec 書いてる最中に「API リクエストで例外が発生した際にレスポンス内容を保存する処理のスタブ化できないじゃん」ってなって作りました。my_api_client では作成した API Client クラスを stub_api_client または stub_api_client_all というメソッドでスタブ化できます。例えば以下のような ExampleApiClient というクラスを定義した時:class ExampleApiClient < MyApiClient::Base  endpoint 'https://example.com'  error_handling status_code: 400..499, raise: MyApiClient::ClientError  error_handling status_code: 500..599, raise: MyApiClient::ServerError  # GET https://example.com/path/to/resouce  def request     get 'path/to/resouce'  endendstub_api_client_all を実行すると ExampleApiClient のインスタンスが全てスタブ化されるようになります。以下の例だと、 #request を実行した時、API から { "message": "Hello world!" } という JSON が返ってきた時と同じ振る舞いをするようになります。stub_api_client_all(  ExampleApiClient,  request: { response: { message: 'Hello world!' } })api_client = ExampleApiClient.newresponse = api_client.requestresponse.message # => 'Hello world!'で、 error_handling でステータスコードが 400..499 の時は MyApiClient::ClientError という例外が発生する、という定義になっているのですが、このような例外が発生した時のテストを書くために、 raise のスタブ化も出来るようになっています。stub_api_client_all(  ExampleApiClient,  request: { raise: MyApiClient::ClientError })begin  api_client = ExampleApiClient.new  response = api_client.requestrescue MyApiClient::ClientError  puts '4xx error!'end大抵の場合、これらのスタブ化ができれば問題ないのですが、例外発生時のレスポンスを見たい、というケースも無くはないかと思います。仕様として、例外インスタンスの #params や #matadata というメソッドからリクエストパラメータとレスポンスパラメータを参照できるようになっています。従来のスタブ化メソッドでも一応指定できなくはなかったんですが、結構手間だったので raise オプションの指定を以下のように拡張しました。raise と一緒に指定した response が API のレスポンスとして返されて、それが例外として処理された、というスタブ化になります。stub_api_client_all(  ExampleApiClient,  request: {     raise: MyApiClient::ClientError,    response: { error_code: 10 }  })begin  api_client = ExampleApiClient.new  response = api_client.requestrescue MyApiClient::ClientError => e  e.params.response.data.error_code #=> 10endmy_api_client は内部で Sawyer を使っています。github.come.params.response が Sawyer::Response をそのまま返しているので、Sawyer::Response#data からレスポンスボディを参照できます。Sawyer::Response#data では、レスポンスの JSON を OpenStruct) のようにメソッドアクセスできるように変換してくれます。ただし、 V1.0.0 で Sawyer の依存を無くしたいと考えているので、いずれ #data を挟む書き方は変更になるかもしれません。一応こういう使い方もできますよ、という新機能でした。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[my_api_client v0.12.0 をリリースしました🚀]]></title>
        <id>https://ryz310.hateblo.jp/entry/2020/01/19/152826</id>
        <link href="https://ryz310.hateblo.jp/entry/2020/01/19/152826"/>
        <updated>2020-01-19T06:28:26.000Z</updated>
        <summary type="html"><![CDATA[my_api_client v0.12.0 に含まれる PR の内容について解説していきます。より詳しい使い方は README.jp.md をご参照ください。#173 Avoid sleep on testingmy_api_client では以下のように書くと、任意の例外を補足して自動的に API リクエストをリトライしてくれます。ネットワーク系のエラーとか、 API Rate Limit に引っかかった時とかに便利なやつですね。ActiveJob の retry_on とほぼ同じ使い方になっています。class ExampleApiClient < MyApiClient::Base  endpoint 'https://example.com'  retry_on MyApiClient::ApiLimitError, wait: 1.minute, attempts: 3  error_handling json: { '$.errors.code': 20 }, raise: MyApiClient::ApiLimitError  # GET https://example.com/users  def get_users    get 'users'  endendそれは良いんですが、 rspec 上で wait が効いてしまっていたので、上記のコードだとリトライ3回分 wait するので、合計 3 分も待たされてしまっていました。一応 rspec で sleep を stub するとかやれば回避できますが、そもそもテストでは wait を無視して欲しいですよね。my_api_client では be_handled_as_an_error という rspec の matcher を用意しているのですが、今回の対応で、この macher を経由してリトライが実行された場合は wait を無視するようになりました。RSpec.describe ExampleApiClient, type: :api_client do  let(:api_client) { described_class.new }  # NOTE: レスポンスで `{ "errors": { "code": 20 } }` を受診した際、3 回リトライが実行された後に `MyApiClient::ApiLimitError` として例外処理される。  it do    expect { api_request! }      .to be_handled_as_an_error(MyApiClient::ApiLimitError)      .after_retry(3).times      .when_receive(body: { errors: { code: 20 } }.to_json)  endendまた、 ExampleApiClient は stub_api_client や stub_api_client_all を使用するとスタブ化できます。スタブ化した状態だと任意の API レスポンスを返すか、任意の例外を発生させる、という動作になってリトライが発生しなくなるので、上記の問題はありませんでした。stub_api_client_all(ExampleApiClient, get_users: { users: [{ id: 1 }, { id: 2 }, { id: 3 }] })response = ExampleApiClient.new.get_usersresponse.users # => [{ id: 1 }, { id: 2 }, { id: 3 }]#175 Verify arguments on error handling definitionerror_handling の定義でレスポンスのステータスコードを指定することができるんですが、このオプション名が status_code なのか status なのかをよく間違える、という問題がありました。$ rails g api_client を使用するとテンプレが作成されるので、そこからエラーハンドリングの定義を行うと間違えにくいのですが、後からエラーハンドリングを追加する時とかにやらかします。作者自身もたまにやらかしてました😇# 正解error_handling status_code: 400..499, raise: MyApiClient::ClientError# 間違いerror_handling status: 400..499, raise: MyApiClient::ClientErrorこの PR の対応で間違ったオプションを指定すると以下のような例外が発生するようになりました。RuntimeError:  Specified an incorrect option: `status`  You can use options that: [:response, :status_code, :json, :with, :raise, :block]#176 Provides a syntax sugar of retry_on on error_handling最初の PR でも出てきた retry_on ですが、 error_handling raise: MyApiClient::ApiLimitError でも同じ例外を指定していて DRY な感じじゃなかったり、retry_on と error_handling をそれぞれ定義してるとお互いの関連が実感しづらい、という不満がありました。class ExampleApiClient < MyApiClient::Base  endpoint 'https://example.com'  retry_on MyApiClient::ApiLimitError, wait: 1.minute, attempts: 3  error_handling json: { '$.errors.code': 20 }, raise: MyApiClient::ApiLimitError  # GET https://example.com/users  def get_users    get 'users'  endendこの PR では retry というオプションを error_handling に追加しています。これにより、以下の 2 つのコードは等価になります。retry_on MyApiClient::ApiLimitError, wait: 1.minute, attempts: 3error_handling json: { '$.errors.code': 20 }, raise: MyApiClient::ApiLimitErrorerror_handling json: { '$.errors.code': 20 },                           raise: MyApiClient::ApiLimitError,                           retry: { wait: 1.minute, attempts: 3 }retry_on にオプションを指定する必要がなければ retry: true と書けば OK です。error_handling json: { '$.errors.code': 20 },                           raise: MyApiClient::ApiLimitError,                           retry: trueただし、 retry オプションを使用する際は以下の点に注意が必要です。error_handling に raise オプションの指定が必須となります。Block を使った error_handling の定義は禁止されます。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[認証認可の情報の追い方みたいな]]></title>
        <id>https://developer.feedforce.jp/entry/2020/01/10/185250</id>
        <link href="https://developer.feedforce.jp/entry/2020/01/10/185250"/>
        <updated>2020-01-10T09:52:50.000Z</updated>
        <summary type="html"><![CDATA[今日の社内勉強会担当だった id:tmd45 です。去年「年が明けたらすぐに勉強会担当だから年末年始連休中にがんばるぞい」と考えていたことをすっかり忘れて、前々日に「今週末の担当やん！」と気づいたので肝が冷えました。今年は Switch 版 脳トレで記憶力を強化していきたい所存。2019 年後半は認証認可界隈の面白い話がたくさんあったと思っていて、それをざーっと話そうかなとスライドを書き始めたんですが、そういう情報を読むときにどんなことを考えてるのかというのを想像してみたら面白かったのでそっちの話になりました。スライドp.21 以降の『付録: 認証認可の未来の話』が本編だったかもしれない（発表時間5分）。「認証認可こわくないよ〜🤗」というのを伝えたかったんですが、登場人物の多さで逆に怖がらせてしまった感が否めなかったですね😇"エンジニアの情報収集の仕方" 程度になにかの役に立てば幸いです。宣伝: ruby-jp Slackruby-jp.github.ioスライドの中でも紹介していますが #authz というチャンネルで認証認可の話をしています。していますというか、自分は聞いているばかりなので :感謝しかない: 。興味あるかたはぜひ Join してみてくださいね！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[私と gem]]></title>
        <id>https://ryz310.hateblo.jp/entry/2019/12/15/224312</id>
        <link href="https://ryz310.hateblo.jp/entry/2019/12/15/224312"/>
        <updated>2019-12-15T13:43:12.000Z</updated>
        <summary type="html"><![CDATA[どーも、サトウリョウスケです。金曜日に登壇した勉強会 で うっかり ブログ作るって言ってしまったので 10 年ぶりくらいにブログを復活させてみました。ブログのタイトルも 10 年前のタイトルと同じです（元ネタはファミコン時代のドラクエ IV）勉強会の感想記事は近日中に書こうと思います✍️この記事は Feedforce Advent Calendar 2019 の 15 日目です。さて、最初の記事からいきなり会社のアドベントカレンダー記事になります🙏adventar.org昨日は Yutaka KAWAI さんの「コーヒーは科学である ~抽出器具による味の違い~ ペーパードリップ編」でした。note.com珈琲屋さんかな？ってくらい凄い記事でしたね☕️記事に出てきたドリッパーは全種類持ってるってヤバくないですか？？？（もちろん良い意味で）ペーパードリップ編ってことは続編もあるのかな？この感じで記事が量産されたらそのうち書籍化されるかもしれません📚本編予告通り個人で gem を作る流れについて話そうと思うのですが、アドベントカレンダーから流れてくると非エンジニアの方もきっと読まれると思うので、あんまり技術的な話題にせずにフワッとした話でもしようかと思います。そもそも gem ってなんだっけ？そもそも gem っていうのは Ruby でできたライブラリの事でして、例えば僕が凄く便利なプログラムを書いて、それを gem として公開すれば、世界中の人が僕のイケてるプログラムを使えるようになる、というものです。まさに Win-Win しかない仕組み。gem は世界を救います。一方で、プログラムってのは新機能が追加されたり、不具合が修正されたりして日々アップデートが繰り返されています。「この機能にはバージョン 1.3 以降でないと使えません」とか「色々イケてない部分が多いからこの機能は廃止します」という変更もあるので、自分のプログラムは一体どのバージョンの gem を使っているのか、という話が物凄く重要だったりします。gem にはどのバージョンを使っているのか（依存しているのか）という情報を管理する機能も備わっていますので、ある日突然新バージョンで挙動が変わっても「うちは一個前のバージョン使ってます！」という管理ができていれば動かなくなることはないのです。とはいえ新バージョンでいきなりそんなトンデモ変更されたら困りますけども。僕が初めて作った gem自分が生まれて初めて作った gem はこの rubocop_challenger という gem です。github.com一般的に gem は他のプログラムと組み合わせて使う事が多いのですが、 rubocop_challenger は単体で動作するやつでして、実行したディレクトリ（フォルダ）にあるプログラムのソースコードを少しずつ綺麗に（人間にとって読みやすくしたり、書き方のルールを統一したり）してくれる、という gem です。というと物凄い神 gem ですが、RuboCop というソースコードを解析してくれる便利な gem を内部で呼び出してプルリク（プルリクが何なのかはググってください）作ってくれる、という仕組みになっているので、「人間が毎日手作業でやらないといけなかった事を自動的にやる」というのが rubocop_challenger の提供する価値になります 🤖詳しくはちょうど一年くらい前に会社のブログに書いたので、ご興味ありましたら是非。古い内容なので、現在のバージョンからは少しずれてますけど。developer.feedforce.jpちなみにもうすぐ v2.0.0 をリリースする予定です。（pre バージョンですが、現時点でもすでに使えます）ニッチな gem初めて作ったのは rubocop_challenger という「人間が毎日手作業でやらないといけなかった事を自動的にやる」 gem でした。完全に自分の会社のプロダクト用に作った gem でしたが、 Twitter などをエゴサしてると、ぼちぼち使って頂けているようです。同じような悩みを抱えている人は世の中にはいるもんですね✨とはいえ、 gem を公開したら世界中の人たちが使ってくれる、ということには中々ならないです。一応頑張って英語で説明を書いたりはしていますが、個人の発信力には限界もありますし、何よりニッチです。というか、個人が作る gem なんて大抵はニッチなものになります。「あー、こんな gem あったらめっちゃ便利やん？」という gem は大抵世界のどこかの誰かが作ってます。なので、今までにないような新しいgem を作ろうと思ったら大抵ニッチになります。じゃあ gem を公開しても大して使ってもらえないし、あんまり意味ないじゃん、って思うかもしれませんが、意味無くはないんですよね👍効能 1. スキルアップまず、めっちゃプログラムを書く勉強になります。Ruby で Web 開発をしている人は大抵 Ruby on Rails を使って書いてると思います。ちなみに Ruby on Rails も gem です。Ruby on Rails は凄くよく出来ているので、Web 開発の難しい部分を 9 割くらいの肩代わりしてくれます。ところが、自分で一から gem を作ろうとすると、自分の力で解決しないといけないプログラム的な課題がめっちゃあります。Web 開発はインターネット特有の課題が多いですが、 gem の開発には Web 開発以外の知識も色々要求されたりします。何より、自分自身で仕様を一から考えないといけないので、普段の開発以上に意思決定量がめっちゃ多いです。自分は rubocop_challenger 以外にもメンテナンスしている gem が 3 つほどありますが、これらの開発を通して日頃の Web 開発の品質も一段レベルが上がったな、と感じる事が多いです。なので、gem の開発は自分自身の修行のためだと思ってやると良いかもしれません。誰かに使えてもらえたらラッキー、みたいな。逆に使ってもらえる事をモチベーションにするとちょっと辛いかもしれないです。思った以上に流行らない。もっと流行れ！効能 2. ポートフォリオgem を公開してるせいか、企業からのスカウトがめっちゃ来るようになります。自分自身、会社の採用活動に関わる機会が多いのですが、例えばスカウト候補を探す際に候補者の GitHub は必ず見るようにしています。その経験からですが自分で gem 書いて公開しているエンジニアは世の中の 1 割もいないんじゃないかな、って思っています。自分も gem を作るようになったのはここ 1 ~ 2 年ですし、前職にいた頃だと普段の仕事の帰りが遅かったりもしたので家に帰ってから gem を作るような余裕もありませんでした。なので、本人の熱量や環境が整わないと gem の開発は難しいかもしれません。しかしながら、自分も採用活動していて「お。この人すごいやん」ってなるのは GitHub や Qiita とかで何かしらアウトプットのある方なんですよね。もちろんアウトプットが無くても実際会ってみたら凄かったって人は沢山いるんですが、採用活動だとその人の仕事でのアウトプットって見えないもんですから。。別に gem じゃなくても良いんですが、自分自身のキャリア形成を意識するためのアウトプットの一環としてとても有用だと思います。効能 3. 魔法のアイテム自分が今年作った gem に my_api_client という gem がありまして、API Client を作るためのフレームワークなんですが、これはプロダクトのソースコードにも使っています。自分が携わってるサービスは ソーシャルPLUS というものでして、企業の Web サービスとソーシャルログインプロバイダー（ LINE とか Twitter とか）のハブになってるサービスなんですね。そのせいもあって、外部の Web API へリクエストするという処理が多く、毎回同じようなエラーハンドリングやリトライの処理を何度も書かないといけなくて大変だった訳です。あと、エラーが発生した際に「ソーシャルPLUS」「ソーシャルPLUSを利用している企業」「ソーシャルログインプロバイダ」の誰が原因なのかを特定するための情報をログに残すとかも都度対応しないといけなくて超大変でした。my_api_client はその辺の処理をすっきり簡単に書けるようにするための gem でして、すっきり簡単なもんだから、チームメイトが my_api_client を活用して自発的にガンガン課題を解決してくれる、という最高にホットな状況を産み出すことに一役買っております。やっぱエンジニアも人間なので、普段の業務で忙しい中で改善活動も同時にやろうなんてモチベーションは普通は湧いてこない訳です。でも、これを使えばすっきり簡単だよ、っていう魔法のアイテムがあれば、みんなの重い腰を少しだけ軽くする事ができます。my_api_client は自分の中でも割とよくできた gem なので、毎回こんな良い gem が作れる訳じゃないですけど、 gem を作ってチームの生産性が上がるってのはやっぱ魔法のアイテムだなぁって思うんです。gem は世界を救います！（２回目）なお、 my_api_client については 銀座 Rails #10 で登壇したときの資料があるので貼っておきます。これも若干古いので、最新の仕様とは少し異なるかもしれません。最新の仕様は こちら をご覧下さい。効能 4. たのしい最後はここに帰ってくるんですが、自分の gem を作るのはやっぱ楽しいのです。いきなり頭悪い文章になりました。いつから頭良い文章書いてると錯覚していた？以前勉強会で「これまでどういうキャリアを意識してやってきましたか？」って若手のエンジニアから聞かれたのですが、自分みたいな 30 半ばのエンジニアって、エンジニアになった当初は今みたいにエンジニアが持て囃される時代でもなかったので、自分からエンジニアになろうって思った人は少なからず「ただプログラムが好きだった」っていう人が多いんじゃないですかね？わかんないですけど自分はそうでした。自作の gem を作ってると「やっぱプログラム書くのって楽しい」ってのを思い出させてくれます。自分は他にも趣味でバンドやったり絵を描いたりしていますが、プログラミングが子供の頃に好きだった工作に一番近いような気がしています。中々プライベートで時間を作るのは難しいんですが、やはり楽しさが一番根底の原動力になっているのかもしれないですね。まとまらないまとめ会社のアドベントカレンダー向けだし、非エンジニアにも伝わるようなフワッとした文章にしようと思って書いてたら、途中から俺のポエムを書き殴ってただけになった気がしますが、役に立つかどうかは 2 の次にして、とりあえず gem 作ってみると楽しくスキルアップできるし、もしかしたら誰かの魔法のアイテムになってるかもしれないよ、というお話でした。久々にゆる〜い文章書いてて自分的には楽しかったです 笑さて、明日の Advent Calendar は？（CV. 加藤みどり）Feedforce Advent Calendar 2019、明日は上岡君が「野球についてor遠隔インターンについて」書いてくれるみたいです。最近はインターンも遠隔で出来るんですね。野球も遠隔インターンも未経験のままおじさんになってしまったので自分には未知の領域です⚾️💨乞うご期待！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heroku Meetup #27 で『デプロイで止まらないバッチ処理を求めて』という話をしてきた #herokujp]]></title>
        <id>https://developer.feedforce.jp/entry/2019/12/14/153000</id>
        <link href="https://developer.feedforce.jp/entry/2019/12/14/153000"/>
        <updated>2019-12-14T06:30:00.000Z</updated>
        <summary type="html"><![CDATA[土曜日にこんにちは id:masutaka26 です。この記事は Heroku Advent Calendar 2019 の 14 日目の記事です。qiita.com昨日は @takahito0508 さんの『Salesforce の Org Development 用の Buildpack を作った話』でした。Heroku の Buildpack は割と簡単な仕組みなので、ハードルが低いですよね。さて、同じ日（2019/12/13）に開催された Heroku Meetup #27 "Heroku Vitamin!" で『デプロイで止まらないバッチ処理を求めて』というお題で発表してきました。Heroku meetup は前々回 #25 以来の参加と登壇です。developer.feedforce.jp所感世の中では割と多くのバッチ処理が動いていると思います。ですが、Heroku 上でのバッチ処理はあまり情報がないので、今回紹介させて頂きました。技術スタックとしては、Rails の Active Job と Sidekiq です。今回もニッチな発表ができてとても満足しています。会場にいる一人に刺されば大成功くらいの気持ちで発表していました。中の人曰く、こんな One-Off Dyno の使い方をする人は初めて見たとのこと。(^^;使い方としては問題ないそうですが、Private Spaces では Dyno の起動が遅いそうなので、そこはやや注意とのこと。※ EC Booster では一般向けの Common Runtime を使っています。とは言え、Dyno Create API は Dyno の起動完了を待たずに即座に終了するため、Private Spaces でも大丈夫みたいです。.@masutaka さんのお話 #herokujp pic.twitter.com/RZnLFWq9qn— Satoshi Nagano (@thisisnagano) 2019年12月13日明日は @seijikohara さんです。お楽しみに。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[シングルサインオンのひとことで片付けない]]></title>
        <id>https://blog.tmd45.jp/entry/2019/12/09/090000</id>
        <link href="https://blog.tmd45.jp/entry/2019/12/09/090000"/>
        <updated>2019-12-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[この記事は 認証認可技術 Advent Calendar 2019 の 9 日目の記事です。前日は Munchkin さんの『認可機構によるアクセス制御とビジネスロジックによるアクセス制御の使い分け』でした。kishisuke さんの『Sign in with Apple+Cordovaについて』でした。id:tmd45です、ごきげんよう。仕事では主に認可の RP 側として toC な認可 IdP（LINE, Google, Yahoo! JAPAN, Facebook, Twitter, など）に接しています、よろしくお願いします。§認証認可界隈でお仕事していると、社内からもお客様からも「シングルサインオン」（Single Sign-On, 略称 SSO）という便利機能について話題に上がることがあります。ただこの「シングルサインオン」という単語の定義がちょっとあいまいで、開発者からすると「シングルサインオンをやりたい」と言われて想像するものに認識のズレがあったりします。たとえば、私が聞いたことがあって「それはシングルサインオンなのか…？」と引っかかったのはこんなところ:ソーシャルログイン＝シングルサインオンID/Password による自社会員 DB でのログインしかなかったサービスで、ソーシャルログインが導入されたときにプレスリリースに「SSO 対応！」というウリ文句が 🤔LINE Login v2.1 から SSO 対応ブラウザでの LINE ログインセッションが維持され、v2.0 までは認可フロー中に毎度（LINE ログイン画面上での）ID/Password の入力が必要だったものが、不要になったというもの 🤔「シングルサインオン」は Wikipedia でも"不十分" な記事としてふわっと書かれており、"ユーザがシステムごとにユーザIDとパスワードの組を入力する必要がなくなる" という意味ではこれらは正しそうです。詳細な実装を見て広義に「これはシングルサインオンだね」と呼ぶのはいいのですが、開発者が困るのは、要望で「シングルサインオンをやりたい」と言われることです。§というわけで、今回は自分の思いつく範囲で「シングルサインオン」でないものとあるものをいくつか図で整理してみようと思います。注）このあと添付する図中の URL のドメインやパスはイメージです。わかりやすさのために実在するサービスのドメインも利用していますがパスについては不正確ですのでご了承ください。またその例として挙げているサービスについて批判や毀損を意図しているものではございません（あんまり適当すぎるドメインだと話がわかりづらくて…すみません）。明らかにシングルサインオンじゃないやつどこに行っても ID と Password の入力を求められるやつ。シングルサインオンに対応していない単一サービス単一サービスの場合は、そもそもシングルサインオンそのものが不要。シングルサインオンに対応していない複数サービス群企業から見れば１社内に複数サービス有していなければこの構図にはならないが、ユーザから見るとインターネット上のあらゆるサービスは大きくまとめて「複数サービス群」とも捉えられる。そういう意味では「単一サービスの場合は、そもそもシングルサインオンそのものが不要」とも言い切れなくなる。ソーシャルログインからログインセッションをつくるソーシャルログインを用いると、たいていの場合、ユーザが ID と Password を入力するのは認可プロバイダ（ソーシャルログインプロバイダ）のログインセッションを確立する初回アクセス時のみになる。ユーザが普段から利用している（＝ログイン済みである）サービスが認可プロバイダとなれば、ユーザは認可フローのなかで「認可する」以外のアクションが不要となるため、手間が減るという利点がある。サービス側は認可プロバイダから得られた属性情報（プロバイダ側のユーザID など）をもとにして、サービスのログインセッションを確立する。ソーシャルログインに対応したサービス（広義のシングルサインオン）インターネット上のあらゆるサービスを大きくまとめて「複数サービス群」と捉えた場合、認可プロバイダを介したシングルサインオンになっていると言える。これが認可プロバイダ側で都度 ID/Password の入力を求める形になっていると、その利点が死んでしまう。残念な例: ソーシャルログインプロバイダがログイン状態を維持しないタイプとはいえ、このケースでも決済前にはあえて再認証を行わせるなど（シングルサインオンとは直接関係しないが）都度認証する手間とセキュリティの向上はケースバイケースでバランスを考える必要がある。想像しやすいシングルサインオンここからやっと、シングルサインオンと言われて（自分が）普通に想像するパターン。ユーザはサービス群のどこか１箇所でログインしていれば、ID/Password 入力の手間をかけずに関連する他のサービスを利用することができる。シングルサインオンに対応した複数サービス群（１サービスのログインに依存）図中にも書いたが、上記はこんなケースだったりする:example.com サービスを作ったあとに example2.com サービスを追加したユーザに手間をかけさせることを避けるためにシングルサインオンを導入したい開発が大変そうなので認証基盤システムを別で作るのは避けたいこういう場合、ユーザはシングルサインオンできて便利かもしれないが、開発者側としては「 example2.com が example.com に依存しており身動きが取りづらい」と感じて、さらなるサービス拡大をしづらくなっている状態だったりする。シングルサインオンに対応した複数サービス群（認証基盤をはさむ）パッと見て健全なのがこの構成。ただし実質 example.com と example2.com に加えて認証基盤である auth.example.com という３つのシステムを管理することになる。開発事情としては、（それぞれは相当ミニマムでなければ）開発者をそれなりの人数確保したり、チームを分割して運営したいところ。…というのが前時代的な話で、最近だと IDaaS（Identity as a Service）などを活用することで、認証基盤システムのお守りを自組織からアウトソースするという手もあります。おまけ: シングルサインオンにするかどうかわからないけど備えておくIDaaS の台頭で、シングルサインオンにするかどうかはともかく、認証基盤部分を早めにアウトソースするなど選択肢も広がりました。§はてさて「シングルサインオン」をキーワードに、ソーシャルログインや IDaaS といったものを絡めて書いてみました。構成を考えるスコープを広めたり狭めたり、必要なセキュリティレベルとのバランスを見たり、開発組織の規模やスキルを加味したり、どんなシステムでもそうですが考えることはたくさんあります。認証基盤に関わる皆様に置かれましては「シングルサインオン」という定義の曖昧な単語ドーーーン！ではなく、ユーザにどうあって欲しいか、開発をどのようにしていくべきか、などなど具体的なストーリー（要望、仕様）で会話できるといいかなと思う次第にございます。§明日は olt さんの『TwitterのOAuth1.0 認可用の中間サーバNode.jsで構築する』 お話とのこと。ではでは。編集後記blog.tmd45.jp]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[月末に起動したいバッチを sidekiq-cron で指定し、healthchecks.io で監視する]]></title>
        <id>https://developer.feedforce.jp/entry/2019/12/05/101931</id>
        <link href="https://developer.feedforce.jp/entry/2019/12/05/101931"/>
        <updated>2019-12-05T01:19:31.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。id:masutaka26 です。去年の 6/30 からジムに通い始めて、なんとか週に 2~3 のペースで続いています。サプリメントも半信半疑ながら毎日飲んでいますが、ドラッグストアや Amazon で買うと割と高いと思います。個人的には iHerb がだいぶオススメです。以前 3.6kg のプロテインを買ったらなぜかキャンペーンが適用されて、30% オフの 6,517 円になりました。さらにこの案件動画を見て、プレワークアウトサプリ（C4 SPORT）にも手を出してしまいました。BCAA も飲んでいます。Twitter とかで検索すると、初回 10% OFF の紹介コードが見つかると思います。さらにお得です。何の記事でしたっけ...？そうだ、この記事は Feedforce Advent Calendar 2019 の 5 日目の記事です！昨日は id:kano-e の「2019 年に会社ボドゲ棚から自宅に持ち帰ったボドゲたち」でした。変わらぬボドゲ愛・・・！ff-boardgame.hatenablog.comsidekiq-cron で月末を指定する脈絡ありませんが、sidekiq-cron では last で月末を指定することが出来ます。例えば 0 12 last * * Asia/Tokyo の次回実行は 2019/12/31 12:00 JST です。2019/11/30 12:00 JST は無事動きました。このような設定になります。sample_job:  active_job: true  cron: "0 12 last * * Asia/Tokyo"  class: "SampleJob"  queue: defaultこの cron 書式は sidekiq-cron が依存する fugit による実装です。確認はしていませんが、last の他にも -1 や L などが使えるようです。🔗 https://github.com/floraison/fugit/blob/v1.3.3/spec/cron_spec.rb#L734-L747context 'negative monthdays' do  [    [ '* * -1 * *', '* * -1 * *' ],    [ '* * -7--1 * *', '* * -7,-6,-5,-4,-3,-2,-1 * *' ],    [ '* * -1--27 * *', '* * -31,-30,-29,-28,-27,-1 * *' ],    [ '* * -7--1/2 * *', '* * -7,-5,-3,-1 * *' ],    [ '* * L * *', '* * -1 * *' ],    [ '* * -7-L * *', '* * -7,-6,-5,-4,-3,-2,-1 * *' ],    [ '* * last * *', '* * -1 * *' ],  ].each { |c, e|    it("parses #{c}") { expect(Fugit::Cron.parse(c).to_cron_s).to eq(e) }  }endnegative monthdays って表現は面白い。healthchecks.io で月末のバッチを監視するバッチが本当に起動したかを監視するのは難しいと思います。月末の監視はさらに難しいと思います。過度な依存は禁物ですが、healthchecks.io は選択肢に入れても良いかもしれません。以下の記事をどうぞ。developer.feedforce.jp前述の cron 設定だと 0 12 l * * で監視することが出来ます。l は小文字のエルです。terraform-provider-healthchecksio の設定例です。resource "healthchecksio_check" "sample_job" {  name     = "SampleJob"  tags     = ["production"]  grace    = "60"  schedule = "0 12 l * *"  timezone = "Asia/Tokyo"  channels = [data.healthchecksio_channel.slack.id]}data "healthchecksio_channel" "slack" {  kind = "slack"}こちらの cron 書式は healthchecks が依存する croniter による実装です。l 以外の指定は見当たりませんでした。🔗 https://github.com/taichino/croniter/blob/0.3.29/src/croniter/tests/test_croniter.py#L209-L223def testLastDayOfMonth(self):    base = datetime(2015, 9, 4)    itr = croniter('0 0 l * *', base)    n1 = itr.get_next(datetime)    self.assertEqual(n1.month, 9)    self.assertEqual(n1.day, 30)    n2 = itr.get_next(datetime)    self.assertEqual(n2.month, 10)    self.assertEqual(n2.day, 31)    n3 = itr.get_next(datetime)    self.assertEqual(n3.month, 11)    self.assertEqual(n3.day, 30)    n4 = itr.get_next(datetime)    self.assertEqual(n4.month, 12)    self.assertEqual(n4.day, 31)2019 年もそろそろ終わりです。年末にだけ動くバッチを設定＆監視するのも良いかもしれません。明日は、こばりゅうの記事です。ひこうきかっけー！？お楽しみに。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ソーシャルPLUS の技術スタックを整理してみた]]></title>
        <id>https://developer.feedforce.jp/entry/2019/11/25/120000</id>
        <link href="https://developer.feedforce.jp/entry/2019/11/25/120000"/>
        <updated>2019-11-25T03:00:00.000Z</updated>
        <summary type="html"><![CDATA[ソーシャルPLUS 開発チームリーダーの id:tmd45 です。ごきげんよう。ソーシャルPLUS チームではバックエンドエンジニアの絶賛採用活動中なのですが、そのときにまとめた技術スタックについて、採用メディアにだけ使うのももったいないと思ったので、普通にブログ記事に書いてみることにしました。よろしくおねがいします。'`ｨ (ﾟдﾟ)/ソーシャルPLUS って？システムの役割ソーシャルログインサービスLINE メッセージ配信サービス技術スタックの紹介バックエンド（サーバーサイド）フロントエンドインフラストラクチャその他おわりにソーシャルPLUS って？おそらく知らない方が大半だと思うので、かんたんにサービス紹介をば。socialplus.jpソーシャルPLUS は、WEB サイトでの会員登録や再ログインがかんたんになる「ソーシャルログイン」を複数プラットフォームまとめて一括で実装できる BtoBtoC の ID 連携サービスです。対応するプラットフォーム（認可プロバイダ）には LINE、Yahoo! JAPAN、Google、Facebook、Twitter があります。とくに 2016年からは LINE Messaging API によるメッセージ配信や 1to1 トークに対応し、コンシューマー（エンドユーザー）への継続的なリーチや商品購入後のカスタマーサポートなどが可能になりました。ID 連携によりエンドユーザーとのコミュニケーションチャネルが広がり、さらに精度も高められるようになります。企業の課題を解決するだけでなく、企業が実現したい事の先にいるエンドユーザーにとってメリットがあるかを重要視したユーザーファーストの考え方で、機能追加やサービスの活用提案を行なっています。2012 年 4 月からサービスを開始し、7 年目となる今年もまだまだ成長しているサービスです。システムの役割ソーシャルPLUS では大きく以下の２つの Rails アプリケーションが中心になっています。ソーシャルログインサービスソーシャルログインの提供プロバイダからのユーザ情報取得API 提供設定管理画面こちらはサービス稼働当初から存在する、コアとなる機能を扱っています。昔ながらの MVC すべてを Rails で構築しているアプリケーションです。瞬間的なアクセス増加に耐える運用設計や、パフォーマンス改善、プロバイダの仕様変更への追従が、最近の主なタスクでした。技術スタックは後述しますが、「ソーシャルログインの提供」と「設定管理画面」が１つのアプリケーションとして動いているのは、可用性も保守性もいまいちなので近々分離したいと考えている部分です。LINE メッセージ配信サービススケジュール一括配信メッセージ（Webhook）受信1to1 メッセージ（チャットのようにエンドユーザーと対話できる機能）API 提供新機能: ステップ配信機能 開発中メッセージ配信サービスは 2017 年 4 月から提供を始めた（このプロダクトのなかでは）比較的新しいサービスです。フロントエンドを React & Redux で、サーバーサイドを Rails の API モードで構築しています。これまでエンドユーザーからのメッセージを漏れなく受信するためのアーキテクチャや、メッセージ配信のパフォーマンスを上げる施策に取り組んできました。直近ではデザイナーが参加し、より使いやすい UI や UX を意識しながら新機能の開発を進めています。技術スタックの紹介現在、ソーシャルPLUS チームには８名の開発者が在籍していますが、メンバーの得意な技術領域によってバックエンド、フロントエンド、インフラのタスクを分けて取り掛かり、相互にフォローしながらチーム開発を進めています。もちろん本人が希望すればどんな仕事でも関わってもらって構いません。実際、フルスタックを志して手が空けばいろんなタスクを消化しているメンバーもいます。バックエンド（サーバーサイド）ソーシャルログインサービスは Ruby on Rails 4.2（来年中にはバージョンアップ予定 2020年9月 バージョンアップしました！）LINE メッセージ配信サービスは SPA（Single Page Application）として構築しており、サーバーサイドは Ruby on Rails 5.2 の API モードを利用いずれも RDB は MySQL、インメモリデータストアは Redis を利用RSpec による機能テスト（85 〜 91 % のカバレッジ）Bugsnag, Scout によるアプリケーションレイヤーの監視Google BigQuery へのログ集積システムの一部に AWS Lambda, Amazon DynamoDB, Amazon Kinesis などを利用フロントエンドReact, TypeScript, Firebase, PWA などを用いた BtoB 向け SPAAtomic Design をベースに UI を構造化、Storybook でコンポーネントを管理Flux アーキテクチャにはおなじみの Redux を用い、ファイル構造は re-ducks パターンを採用モジュールのテスト、E2E テストや ビジュアル回帰テスト等にてプロダクトの品質を維持Bugsnag, LogRocket によるフロントエンドアプリケーションレイヤーの監視インフラストラクチャAWS を中心に構成。Route 53, ALB, EC2, Aurora MySQL ほか複数 AZ（Availability Zone）に冗長化、構成変更は Blue/Green DeploymentEKS や ECS へのコンテナ化移行を検討中（技術検証や運用コストを踏まえて慎重に検証を重ねています）Infrastructure as Code に従って Terraform や Chef でコード管理作業の自動化を意識。サーバー台数の増減はほぼ自動で行われる仕組みを実現Datadog を利用した監視、異常は Slack へ通知その他全体で Slack, GitHub, CircleCI, Redash を利用リリース作業を ChatOps 化（中継に Jenkins を利用）リモートペアプロも挑戦中（おもに Slack Call と画面共有。方法は模索中）モニタリング内容やアラート、タスクカード（GitHub Project）はチームの席近くにある大型ディスプレイに表示して、朝会やイテレーションミーティングで眺められるようにしています。リモートじゃないモブプログラミングなんかもここでやります。カンバンディスプレイおわりにすべての技術スタックについて細かに説明すると相当長くなるので、ざっくりになりました。技術スタックだけでなく、チームの仕事の仕方なんかも今後公開していければと思います。弊社ではカジュアル面談も行っていますが、ビジネスの説明からこういった技術スタックの解説まで、私やチームメンバーからさせていただいてます。選考というフローに進む前にぜひ会社やプロダクトについて知ってもらえればと思っておりますので、気になる部分があればお気軽にご質問ください〜 (・ω・)ノ]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slack に便利機能 "ワークフロービルダー" が増えたので勇み足で使ってみた]]></title>
        <id>https://developer.feedforce.jp/entry/2019/10/16/140901</id>
        <link href="https://developer.feedforce.jp/entry/2019/10/16/140901"/>
        <updated>2019-10-16T05:09:01.000Z</updated>
        <summary type="html"><![CDATA[Slack に「ワークフロービルダー」という機能ができたので現時点のメモ。ワークフロービルダーが新登場 : Slack で簡単にタスクを合理化 | The Official Slack Blogどこにあるの？いまのところ、左上（ワークスペース名）のメニューから使える。ワークスペースのメニューワークフローの編集上のメニューをクリックすると別ウィンドウが開いてこういう編集ができる。ワークフロービルダーの初期画面「ワークフロー」作ってみた例ワークフローの編集「設定」の内容ワークフローの設定Slack チャンネル上でできることワークフローを登録したチャンネルに ⚡ マークが出てきて、そこからアクションが始められる。このトリガーも何種類かあるみたい。"このチャンネルのアクション"ワークフローで作ったフォームが表示された例。セレクトボックスに複数選択タイプは（いまのところ）なかった。ワークフローで作ったフォームの表示Slack チャンネルで動いた様子Slack チャンネルで動くワークフローなんか上手いこと使えるといいですね！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[「aws-vault loginでChromeのウィンドウをAWSアカウント毎に分離する」を Alfred 用に作った]]></title>
        <id>https://blog.tsub.me/post/create-alfred-aws-vault-workflow/</id>
        <link href="https://blog.tsub.me/post/create-alfred-aws-vault-workflow/"/>
        <updated>2019-10-06T05:35:00.000Z</updated>
        <summary type="html"><![CDATA[tsub/alfred-aws-vault-workflow: A Alfred workflow to open the AWS Management Console with aws-vaultChrome 版Firefox (Multi-Account Container extension) 版aws-vault 自体今回初めて知ったのですが、以下の記事を読んで、複数の AWS アカウント使いには大変便利そうだったので Alfred 用のものをシュッと作りました。aws-vault loginでChromeのウィンドウをAWSアカウント毎に分離する - Qiita]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[【2019年版】バックエンドエンジニアが React でモダンなフロントエンド開発を始めるまで]]></title>
        <id>https://developer.feedforce.jp/entry/2019/09/04/140000</id>
        <link href="https://developer.feedforce.jp/entry/2019/09/04/140000"/>
        <updated>2019-09-04T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[id:daido1976 です。入社してからあっという間に1年が経っていました。直近3ヶ月ほどプライベートでフロントエンド開発の勉強をしていたのですが、ここ数年で CSS の Grid や React の Hooks が新しく導入されたことなどもあり、少し古いコンテンツだと教材として役立たない1 と感じることが多かったので、本記事では私が実際にやってみた中で 2019年時点で オススメできると判断した教材や学び方を皆さんにご紹介したいと思います。はじめにやったことJavaScriptMDN の JavaScript の部分を読む & 手を動かすJavaScript Primer を読むYouTube 動画で Promise を学ぶデバッグ方法を学ぶReactReact 公式のチュートリアルを2周するegghead.io の動画で Redux を学ぶヘルシンキ大学の Web 開発カリキュラム Full Stack Open 2019 をやるCSSCSS in Depth を読む & 手を動かすその他Atomic Design を理解する最後にはじめに今回は実務レベルでフロントエンド開発できるスキルを身につけたい、という目標があったので JavaScript はもちろん所属チームである EC Booster で採用している React、さらにフロントエンド開発において避けて通ることのできない CSS の学習を前提としています。参考までに筆者のスペックは 👇 の記事の1年後をご想像ください。developer.feedforce.jp具体的にはこの後 Ruby や Rails を使って、SPA のための GraphQL API サーバや Google 広告系の API を利用したバッチ処理の実装など、バックエンド開発をメインに行なっていました。やったことJavaScriptMDN の JavaScript の部分を読む & 手を動かす所要期間: 約5日Rails 開発をする前に Ruby の基礎をがっつりやったことで早期にキャッチアップできた経験から、まずは JavaScript の言語仕様をしっかり把握しようと思いました。ここでは React でも頻出の import/export、アロー関数、分割代入に加えて、変数のスコープ（Ruby と違って最初戸惑った）などについて学びました。数行のスクリプトを書いて実行、をとにかくひたすら繰り返していました。developer.mozilla.orgJavaScript Primer を読む所要期間: 約2日こちらは ES2015 を前提とした JavaScript の入門書。MDN で分かりづらかった部分を補完したり、ES2015 以降の仕様のみを把握したい時に利用しました。jsprimer.netYouTube 動画で Promise を学ぶ所要期間: 約0.5日Promise、難しいですよね…。有名な JavaScript Promiseの本 を読んだのですが、自分には難しくて中々理解できませんでした。色々探し回った結果、以下の YouTube 動画がものすごく分かりやすくて何とか概要を掴むことができました。youtu.beデバッグ方法を学ぶ所要期間: 約0.5日Ruby には pry という大好きなデバッガー（としても使える）ライブラリがあるのですが、JavaScript にも debugger ステートメントという素晴らしいものがあるのだと知りました。JavaScriptのデバッグ方法 – JSを嫌いにならないためのTips | POSTDあと Chrome DevTools の使い方も以下の記事を読んで把握しました。ChromeのデベロッパーツールでJSをデバッグする方法（2019年版） - ICS MEDIAReactReact 公式のチュートリアルを2周する所要期間: 約2日JS を学ぶ前に一度やり、JS を学んでからもう一度やりました。React の概要を掴むには良かったです。ただ、このチュートリアル内では Class Components が前提で Hooks については全く触れられてないので、後述の Full Stack Open 2019 で補完しました。ja.reactjs.orgegghead.io の動画で Redux を学ぶ所要期間: 約2日Redux 公式のドキュメント をさっと読んでから Redux の作者である Dan Abramov 氏が作成した以下の動画を観ながら手を動かしました。序盤で createStore を自分で実装してみようという内容があるのですが、それがとても分かりやすかったです。egghead.ioヘルシンキ大学の Web 開発カリキュラム Full Stack Open 2019 をやる所要期間: 約10日React × Express.js × MongoDB の構成でイチから SPA 作れるようになろうぜ、という趣旨のチュートリアル。React は Hooks 前提で、さらに Jest を使ったテスト、Redux、GraphQL などのコンテンツもあり、何より説明が半端なく分かりやすい。今だったら React 公式のチュートリアルすっ飛ばしてこっちやった方がいいかも、と思うほど素晴らしい。今回はフロントエンド技術の習得に注力したかったので、バックエンド開発部分（Part3, 4 辺り）は飛ばしました。fullstackopen.comCSSCSS in Depth を読む & 手を動かす所要期間: 約5日CSS がずっと苦手だった私を助け出してくれた一冊。Flexbox と Grid Layout の章を読んだだけで元を取れた気がします。タイトルや出版社からしてめちゃくちゃアカデミックな内容を想像していたんですが、全然違いました。一つ一つのプロパティ指定について丁寧に解説してくれるようなとても優しい本です。出版日が2018年とけっこう新しいのも嬉しい。CSS in Depth作者: Keith J. Grant出版社/メーカー: Manning Publications発売日: 2018/04/07メディア: ペーパーバックこの商品を含むブログを見るその他Atomic Design を理解する所要期間: 約10分弊チームでは Atomic Design に則ったディレクトリ構成をしているので、以下の記事を読んで概要を掴みました。design.dena.com最後に以上の教材で学んだことで、Ruby と Rails を使ってのバックエンド開発しかほぼやったことなかった私でもある程度自走してフロントエンドのコードを読んだり書いたりできるようになりました。個人的には CSS in Depth とヘルシンキ大学の Full Stack Open 2019 が教材として最高でした。他の会社さまでも Rails 経験のみのエンジニアを採用して React も書けるように教育していく、みたいなケースがあると思います。そんな時にこの記事が参考になれば幸いです。それではまたお会いしましょう ＾＾例えば React の Hooks が正式に導入されたのは2019年2月 なので、2018年代のコンテンツでは Hooks について言及されていないものがほとんどです。↩]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[社内の情報共有ツールを Qiita:Team から esa に乗り換えました]]></title>
        <id>https://developer.feedforce.jp/entry/2019/08/22/141055</id>
        <link href="https://developer.feedforce.jp/entry/2019/08/22/141055"/>
        <updated>2019-08-22T05:10:55.000Z</updated>
        <summary type="html"><![CDATA[猛烈に暑かったり暑くなかったりするなか皆様いかがお過ごしでしょうか。自宅のエアコンが故障して修理待ち半月の id:tmd45 です。先月、5 年間使い続けてきた Qiita:Team から esa へ、情報共有ツールの乗り換えを行いました。80 名ほどの全社移行となかなか大きなプロジェクトだったので、ここに記録を残したいと思います。なぜ乗り換えに至ったかフィードフォースの情報共有文化抱えていた課題動き出した移行プロジェクト検討した情報共有ツールesa に決めるまで社内への働きかけデータ移行のノウハウEmoji の登録記事データの移行プロジェクトページ記事の扱いプライベートグループの記事の扱いメンバーの移行とデータの移行のタイミングesa と一緒に使っている便利ツール記事の自動作成「自動 esa やり機」Slack に記事 URL から概要を展開してくれる「Kujaku」Emacs x esa 利用者におすすめ「emacs-helm-esa」Qiita:Team から移行して esa に思ったこといろいろサポートの対応がフレンドリーかつ早い！導入時に助かったところ導入時に困ったところQiita:Team と esa の情報整理思想の違いQiita:Team のトップページのように、すべての記事が流れていくようなものが欲しい。情報のザッピングがしたいRecently Update が WIP も含むすべての記事の更新を age てしまう内部 URL でタイトルが展開されるのは便利だった…他人の記事をかんたんにいじれてしまうさいごになぜ乗り換えに至ったかフィードフォースの情報共有文化もともと弊社では Qiita:Team 以前にも、Redmine や社内サーバに構築した Wiki システムを利用し、ナレッジの蓄積と共有を習慣化していました。すばらしいですね。そこにビジネスチャット（当初は HipChat、現在は Slack を利用）を導入、フロー型の情報が増えていきました。Redmine や社内 Wiki のようなストック型のナレッジだけでなくフロー型の日常をまとめられる場所が欲しくなり、当時台頭してきた "今風" の情報共有ツールとして 2014年4月から Qiita:Team を使いはじめました。この頃の記事を見てみると、日報をはじめ、これまで Redmine や Wiki には書きづらかった（書かれたとしても個人メモとして閉じていた）「〇〇やってみた」「〇〇読んでみた」などの情報や、ちょっとした振り返りをまとめた記事が並んでいます。完全なストック型（Wiki 系）と完全なフロー型（チャット系）の中間をツールとして、Qiita:Team はとてもよく機能しました。社内の情報共有文化がこれだけ育っているのも、そういう場を用意できていたことが大きいと思っています。うれしいことに日報や手順書、ポエム、ノウハウなどさまざまな記事が気軽に投稿される弊社でしたが、社員が 80 名近くになるとその情報の整理に課題を感じるようになりました。フロー型の情報とストック型の情報について、参考記事フロー情報とストック情報を分ける～情報過多の時代を生き抜くために：ナレッジ！？情報共有・・・永遠の課題への挑戦：オルタナティブ・ブログ抱えていた課題このときの課題とは、大量に増えた記事のなかから必要な記事が見つけられない ことでした。情報に埋もれる我々記事が埋もれてしまっても検索などでかんたんに見つけられればいいのですが、残念なことに Qiita:Team の機能では 検索ノウハウのある人間であれば頭を使ってなんとか意図した記事を見つけられるものの、そうでなければほとんど目的の記事を見つけられないような状況になりました。 記事を探すというだけで "頭を使う" 必要があるというのはなかなかヘビーです。必要な記事が見つからず、似たような内容の記事が量産されてしまいました。埋もれてしまった古い記事はアーカイブをされることもなくときどき情報検索の邪魔をするようになりました。似たようでちょっと違うタグが乱立しました。流れてしまう情報をなんとかしようとグループ機能を試してみたこともありましたが、グループに閉じてしまうと今度はグループを跨いだ情報の流し読みが困難になりました。ちなみに一時期（2015 年頃）の Qiita:Team は機能として "タグの階層化" が可能でしたが、これに頼ったタグ整理を行ってしまったことも、後々のタグ運用が難しくなる要因であったと思います。いつのまにか（2019 年確認）タグにスラッシュを入れると入力チェックに引っかかって記事が保存できなくなったことにはモヤっとしました…タイトルに絞った部分一致やタグによる検索は可能でしたが、これが上手く機能するように、あとから膨大な記事やタグを整理するのは、なかなか骨の折れる作業です。ナレッジマネジメントに特別な興味のないひとも徐々に「なんとなく使いづらいな…」と感じるようになっていました。動き出した移行プロジェクト社内 Slack に "情報共有ツールをなんとかしたい" チャンネルが生まれたのが 2018/10/18、私がそこに Join したのが 2018/11/13 でした。他のツールに乗り換える前に、現状のナレッジマネジメントのルールやポリシーを整備したほうがいいのではという議論は何度も発生しました。Qiita:Team のプロジェクトページなどを使って INDEX 記事を作成したり、タグについて議論したり、Slackbot に特定の発言をしたときに記事 URL を返すようにさせたり…プロジェクトページで INDEX を作ると、今度は "プロダクト用のプロジェクトページ"（紛らわしいですね 😅）が埋もれてしまうということもありました。これは一社内に多くのプロダクトを持っている弊社ならではかもしれません。このまま「工夫」でなんとかするには、情報整理が得意な人間が「がんばり」続ける必要がありそうでした。情報整理のポリシーなどを定めたとしても、定期的にそれをリマインドし、自警する誰かが必要になるのではないかと思いました。そのため他の情報共有ツールについて少しずつ調査やトライアルをしていきました。検討した情報共有ツールたくさんあるので詳細は割愛しますが、わりと最後まで有力候補だったのが Kibela でした。開発の動きが活発に見えましたし、信頼感のある著名なエンジニアの方が関わっていることも知っていたので強い興味を持っていました。その他、挙がったツールをご紹介しておくと DocBase, marchily, Scrapbox など、そして esa です。いろんなツールを検討するなかで、解決したいことと求める条件をざっくりと以下のようにまとめました。記事検索のしやすさ「プロダクト毎のグループ」と「社内活動毎のグループ」の区別（できれば）「日報」との区別チームをまたいで誰でも全体を眺められる感じナレッジをマネジメントしたいエンジニアも総合職も使いやすい何かを区別して分類したいというのは、つまりカテゴリなりディレクトリなりが欲しいということで、DocBase を試したときに「いいな」と思ったポイントでした。またプロダクトでチームが分かれていても、情報を完全に分断してしまいたくないという思いが以前からあり、他のツールを触るなかでもよく気にしていたポイントでした。この部分は Qiita:Team のトップページのように、すべての情報が流れてザッピングできるものがよかったです。そんなこんなで仕事の合間にいろいろ試していたら 2019 年も春になっていました。どのツールも一長一短なうえに、80 名近くのメンバーを一度に動かす POWER が必要です。当時の自分の Slack 発言曰く「移行したい、と、移行めんどくさいのはざま」でした。esa に決めるまではざまに居る間もどんどん記事は増えていくので、その都度どうしたらその記事が埋もれずに済むか悩んでいました。ここでもまだ「Qiita:Team の記事を一斉に整理する時間をとれば…」みたいな話は出ていました。諦めが悪い（笑）このへんの時期に他社さんで esa を導入したとか、（Qiita:Team から）esa に乗り換えたという記事を目にすることが増えました。Qiita:Teamからesa.ioに乗り換えました - LCL Engineers' Blogカルチャー崩壊と再構築。 Goodpatchが取り組んだ組織デザインの2年間 - 前編｜naofumit｜note例のチャンネルメンバーのなかで「esa を使うとしたらどうやって使ったらカオスにならずに済むだろう」みたいな話をしていました。個人的にも（元から好きなサービスだったこともあり）なんとなく esa にしたいなぁと思い始めて、他社の事例を積極的に探しに行きました。esa の中の人 (\( ⁰⊖⁰)/) がまとめている「esa トーク」の記事も穴が開くほど読みました。とくに大人数で利用されているピクシブさんと、カテゴリルールなどかなり具体的なお話をされていた Misoca さんの記事は、その後の移行のときにもとても参考になりました。esaトーク/サービスへの思いをesaに乗せて、ポエム駆動開発が捗る ─ Vol4.ピクシブ株式会社 様 - docs.esa.ioesaトーク/日常もesaもハックするリモートワーカー集団 ─ Vol.5 株式会社Misoca様 - docs.esa.io結果的に移行先の決め手となったのは、利用事例のわかりやすさと、すでに利用しているひとたちからの評判の良さかもしれません。運命の 2019/04/24、まだ「移行めんどくさい」のはざまでウゴウゴしていた我々を動かしたのは、これまでナレッジマネジメントに悩んでいたチャンネルメンバー…ではない役員メンバーからのこんなお言葉でした。"いまの情報共有ツールの使いづらさって、だれも問題に感じてないの？"「ですよね！！？」という衝撃とともに個人的に火もついて、ここでやっと「情報共有ツールの移行、ワイが esa で進めたるでー！（POWER）」となったのでした。※画像はイメージです社内への働きかけesa 移行を決心してから、社内への働きかけも行っていきました。一番大切なのは 情報管理ツールを移行したい思いと目的 で、これは前述のようにいろいろ考えてきたので１つの記事を作って社内に共有しました。また、移行プロジェクトに関わってくれたデザイナー氏が、移行の意欲を高めるために弊社のキャラクターと esa の (\( ⁰⊖⁰)/) を（勝手に）コラボしたイメージを作ってくれて、なかなかに社内の評判もよかったです。こういう「たのしい」工夫も大事だと思います。今も esa の HOME ページを飾る非公式コラボ画像移行期間中に社内の士気を上げるため貼っていたポスターそこからトライアルへの協力と、実際のアカウント移行のお願いなども同様に進めました。とくに新卒の若者たちは新しいツールを使いこなそう(｀・ω・´)+という意欲も高く、積極的に使い方を聞きに来てくれたり、同期で共有したりしていて頼もしかったです。社長にも「とりあえず１つ何か記事書いてください！」とお願いしたら、もともと Qiita:Team で書いていた週報をシュッと esa で書いてくれました。おかげでそれを読むためにアカウント登録するメンバーも増え、スムーズに移行できました。データ移行のノウハウここからはデータの移行で得た気づきなどです。ご紹介しているスクリプトの利用につきましては保証しかねますのでご了承ください！Emoji の登録記事の移行より先にやったのが Emoji の登録でした。Qiita:Team は Slack と連携して Emoji を同期させる機能がありますが、esa にはその機能はありません。 API で登録することができるので、ひとまず更新のことは考えず、その時点の Slack Emoji を吸い出して、esa に流しこみました。Slack Emoji を取ってくるスクリプトは拾いものなので割愛します、すみません。流し込んだスクリプトはこちら。qiita2esa/slack_emoji_import.rb at master · tmd45/qiita2esaすでに登録済みの Emoji に重複したキーワードだと 400 Bad Request になります。また Slack は日本語キーワードでの Emoji 登録が可能ですが、esa のほうでは日本語キーワードには対応していないので、予め除外するか当該エラーでスキップさせましょう。余談ですが、一度、Emoji の画像ファイルとキーワードをあべこべに登録してしまってカオスを生んでしまいました。いまはリカバリ済みです:(；ﾞﾟ'ωﾟ'):ありがたさ → からい。からいもの好きなひとか！いいとおもう〜 → 却下。真逆！ムズい → 輪廻転生。ｿｳﾃﾞｽﾈ記事データの移行Qiita:Team の記事エクスポート機能（Owner のみ可能）で得られるデータは１記事＝１JSON ファイルになっていて、メタ情報やコメントも含まれています。esa に移行するために以下のようにしました。▼記事作成・更新 API esa パラメータ  内容  注意点  name  記事タイトル  / や # は数値参照に置き換える（カテゴリやタグになってしまうのを避ける）  category  カテゴリ  (unsorted)/all 移行用に定義  body_md  本文 Markdown  1. パラメータとして指定できないメタ情報（記事作成日時など）を本文に追加する形で残す2. 本文中の他記事 URL や画像 URL を置換3. 本文中の ユーザID を小文字化（ ScreenName に合わせるため）  user  記事作成者  記事作成者の ScreenName を指定  created_by  記事作成者（上書き）  同上。更新時に Owner 権限をもつ token でのみ上書き可能 ▼コメント作成 API esa パラメータ  内容  注意点  body_md  コメント本文 Markdown  記事本文と同様にメタ情報の追加と、他記事 URL、画像 URL、ユーザID の置き換えを行う  user  コメント作成者  コメント作成者の ScreenName を指定 esa API の利用については 公式の開発者向けドキュメント を参照ください。記事本文に追加したメタ情報の例今回は「タグ」の移行をしませんでした。一度リセットして、混沌としたタグの整理ができればと思ったんですが、タグがなくなったことで意味がわからなくなってしまった記事が意外とあって、メタ情報と一緒に移行しておけばよかったな、と思いました。インポートの流れとして 先に記事 URL や画像 URL だけ esa 上で作っておいて、記事本文を流し込む段階で記事中の URL の置換 を行いました。先に作った URL たちは中間データとしてテキスト（.tsv）に吐き出しました。口頭だとややこしいので詳しくは拙作のコードで。qiita2esa/article_import.rb at import_articles · tmd45/qiita2esa使い捨て用に書きなぐったものなのでお恥ずかしいですが、なんか頑張ってるのが伝わればいいかなと思います 🙈esa の API 利用はもちろんですが、画像取得のために Qiita:Team の API も利用します。プロジェクトページ記事の扱いQiita:Team のプロジェクトページとなっている記事も１記事＝１ファイルです。ただし通常の記事と、含まれている属性が若干異なるため注意は必要です。基本的には通常の記事と同じく、渡せるメタ情報は渡して、Markdown の本文を投入するという感じで進めました。もちろん esa には「プロジェクトページ」という概念はないので、適当に (unsorted)/projects のようなカテゴリを定義してそこにまとめました。qiita2esa/project_import.rb at import_articles · tmd45/qiita2esa実はメインの記事移行より先に、件数の少ないこちらを試していたため、結果的に URL の置き換え処理などすっかり忘れていました。INDEX の用途で利用していたページではこの置き換えを行わないとツラいと思いますので、お気をつけください。プライベートグループの記事の扱いプライベートグループでは、人事や役員会議などで扱う社内にも非公開の情報が含まれていました。これらは esa の決済連結機能を使って、別の esa チームを作成しデータ移行しました。エクスポートした JSON データは、記事の所属するグループもパラメータとして持っているので、そこを見てファイルごと分別しておきます。スクリプトが見当たらなくて自分でどうやったんだっけ…と思いましたが、雑に以下のコマンドで分類してました。ご参考まで 😇$ cd path/to/export_articles$ ag -l '"url_name": "group_name"' ./*.json | xargs -I% mv % ./group_nameメンバーの移行とデータの移行のタイミング今回は先にメンバー全員に、あるタイミングから新規記事は esa で作ってもらうよう案内しました（それゆえに Emoji の登録を先にやった）。Qiita:Team 側に新しい記事が作成されなくなったのを見計らって、告知しつつ、記事データのエクスポートと、esa への投入を開始しました。記事の参照は一時的に Qiita:Team と esa の双方を見ることになりましたが、"記事を書けない期間" を無くして移行できたのはよかったです。esa と一緒に使っている便利ツール事例といえば先駆者たちの esa 公式の紹介記事まとめ も大変助かりました。弊社でもいくつかの便利なツールを併用しています。記事の自動作成「自動 esa やり機」用意したテンプレートを元に、指定した曜日に自動で記事を生成してくれる Heroku アプリです。standfirm/esa_feeder - GitHub自動esaやり機とクリーンアーキテクチャ - Misoca開発者ブログ弊社でも定期的なミーティングの議事録は、まず WIP の空記事を用意して、事前に議題を追記したり、話しながら記録したりということをやっています。Qiita:Team 時代からテンプレートを元に人が WIP 記事を作って共有していましたが、"誰か" がリマインドに気づいて記事を作成するといったことが不要になりました。記事の作成と同時に指定した Slack チャンネルへの通知も行われることで、議題の追記のリマインドも兼ねていて非常に便利に利用しております！Slack に記事 URL から概要を展開してくれる「Kujaku」最近ツールの更新があってコメントURL からコメント内容も展開されるようになった、Slack x esa の情報共有を促進してくれる便利ツール。こちらもバックエンド部分は Heroku アプリです。FromAtom/Kujaku - GitHubClosedなesaの記事URLをSlackに貼ったら展開されるようにした - pixiv inside#esa のURLを展開してくれるKujakuで、コメントも展開できるようになりました。 - 文字っぽいの。ログインしないと見られないようなクローズドなページの URL は、Slack に流してもその内容が展開されません。それを解決してくれるのが Slack の "Unfurling links in messages" という機能なのですが、その機能をかんたんに使えるようにしてくれたのがこのツールです。これまでは URL と一緒にタイトルなども投稿してもらわないと「何？この URL…」となっていたのが、URL を貼るだけで手軽に共有できるようになったのはハイパー捗りました！Emacs x esa 利用者におすすめ「emacs-helm-esa」手前味噌ですが、esa 移行をきっかけに弊社の Emacs 使い id:masutaka26 が作った Emacs-Helm Interface もご紹介。masutaka/emacs-helm-esa - GitHubesa の記事を Emacs から素早く開ける helm-esa.el を作った / マスタカの ChangeLog メモ普段利用している環境から手軽に記事が探せるのは強いですね！よろしければお試しください。Qiita:Team から移行して esa に思ったこといろいろなによりも課題であった「記事検索が上手くいかない」件はすっきり解決しました！以前は言いづらかった「○○で検索してみてください！」というのが気軽に言えるようになってよかったです。それ以外の部分を挙げてみたいと思います。サポートの対応がフレンドリーかつ早い！esa のフィードバックフォームから投げたものに対する反応が早い相談に対する回答も丁寧でとてもありがたいバグ報告したらすぐ直って 社内で２，３人鼻血出した 素敵情報セキュリティポリシー、運用ポリシーが公開されているので社内のPマークチェックもスムーズでした（確認にもご協力いただけて助かりました）移行するデータ量が多いことは分かっていたので、先に esa の人に「たくさんデータ移行する予定なんですが大丈夫ですか…？」と連絡してみました。移行当時の Qiita:Team メンバー数が 79 人、記事数が 5 万記事以上ありました（あとで気づきましたが、画像も相当な数とサイズがありましたね…）。esa の深谷さんや越川さん、赤塚さんからすぐにお返事をいただいて、とても助かりました。導入時に助かったところドメイン制限が可能な Google ログインG Suite 利用のため、ソーシャルログインでアカウント管理が一元化できてハッピーです「添付ファイルに関するセキュアオプション」→ esaにログイン中のみ添付ファイルへのアクセスを可能にするesa 記事の外部公開ができなくなってしまうのは残念ですが、これがあると安心して業務情報をまとめられるので非常に助かります「複数チームの連結決済」Qiita:Team 利用時から、人事や経営に関わるプライベートグループの運用がありました。これを引き継ぐため、メンバーを限定した esa チームを作り、連結決済の設定をしています「Webhook・アプリ連携」での Slack 連携Qiita:Team のころから Slack にフィードを流すという運用をしていたので、同様のものがあり助かりました導入時に困ったところアカウント登録のとき Name と ScreenName の違いが分かりづらかったようです。Name のほうに氏名、ScreenName にユーザID を入れてねー、という説明が必要でした。また移行ならではですが、esa の ScreenName では、Qiita:Team の ユーザ名 で使えた「英大文字」が使えません。そのため Qiita:Team で英大文字を含む ユーザ名 を利用していた場合には、すべて小文字に置き換えたものにしてもらいました。アカウント登録の案内にこのような注意を書きました。どう判定させるかは移行スクリプトの作りに依ります。大文字を小文字に置き換えた場合も「同一のIDである」と判断します :pray:Qiita で ichiro3 → esa で ichiro3 ⇒ OK ✅ （完全に一致）Qiita で Taro-san → esa で taro-san ⇒ OK ✅（小文字になっただけ）Qiita で Hanako99 → esa で 99hanako ⇒ NG 💥（同一と判断できません）大文字・小文字とは別の話ですが、ハイフンやアンダーバーを使われている方は間違えないようにお気をつけください。Qiita で Taro-san → esa で taro_san ⇒ NG 💥（ハイフンがアンダーバーに。同一と判断できません）Qiita:Team と esa の情報整理思想の違い見出しのとおり、２つのツールは情報整理に対する姿勢が異なります。それゆえに移行して戸惑いの声が挙がったのは事実です。これらの意見は「esa も Qiita:Team と同じようにすべき」という話ではありません。UI や使い方が変われば戸惑うひとがいるのは当たり前で、いずれ良い方向に慣れるか、悪い方向に慣れるか分かれます。使い続けて感じる不便な点は、きちんとフィードバックすれば答えてくれるサービスだと思うので、そこは真摯にフィードバックしていきましょう 💪Qiita:Team のトップページのように、すべての記事が流れていくようなものが欲しい。情報のザッピングがしたいesa では "POSTS" のページがそれに当たりますが、Qiita:Team に慣れていたメンバーからすると「トップページ」から１ステップ踏まないとそれが見えないという戸惑いがありました。"POSTS" ページに移動しても、ソート順が意図した順でない（ソート順を変更するというリテラシーも必要になる）ことでも迷いが発生したようでした。これについては、"HOME" の README 記事に便利リンク集を追加して、周知することでひとまず解決できました。README に作った「オススメ検索リンク」集Recently Update が WIP も含むすべての記事の更新を age てしまうこれについては esa のコンセプトにどうしても馴染めないメンバーがいくらかいるという話でもあります。esa.io より、esa のコンセプトまず 「下書き」機能が無いことで、以前より記事が書きづらい、公開もしづらくなった という声がありました。不完全な状態では公開したくなく「下書き」のような非公開の状態で推敲を重ねて、キチンとした記事にしてから公開したい、という意見です。これを他人に強制するような人は見かけませんが「自分が記事を書くならそのようにしたい」と思うメンバーは年齢関係なくいる状況です。単純に下書き機能を「個人メモを置く場所」として使っていたという人もいました。また、WIP で書いた記事を更新したときに、Recently Update で更新順として上がってきてしまうことに不便を感じる 声もあります。これは記事を書く方も、見る方もそうで「未完成の間は sage 進行したい」「更新された記事は知りたいけど ShipIt されてから読みたい」という状況です。 [skip notice] しても Recently Update には流れてくるというのも戸惑われているようです。読む側は、先程のザッピングの検索クエリと同じもので解決できるので、そちらを利用してもらっています。書く側に対しては、気にせずガンガン更新しちゃいなよ！と意識改革していくしかないかなと思っています（自分は未完成でも気にしない星人）。内部 URL でタイトルが展開されるのは便利だった…Qiita:Team では、Qiita:Team 内の URL を貼るだけで、記事の表示時にリンク先記事のタイトル（と作成者のアイコン）が自動で展開されていました。"リンクされた記事" のタイトルを修正したときに、"リンクしている記事" 側を修正しなくてよいのは大変便利でした。Qiita:Teamで、チーム内記事へのリンクのタイトルが自動で表示されるようになりました - Qiita Blog情報整理をしていると、よりわかりやすいタイトルに修正したくなることも多いですし、そういう記事に限っていろんなところから参照されていたりして…いまでもこの機能は欲しいなーと思います（たしか esa にもフィードバックを送らせていただいたと思う）。他人の記事をかんたんにいじれてしまうこれは最近不安を訴える人が減ってきたので、みんな慣れてきたのかもしれません。Qiita:Team のときは「共同編集の記事」という区別があったり、「編集リクエスト」の機能があったため、うっかり他人の記事を触ってしまうということがありませんでした。これに慣れていると、逆にどの記事でも誰でもいじれてしまうのは、不安があったようです。編集履歴から内容をロールバックすることもできるし、（自分のようなおじさんから言わせれば）昔からある Wiki だってそういうものですよ、という感じで、こちらも意識改革をしていくのがよいと思いました。誰でも触れるおかげで、記事を書いた人に依存せずに情報の整理ができるというのはとても良い点だと感じます。記事の Delete が出来てしまうことに不安を持つ人は多いかもしれませんが、まず記事の削除自体が２ステップ踏む UI になっていて「うっかり」することは少ないのではないかと思います。さいごに情報共有ツールの移行で得られた結果は以下のようなものです。検索で記事が見つけやすくなったナレッジの階層化ができるようになったナレッジの構造化を意識するようになったフロー情報とストック情報を区別できるようになった移行をきっかけに記事の整理が行われた問題を感じることに鈍感にならず、今後も「日々混沌、日々進化1」のバリューを実践していきたいと思います。弊社バリューである FF memes の１つです。詳しくはこちら: https://recruit.feedforce.jp/ ↩]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ Dynamoid のスレッドセーフではない実装を直しました]]></title>
        <id>https://developer.feedforce.jp/entry/2019/08/13/183130</id>
        <link href="https://developer.feedforce.jp/entry/2019/08/13/183130"/>
        <updated>2019-08-13T09:31:30.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。インフラエンジニアの id:tsub511 です。私は Dynamoid のメンテナではないのですが、弊社内で今回それなりに大きい問題が起きて、得た知見も大きかったため記事にしました。TL;DRDynamoid にスレッドセーフではない実装があったが PR をマージしてもらって修正済み。2019/08/13 時点ではまだリリースされていないようなので、エラーで困っている方は master ブランチをお使いください。今回起きた問題弊サービスでは Sidekiq 上で Dynamoid を使っています。github.com基本的に問題なく稼働していたのですが、デプロイ時に Sidekiq を再起動した後、Bugsnag に以下のような二種類のエラーが継続的に飛んできました。undefined method `[]' for nil:NilClassundefined method `query' for #<Dynamoid::AdapterPlugin::AwsSdkV3:0x00000000078dc1b8> Did you mean? to_queryこのエラーが起きると、自然には回復しないため、Sidekiq のワーカーを再起動する必要があります。また、NoMethodError という一般的な例外クラスのため ActiveJob の retry_on によるリトライ処理の考慮はしておらず、大事なジョブが実行されないままになってしまうのも問題です。エラーをパッと見ただけでは、キャッシュの実装に考慮漏れがあるのかな？とか、インスタンスが生成されていてコード上ではメソッドが実装されているのになんで undefined method エラーが起きるんだ？などと、不思議なエラーが出ていて調査が難航しそうな印象でした。何が原因だったのかSidekiq + Dynamoid でピンと来る方もいると思いますが、エラーの原因は Dynamoid にスレッドセーフではない実装があったことでした。スレッドセーフではない実装がどこにあったのか探すために、ここで発生していたエラーをもう一度見てみます。' for nil:NilClass">undefined method `' for nil:NilClassまず 1 つ目のエラーは nil に対して #[] を呼び出そうとしてエラーになっていますが、nil になっている変数は table_cache です。table_cache は以下で初期化されています。# https://github.com/Dynamoid/dynamoid/blob/v3.2.0/lib/dynamoid/adapter_plugin/aws_sdk_v3.rb#L66-L69def connect!  @client = Aws::DynamoDB::Client.new(connection_config)  @table_cache = {}endそして、この初期化のための Dynamoid::AdapterPlugin::AwsSdkV3#connect! を呼び出しているのは Dynamoid::Adapter#adapter です。adapter.connect! の部分に if adapter.respond_to?(:connect!) という条件がありますが、ここが false になっていて adapter.connect! が実行されていないため、table_cache の初期化処理が動いていないようです。# https://github.com/Dynamoid/dynamoid/blob/v3.2.0/lib/dynamoid/adapter.rb#L29-L37def adapter  unless @adapter_.value    adapter = self.class.adapter_plugin_class.new    adapter.connect! if adapter.respond_to?(:connect!)    @adapter_.compare_and_set(nil, adapter)    clear_cache!  end  @adapter_.valueend2 つ目のエラーも見てみます。undefined method `query' for #<Dynamoid::AdapterPlugin::AwsSdkV3:0x00000000078dc1b8> Did you mean? to_queryこちらは Dynamoid::AdapterPlugin::AwsSdkV3 のインスタンス adapter に対して query を実行しようとしてメソッドが定義されていないというエラーです。しかし、実際のコードにはメソッドが定義されています。https://github.com/Dynamoid/dynamoid/blob/v3.2.0/lib/dynamoid/adapter_plugin/aws_sdk_v3.rb#L489-L500 def query(table_name, options = {})  Enumerator.new do |yielder|    table = describe_table(table_name)    Query.new(client, table, options).call.each do |page|      yielder.yield(        page.items.map { |row| result_item_to_hash(row) },        last_evaluated_key: page.last_evaluated_key      )    end  end end視点を変えて、adapter インスタンスを定義しているコードを見てみると、やはり Dynamoid::Adapter#adapter に行き着きます。# https://github.com/Dynamoid/dynamoid/blob/v3.2.0/lib/dynamoid/adapter.rb#L29-L37def adapter  unless @adapter_.value    adapter = self.class.adapter_plugin_class.new    adapter.connect! if adapter.respond_to?(:connect!)    @adapter_.compare_and_set(nil, adapter)    clear_cache!  end  @adapter_.valueendself.class.adapter_plugin_class.new で生成したものをメモ化しています。メモ化には concurrent-ruby を使っていて、過去に対策されたようなのでそこは問題なさそうです。self.class.adapter_plugin_class.new の先が怪しそうなのでコードを見てみると、なにやら動的に require しています。# https://github.com/Dynamoid/dynamoid/blob/v3.2.0/lib/dynamoid/adapter.rb#L181-L187def self.adapter_plugin_class  unless Dynamoid.const_defined?(:AdapterPlugin) && Dynamoid::AdapterPlugin.const_defined?(Dynamoid::Config.adapter.camelcase)    require "dynamoid/adapter_plugin/#{Dynamoid::Config.adapter}"  end  Dynamoid::AdapterPlugin.const_get(Dynamoid::Config.adapter.camelcase)end実はスレッドセーフではない実装はこの require する条件の Dynamoid.const_defined?(:AdapterPlugin) && Dynamoid::AdapterPlugin.const_defined?(Dynamoid::Config.adapter.camelcase) の部分です。エラーを再現できるコードを Gist に用意しましたのでそちらを使って確認していきます。Test codes to reproduce not thread-safe errors of Dynamoid · GitHubDynamoid に以下のような変更を加えて、unless の中に入らず require が実行されなかった時の状態を見てみます。diff --git a/lib/dynamoid/adapter.rb b/lib/dynamoid/adapter.rbindex f390ecf..df2a58c 100644--- a/lib/dynamoid/adapter.rb+++ b/lib/dynamoid/adapter.rb@@ -181,6 +181,13 @@ module Dynamoid     def self.adapter_plugin_class       unless Dynamoid.const_defined?(:AdapterPlugin) && Dynamoid::AdapterPlugin.const_defined?(Dynamoid::Config.adapter.camelcase)         require "dynamoid/adapter_plugin/#{Dynamoid::Config.adapter}"+      else+        tmp_adapter = Dynamoid::AdapterPlugin.const_get(Dynamoid::Config.adapter.camelcase).new+        puts <<~EOS+          respond_to?(:connect!): #{tmp_adapter.respond_to?(:connect!)},+          respond_to?(:query): #{tmp_adapter.respond_to?(:query)},+          require: #{require "dynamoid/adapter_plugin/#{Dynamoid::Config.adapter}"}+        EOS       end        Dynamoid::AdapterPlugin.const_get(Dynamoid::Config.adapter.camelcase)すると、スレッドによっては const_defined? の結果が true で、require の結果も false (コードがロード済み) なのに、実際のメソッドが存在しないという現象が起きていることが分かりました。$ bundle exec ruby main.rb...respond_to?(:connect!): false,respond_to?(:query): false,require: false...ここからは Ruby の require の実装を知らないため推測になります。おそらくマルチスレッド環境下で require を実行すると、require を実行したスレッド内では全てのコードがロードされた状態になりますが、別のスレッドではクラス定義などの「ガワ」だけがロードされた状態になっているのではないかと思いました。そのため、ロードが不十分なスレッドでインスタンスを生成できるが、メソッドが定義されていない、という状態になっているのかと思われます。解決方法よって、全てのスレッドで確実に require を実行することで今回のエラーが解決するという結論に至りました。diff --git a/lib/dynamoid/adapter.rb b/lib/dynamoid/adapter.rbindex f390ecf..8449e34 100644--- a/lib/dynamoid/adapter.rb+++ b/lib/dynamoid/adapter.rb@@ -179,9 +179,7 @@ module Dynamoid     end      def self.adapter_plugin_class-      unless Dynamoid.const_defined?(:AdapterPlugin) && Dynamoid::AdapterPlugin.const_defined?(Dynamoid::Config.adapter.camelcase)-        require "dynamoid/adapter_plugin/#{Dynamoid::Config.adapter}"-      end+      require "dynamoid/adapter_plugin/#{Dynamoid::Config.adapter}"        Dynamoid::AdapterPlugin.const_get(Dynamoid::Config.adapter.camelcase)     endエラーの原因と解決方法が判明したため、既に Dynamoid に PR を作りマージまでしてもらいました。github.com2019/08/13 時点ではまだリリースされていないようなので、エラーで困っている方は master ブランチをお使いください。調査に苦労した点マルチスレッドプログラミングの経験が浅いため、まずスレッドセーフではない実装があるということに気づくまでに時間がかかりました。そして、エラーを再現しようとした時になかなか再現出来なかったのもハマりポイントでした。エラーの再現コードを読むと分かりますが、Dynamoid のメソッドを呼ぶ直前に puts を実行しています。# https://gist.github.com/tsub/72e60233ed82a8a453428ea7441e6017#file-main-rb100.times do |i|  safe_thread(i.to_s) do    puts 'debug' # To unlock Ruby's GVL    Document.where(identifier: 'hoge').first  endendこの puts が重要で、Ruby は GVL (Giant VM Lock) という仕組みを使って、実行されるネイティブスレッドが 1 つになるように排他制御をしています。ただし、IO 関連のメソッドを実行する際は GVL が一時的に解放されてスレッドが同時に実行されます。ネイティブスレッドを用いて実装されていますが、 現在の実装では Ruby VM は Giant VM lock (GVL) を有しており、同時に実行される ネイティブスレッドは常にひとつです。 ただし、IO 関連のブロックする可能性があるシステムコールを行う場合には GVL を解放します。その場合にはスレッドは同時に実行され得ます。 また拡張ライブラリから GVL を操作できるので、複数のスレッドを 同時に実行するような拡張ライブラリは作成可能です。docs.ruby-lang.orgつまりスレッドセーフではない実装があった場合に、それを再現させるためには単純にスレッドセーフではないコードを書くだけではダメで、IO 関連のメソッドを実行して GVL を解放しないといけません。少し古い記事ではありますが、こちらが参考になりました。moyomot.hatenablog.com実際の本番環境ではログ出力などにより、IO 関連のメソッドは普通に実行されていることが多いかと思いますので、エラーが起きるのも納得です。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heroku Meetup #25 "Heroku Ghost" で 2 回話してきた #herokujp]]></title>
        <id>https://developer.feedforce.jp/entry/2019/07/29/140000</id>
        <link href="https://developer.feedforce.jp/entry/2019/07/29/140000"/>
        <updated>2019-07-29T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:masutaka26 です。先週金曜日（2019/7/26）に開催された Heroku Meetup #25 "Heroku Ghost" で『デプロイ元をCircleCIからHerokuに乗り換えた』というお題で発表してきました。つい先日、現在関わっている EC Booster のデプロイフローを改善し、それなりに知見を得られたためです。Heroku Meetup の参加は前回 #24 に引き続き、セッションでの発表は前々回 #23 以来です。前々回 #23 については自分のブログにも書きました。Heroku Meetup #23 "Heroku Dynamite!!" で話してきた #herokujp / マスタカの ChangeLog メモLT でも話したLT でも飛び込みで話しました。内容は heroku-buildpack-google-chrome と heroku-buildpack-chromedriver の細かすぎる話です。動かない Chrome や chromedriver のバイナリが本番環境にデプロイされることを防ぐ方法や、それに関して PR を出した話など。スライドの 41~43 ページ目が相当します。無駄に元気な 1 日でした。今日は会社で LT 1 本やって、Heroku meetup で通常セッション 1 本、その後に飛び込み（？）で LT 1 本やった。さすがにもうこの数をやることないかな？ #herokujp— Takashi MASUDA (@masutaka) 2019年7月26日※ 44~45 ページ目は、尺の関係で削ったブランチ戦略のメモです。ベストな戦略ではないので改善予定。雑感初心者向けの知見に比べて、一歩進んだ Heroku 知見はあまり Web 上に存在しない印象です。ここ 1 年近く、それなりに Heroku を学習してきて「かなりの規模まで Heroku で十分では？」「デプロイや Dyno の再起動で中断されるため苦手とされがちなバッチ処理も、簡単に克服できるのでは？」などと、考えが変わってきています。これからもブログや登壇などで、コミュニティに還元していきます。本日の頂き物。キャップはスピーカー特典 #herokujp pic.twitter.com/Whc3UVJrbg— Takashi MASUDA (@masutaka) 2019年7月26日]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[esa をネタに Emacs Lisp と Elm を無理やり繋げた発表をした]]></title>
        <id>https://developer.feedforce.jp/entry/2019/07/01/140000</id>
        <link href="https://developer.feedforce.jp/entry/2019/07/01/140000"/>
        <updated>2019-07-01T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。増田（id:masutaka26）です。今回はネタ探しに特に苦労しました。週次の社内勉強会 FFTT で『esaを支えたい技術』という発表をしました。esa をネタに Emacs Lisp と Elm を無理やり繋げた内容です。ちょっと強引だった。発表の内容流れとしては、↓ こんな感じです。先月 emacs-helm-esa というツールを作ったよElm を学んだら、Emacs Lisp と Syntax が似ていたよesa の検索をするだけの簡単なツールを作ったよ発表の感想付箋紙に感想を書いてくれました。 は私からのコメントです。関数型言語について学んでみたいと思っていたので、おすすめの順序を提示してくださり、参考になりました！ 良かったです。Elm はだいぶ学びやすかったですElm、こうして見てみると Haskell にめっちゃ似てますね！ そうなんですね発表の流れで Elm の話になると思わなかった 強引でしたね (^^;Elm ならふだんバックエンドのコード書いている人でもモダンなフロントエンド書けるって聞いたので勉強がんばる（決意） 刺激になってよかったです良かったです！規模が大きくなってきても同じノリで書けるのも Elm は良い そういう感じなんですね。コンパイルが通ればだいたい動くのが楽でしたElm の HTML を作るのがすごいですね！ ですよね。この辺りの div や i は全部関数ですからねネタバレに慎重になりすぎでは！？ 自分でも全く理解できません来週にはこれが Chrome 拡張になっているんですかね？ はい、次回以降の当番の方が必ず・・・！Elm おもしろそう 🏃‍♀️🚶‍♂️ emacs ってすごいんですね とは言え、今の時代は emacs の他に優先順位の高いものがたくさんありますね 💦まさかの Elm とは！？ 考えに考えた末の Elm です...これで EC Booster Front 開発できますね 見ててくださいよ！これからのマスタカを！！１所感正直、外したかな？と思いながら発表してましたが、感想を読むと良い刺激を与えられたようで良かったです。一方で新しい言語の説明は難しいうえ、皆さんに馴染みがない Emacs Lisp との比較をしたので、余計に分かりづらかったという...。こうするとあまり盛り上がらないという学びを得たことと、個人的には大変満足のいく内容だったので、結果的には良かったです。さて、次回の勉強会のネタはどうするかな...。基礎からわかる Elm作者:鳥居 陽介発売日: 2019/02/27メディア: 単行本（ソフトカバー）]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heroku で Chrome を使ったクローラの IP アドレスを固定する]]></title>
        <id>https://developer.feedforce.jp/entry/2019/04/26/154540</id>
        <link href="https://developer.feedforce.jp/entry/2019/04/26/154540"/>
        <updated>2019-04-26T06:45:40.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:masutaka26 です。少し前に、Heroku の worker Dyno 上で動く、Chrome のクローラで IP アドレスを固定化出来ないか調査しました。実装は見送られましたが、想定よりも難しく調査に時間がかかったので、この記事に残しておきます。方法だけ知りたい方は、下の方にある「手順」をご覧ください。Heroku Add-on を比較する開発組織実装プランCLI のインストールHeroku Buildpack[コラム] 固定 IP アドレスへの私見Chrome のクローラで IP アドレスを固定化する試行錯誤の日々ついに成功手順QuotaGuard Static の注意事項まとめHeroku Add-on を比較する今回は Proximo と QuotaGuard Static を検討し、最終的に QuotaGuard Static を採用しました。💡 他に Fixie, Fixie Socks, Guru301 などがあるようです。以下、その理由です。開発組織Proximo は https://github.com/pirateradio を見た感じ @ddollar の個人開発のようです。この方は foreman などを作っており、多分すごい人です。QuotaGuard Static は https://www.quotaguard.com/ によると Alpine Shark, LLC のようです。会社のほうがちょっとだけ安心できます。実装Proximo は 1 つのようですが、QuotaGuard Static は 2 つの Proxy サーバで冗長化されているようです。Proxy サーバが落ちる確率は低いかもしれませんが、冗長化されているに越したことはありません。🔗 https://devcenter.heroku.com/articles/proximo#provisioning-the-proximo-add-on$ heroku addons:create proximo:developmentAdding proximo to sharp-mountain-4005... done, v18 ($5/mo)Your static IP address is 10.9.8.7🔗 https://devcenter.heroku.com/articles/quotaguardstatic#provisioning-the-add-on$ heroku addons:create quotaguardstatic:starter-----> Adding quotaguardstatic:starter to sharp-mountain-4005... done, v18 (free)-----> Your static IPs are [10.11.12.13, 14.15.16.17]プランQuotaGuard Static は無料プランがあるので始めやすいです。Proximo のプランは $5/mo からです。ちなみにどちらも PROXIMO_MASK や QUOTAGUARDSTATIC_MASK という環境変数を設定すると、一部のレンジの送信だけこれらのサービス経由にすることができ、料金を節約できます。💡 正確に書くと QUOTAGUARDSTATIC_MASK は後述する qgsocksify 用の環境変数です。CLI のインストールどちらも CLI が提供されており、必要に応じてインストールします。Proximo はインストール元が http なのがイマイチでした。https に変えてもアクセス不可です。$ curl http://downloads.proximo.io/proximo-stacklet.tgz | tar xzQuotaGuard Static は普通に https でインストール出来ます。$ curl https://s3.amazonaws.com/quotaguard/qgtunnel-latest.tar.gz | tar xz$ curl https://s3.amazonaws.com/quotaguard/quotaguard-socksify-latest.tar.gz | tar xzHeroku Buildpack今回の調査の過程で Heroku Buildpack を作りました。バイナリファイルをリポジトリに commit せずに済みます。よかったらどうぞ。https://github.com/masutaka/heroku-buildpack-proximohttps://github.com/masutaka/heroku-buildpack-qgtunnelhttps://github.com/masutaka/heroku-buildpack-qgsocksify[コラム] 固定 IP アドレスへの私見IP アドレスを固定化したいのは、どんなケースでしょうか。多くはセキュリティ要件だと思いますが、私はあまり賛同できないです。そもそも当該 IP アドレスは認証されておらず、所有者が変わることがあります。仕組み上それに気づくことは難しいため、逆にセキュリティリスクを増やすことになります。そういう意味では電話番号とよく似ています。サービスのスケールの観点からもデメリットがあります。例えば Heroku から AWS に引っ越す時、固定 IP アドレスは変わってしまうでしょう。顧客との調整が必要ですし、間に開発会社が入っていると、より時間がかかります。Chrome のクローラで IP アドレスを固定化する今回のケースでは Heroku の worker Dyno 上に sidekiq が起動しており、selenium-webdriver 経由で Chrome が子プロセスとして起動し、クロールします。sidekiq└ Chrome   ↓   クロール先試行錯誤の日々当初は sidekiq 自体に qgtunnel や qgsocksify をラップしましたが、うまくいきませんでした。よく考えたら納得です。Chrome に --proxy-server=http://<USERNAME>:<PASSWORD>@<HOSTNAME>:<PORT> や --proxy-auth=<USERNAME>:<PASSWORD> を指定してもダメ。どうやら少なくとも Chrome 73 ではセキュリティ上の理由から、コマンドラインオプションから認証情報を設定できないようです。認証なしプロクシなら大丈夫ですが、QuotaGuard Static は商用サービスなので認証は必須です。ついに成功ダメ元で QuotaGuard Static のサポートに聞いたら、方法を知っており、無事クロール先にアクセスすることが出来ました。👇 Dyno とクロール先を SOCKS5 トンネルで繋ぎ、それ経由でアクセスします。sidekiq└ Chrome (4443 port)   ↓ (SOCKS5 tunnel)   QuotaGuard Static   ↓ (SOCKS5 tunnel)   クロール先 (443 port)TCP アクセスならなんでも IP アドレスを固定化出来そうです。手順(1) 当該 Heroku App に QuotaGuard Static Add-on をインストールします。(2) qgtunnel CLI もインストールします。前述の heroku-buildpack-qgtunnel を使うとお手軽です。$ curl https://s3.amazonaws.com/quotaguard/qgtunnel-latest.tar.gz | tar xz(3) Heroku のダッシュボードから、QuotaGuard Static のダッシュボードに進み、Settings → Setup とクリックします。(4) さらに Tunnel → Create Tunnel とクリックします。(5) Remote Destination にクロール先を、Local Port は 4443、Transparent は true に設定します。Encrypted は設定しません。(6) トンネルが出来ました。127.0.0.1:4443 へのアクセスは destination.example.com:443 へのトンネルになりますTransparent を有効したことにより、destination.example.com の DNS が 127.0.0.1 に上書きされました。destination.example.com:4443 へのアクセスは destination.example.com:443 と等価になりますすでに HTTPS を使っているため、Encrypted は必要ありません(7) selenium-webdriver に与える Chrome のバイナリを qgtunnel でラッピングします。今回は以下のようなシェルスクリプト bin/google-chrome-qgtunnel を作り、selenium-webdriver に指定しました。#!/bin/sh -euexec bin/qgtunnel "$GOOGLE_CHROME_SHIM" "$@"💡 heroku-buildpack-google-chrome と heroku-buildpack-chromedriver を使っています。環境変数 GOOGLE_CHROME_SHIM は /app/.apt/usr/bin/google-chrome です。(8) 左側の Outbound をクリックし、SOCKS5 url を Heroku の環境変数 QUOTAGUARDSTATIC_URL に上書きします。(9) 左側の Tunnel から Download Configuration をクリックし、.qgtunnel ファイルをリポジトリに commit します。(10) 以上の設定により、destination.example.com:4443 へのアクセスが destination.example.com:443 に変換され、且つ IP アドレスも固定されます。なにかトラブルがあったら Heroku の環境変数 QGTUNNEL_DEBUG に true をセットして、ログを確認すると良いと思います。QuotaGuard Static の注意事項app.json の addons に quotaguardstatic を追加して、Review App を作ると Micro plan $19/mo で作られてしまいます。quotaguardstatic:starter とかにしても同じです。QuotaGuard Static のサポートに聞いたところ、Heroku のアカウント単位で設定を変える必要があるそう。問い合わせが必要です。まとめHeroku の worker Dyno 上で動く、Chrome のクローラで IP アドレスを固定化しました。固定 IP アドレス対応は出来るだけ避けたほうが良いと思いますが、どうしても必要な時にこの記事が参考になれば幸いです。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[『OAuth 2.0 の代表的な利用パターンを仕様から理解しよう』を読んだ話]]></title>
        <id>https://developer.feedforce.jp/entry/2019/04/12/184013</id>
        <link href="https://developer.feedforce.jp/entry/2019/04/12/184013"/>
        <updated>2019-04-12T09:40:13.000Z</updated>
        <summary type="html"><![CDATA[社内勉強会の担当でひさびさに認可の話をしました、 id:tmd45 です。ご存知ないかたもいらっしゃると思うので改めて書いておくと、弊社では毎週末 金曜 夕方にエンジニア全員で集まって行う社内勉強会、通称 FFTT を続けています。今回はこちらの記事を読んでまとめた内容（ほとんどそのまま🙇）で、ちょっと RFC8252 - OAuth 2.0 for Native Apps の話題なんかを追加してみました。シリーズ記事で第二回は OpenID Connect について扱われてるので、ぜひ読んでみてください。スライド speakerdeck.com過去に勉強したことOpenID TechNight で OpenID Connect とはなんぞやというのを聞いてきた - TMD45'β'LOG!!!OpenID BizDay で金融 API の動向について聞いてきた - TMD45'β'LOG!!!FAPI Security について聞いてきた話（2017/08/18 社内勉強会）きっかけになった書籍OAuth徹底入門 セキュアな認可システムを適用するための原則と実践作者:Justin Richer,Antonio Sanso翔泳社Amazon原著は『OAuth 2 in Action』ですね。会社の本棚に入ったのでちょっと読みました。まだ全部は網羅できてないです(；´∀｀)ひきつづき仕事でも活かしていきます(๑•̀ㅂ•́)و✧]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[沼 Advent Calendar 2018 総括]]></title>
        <id>https://blog.tmd45.jp/entry/2018/12/25/143619</id>
        <link href="https://blog.tmd45.jp/entry/2018/12/25/143619"/>
        <updated>2018-12-25T05:36:19.000Z</updated>
        <summary type="html"><![CDATA[メリークリスマス、皆様。各 Advent Calendar も最終日、クリスマスでございます。この記事は 沼 Advent Calendar 2018 の 25 日目の記事です。昨日は aini_bellwood さんの『ベイブレードの話 - Bellwood Works』でした。回転吸収かっこいい！見た目も性能も！心が中３男子なのでぐっと来ます。さてそんなわけで、沼 Advent Calendar へご参加いただいた皆様ありがとうございました。総勢 19 個の沼記事が集まりました。ちゃんと全部読ませていただいてます。いや〜 どれも面白かった。人の沼は楽しいなぁ。非常に俺得な Advent Calendar を作ってしまったなぁと思った次第でございます(ﾟ∀ﾟ)ﾌﾊﾊﾊﾊ23:22 追記： asonas さんが 8 日目の記事を書いてくれて「総勢 18 個」から「総勢 19 個」の記事になりました！師走の忙しいなかありがとう！！まとめ一覧のなかにもコメント追加済みです。沼 Advent Calendar 2018 - Adventarまとめインク沼に足首らへん？まで浸かっている話 - TMD45'β'LOG!!!リアル脱出ゲームという沼 - 真夜中の色彩ミュージカルと私 - SHOI’s blog私は沼にハマっていないということを主張しておきたい - はのちゃ爆発ボドゲ沼に辿り着くまでの紆余曲折 - kano-e no memoアドベンチャーゲームはいいぞ(核心ネタバレなし) - よもやま話β版2018年に関西で観た歌舞伎の感想 | ごひいきに沼完全攻略ガイド - 良いあそなすちゃん中古ガジェット沼について - どくぴーの備忘録オールドレンズのはなし - タイトルがないとトップページに戻りにくいことに気がついた女児アニメを楽しむ - mizukmb2018ベスト | jgs - Self-Made Keyboards in Japan沼の三種盛り合わせレシピ - 自転車、カメラ、キャンプ - Infinito Nirone 7ゲーム音楽の沼 - 兄貴の伝説 - hatena edition -標準ズームがすき - azmin's diaryキーボード沼マップ(ぬまっぷ)将太の寿司について - kasei_sanのブログ漫画と私｜kysn｜noteベイブレードの話 - Bellwood Worksおわりにまとめ私のコメントは読まなくていいんで、みんなお互いの沼をのぞきにいってくれよな！（そしてうっかり足を踏み外せ！）インク沼に足首らへん？まで浸かっている話 - TMD45'β'LOG!!!このあと大きめのボトル 10 本ほど増えました。やだこわいリアル脱出ゲームという沼 - 真夜中の色彩私も一度参加したことがありますが、リアル脱出ゲームは参加体験型エンタテイメントとして最高だと思います。また参加したいな〜ミュージカルと私 - SHOI’s blog好きが詰まってて、自分もミュージカル見に行きたくなりました。諸事情で映画館とか劇場とかに居られない質なのですが、なんとか近いうちに見に行けたらなぁ！私は沼にハマっていないということを主張しておきたい - はのちゃ爆発沼の住人には二種類いる。沼に浸かっていることを認めている人間と、認めていない人間だ( ｰ`дｰ´)ｷﾘｯ カメラ・レンズ沼、オーディオ沼、自作キーボード沼。Tech な沼ですね。ボドゲ沼に辿り着くまでの紆余曲折 - kano-e no memoボードゲームは気づいたら増えてるというのがこわいですねー。ホラーですねー。なんでだろー。ファミコン・マイコンが入り口っていうのがイイですね。世代を感じます。アドベンチャーゲームはいいぞ(核心ネタバレなし) - よもやま話β版私も "Ib" 大好きなので、サムネ見た瞬間に「あっ！！！」ってなりました。インディーズのアドベンチャーゲームは、（比較的）短時間で面白いストーリーが楽しめるのがいいと思ってます。2018年に関西で観た歌舞伎の感想 | ごひいきに観劇沼二本目、歌舞伎。こちらも各感想から愛を感じました。うちは明治座が近いのですが、歌舞伎や日本芸能の舞台も一度は見ておきたいものです。沼完全攻略ガイド - 良いあそなすちゃん12月8日428時28分に書いてくれました。レンズ沼の大三元とか、エンドゲームの話から、すっごく深くて狭い穴（沼）が大量に点在してる情景が浮かびました。たのしそうですね。中古ガジェット沼について - どくぴーの備忘録同僚のガジェット好き・電子工作好きの人たちから「わかる」「あるある」ってめっちゃ感想もらいました。わかる。オールドレンズのはなし - タイトルがないとトップページに戻りにくいことに気がついたレンズ沼二本目、オールドレンズ。味のあるレンズっていいですよね。昔トイカメラにハマってましたが、ガチのカメラ持ちでこっちのレンズに興味持ち出したら沼に沈んでたかもしれないと思います。女児アニメを楽しむ - mizukmb女児アニメ沼。なぜかプリキュアとか？と思ってましたが プリティーシリーズのお話でした。幼女おじさん（なんか並べただけで犯罪臭がする…）の配慮、おつかれさまです。2018ベスト | jgs - Self-Made Keyboards in Japan自作キーボード沼二本目。そもそもが "Self-Made Keyboards in Japan"（日本自作キーボード）の scrapbox での参加…沼感増しますね。TOFU の画像が犯行予告みたいで（？）かっこいい。沼の三種盛り合わせレシピ - 自転車、カメラ、キャンプ - Infinito Nirone 7沼の相乗効果、あると思います。とくにこの３つの組み合わせはヤバいですね…わかりみ。同じ組み合わせでハマってるひと、自分の近くでも見かけます。ゲーム音楽の沼 - 兄貴の伝説 - hatena edition -目次からしてすごく沼感ある記事でした。私もゲーム音楽のクラシックコンサートとかライブとか時々聞きに行きます。ゲーム体験と相まって大変良い。自分で演奏までしてるのすごい。標準ズームがすき - azmin's diaryレンズ沼三本目。レンズ沼は数あれど標準ズームの沼って初めて聞いたかもしれないです。写真かっこいいなぁ。自分も仕舞い込んでるデジイチひっぱりだしたくなりました。キーボード沼マップ(ぬまっぷ)ほんとに今年はエンジニア界隈で自作キーボードが一気に広まった年でしたね。自分の身の回りもすごいです。ぬまっぷ見ただけでも広く深いのがよく分かる、おもしろいまとめでした。将太の寿司について - kasei_sanのブログ内容は「ニンジャスレイヤーの動く沼」。忍殺ファン胡乱界隈の皆様が、集団でいろんな沼に浸かりに行く様を私も遠くからたのしく拝見しています。漫画と私｜kysn｜note漫画沼。実際お仕事でも漫画に関わってるという沼っぷり。エモくてちょっと感動しました。ベイブレードの話 - Bellwood Works冒頭でも触れましたが、いまのおもちゃって進化してますよねぇ。ベイブレードは出た当初からベーゴマの進化系として男の子たちの心を鷲掴みにしていたと思いますが、昨今はまたすごいことになってるんですね。おわりにはい、というわけで沼をまとめる沼の記事でした。そろそろ社会人のみなさまはボーナスなんてものの支給もあったのではないかとご推察いたします。楽しい沼クリスマスを。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[『オブジェクト指向設計実践ガイド』の読書会をやっています]]></title>
        <id>https://developer.feedforce.jp/entry/2018/12/22/191620</id>
        <link href="https://developer.feedforce.jp/entry/2018/12/22/191620"/>
        <updated>2018-12-22T10:16:20.000Z</updated>
        <summary type="html"><![CDATA[id:daido1976 です。入社してからあっという間に8ヶ月が経ちました。この記事は feedforce Advent Calendar 2018 の16日目の記事です。(遅くなってごめんなさい…)15日目は id:pokotyamu が 障害対応に強くなりたいのでレポートの書き方について考えてみた を書いてくれました！私も障害対応に強いエンジニアを目指して頑張りやす。さて、今回は現在弊社で行っている『オブジェクト指向設計実践ガイド』の読書会の紹介をしたいと思います。ちなみに昨年も読書会は開催されていたようですね。developer.feedforce.jp私個人としては前回の『メタプログラミングRuby』の読書会以来2度目の読書会です！developer.feedforce.jpで、どんな感じ？ざっくり進め方週に一度、参加したいメンバーが集まって毎回1章ずつ進めていきます。(現在第3章まで終了)その日やる章は読んできている前提で、事前に『気になったこと』『役立ちそうなこと』『疑問に思ったこと』『その他諸々話したいこと』などを付箋に書いてきてもらいます。それをホワイトボードに貼りながら、みんなであれこれ話し合います。その日の読書会の振り返りも含めて所要時間は1〜1.5時間ぐらいです。『オブジェクト指向設計実践ガイド』読書会のホワイトボード個人的にすごく好きな第3章のホワイトボード出てきた話題に個人的な感想を添えて先日やって記憶に新しい第3章の話が中心ですが、、、なんでもかんでも DI すればいいってもんでもないこういう本で出てきたテクニックってすぐプロダクトのコードで使いたくなるけど、何も考えずに DI しまくると、逆に読みづらいコードになるよね、というお話。本書内でも依存を減らす別解として DI ではなく、以下の Gear#wheel のようにクラス内の別メソッドにインスタンスの作成を切り出す方法も紹介されていましたね。class Gear  attr_reader :chainring, :cog, :rim, :tire, :wheel  def initialize(chainring, cog, rim, tire)    @chainring = chainring    @cog       = cog    @rim       = rim    @tire      = tire  end  def gear_inches    ratio * wheel.diameter  end  def wheel    @wheel ||= Wheel.new(rim, tire)  end# ...ちなみに私は DI について Wikipedia で 依存性の注入 のページを10回ぐらい読んでも全然意味がわからなかったのですが、この本を読んですんなり理解が出来ました。個人的には「Dependency injection (DI)」とか「依存性の注入」とか仰々しい名前があまり好きでないので、初学者に教える機会があれば「インスタンスの作成をクラスの外に切り出すと依存が減っていい感じに書けるよ」と言ってあげようと思います。キーワード引数と net/http の話第3章の中盤は「引数の順番への依存を取り除く」ための話が数ページに渡って続けられていたのですが、Ruby 2.4 ぐらいからプログラミングを始めたゆとりの私にとっては「ここに書いてあること全部、キーワード引数でよくないすか？？？」という感想でした。実際大抵の場合はキーワード引数で代替できるという話でしたが、例えば Ruby の標準ライブラリである net/http の Net::HTTP クラス だと、引数の数がむちゃくちゃ多いので全部キーワード引数になると辛いよねという話をしていただき、なるほどなーと思いました。「知っている」と「依存」は同義本書内では「○○を知っているオブジェクト」という言い回しがよく使われますが、オブジェクトが他のオブジェクトについて「知っている」ということは同時にオブジェクト同士が「依存関係にある」ことだ、と明言されており、これはわかりやすいねという話をしました。よくアプリケーションを開発している時でも、「このクラス (モジュール) が○○を知ってるのって自然かな？」みたいな話が上がるのですが、その観点って大事なんだなと腹落ちしました。結論『オブジェクト指向設計実践ガイド』めっちゃ勉強になります！！！オブジェクト指向設計実践ガイド ~Rubyでわかる 進化しつづける柔軟なアプリケーションの育て方作者: Sandi Metz,?山泰基出版社/メーカー: 技術評論社発売日: 2016/09/02メディア: 大型本この商品を含むブログ (6件) を見る最後に今回は弊社で行っている『オブジェクト指向設計実践ガイド』の読書会の紹介をしました！それにしても、『オブジェクト指向設計実践ガイド』がたまにプードル本って呼ばれる理由が全然わからなかったのですが、原題が『Practical Object-Oriented Design in Ruby』だから頭文字を取って POODR (プードル) なんですね…！Practical Object-Oriented Design in Ruby feedforce Advent Calendar 2018  の17日目は id:daikiki の 情熱と強引さが人を巻き込む ~ゴルフで感じた身になる教訓~ です！ゴルフの記事かと思ったら公私ともに活かせる「巻き込み力」のお話でした。ゴルフ好きな方もそうでない方もぜひご覧ください！それでは、またお会いしましょう ＾＾みんなのオブジェクト指向設計実践ガイド (POODR本)]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heroku の Preboot 機能を深掘りした]]></title>
        <id>https://developer.feedforce.jp/entry/2018/12/12/120000</id>
        <link href="https://developer.feedforce.jp/entry/2018/12/12/120000"/>
        <updated>2018-12-12T03:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは id:masutaka26 です。この記事は Heroku Advent Calendar 2018 の 12 日目の記事です。qiita.com昨日は @takuchalle さんの『Heroku で wkhtmltoimage を使う方法』でした。似たツールの webkit2png は使ったことがあります。webkit2png が動作するのは macOS だけですが、wkhtmltoimage は Windows と Linux でも動作するのですね。へぇー。先月、Heroku の Preboot 機能を調べる機会がありました。Web 上の日本語記事は Preboot が GA になった 2014 年頃の情報が多く、若干情報が古かったため、2018 年時点の最新の情報をまとめます。Preboot 機能とは何かどのように有効にするのか使用する Procfileデプロイによる heroku ps の変化デプロイ時の挙動デプロイ時のログ再起動時の挙動再起動時のログWeb dyno 1 台目Web dyno 2 台目注意事項まとめPreboot 機能とは何かProduction Check > Dyno redundancy に書いてあるとおり、Heroku では冗長性確保のため、本番環境の Web Dyno は 2 台以上が奨励されています。Running at least 2 web dynos for any mission-critical app increases the probability that the app will remain available during a catastrophic event. Multiple dynos are also more likely to run on different physical infrastructure (for example, separate AWS Availability Zones), further increasing redundancy.ただし、それだけだとデプロイや 1 日 1 回の自動再起動で、処理の遅延（H27 - Client Request Interrupted）や H28 - Client Connection Idle が発生することがあります。以下のグラフは Dyno Restart のタイミングで H27 と H28 Error が発生している様子です。このグラフはダッシュボードの Metrics から確認できます。これを回避できるのが Preboot 機能です。どのように有効にするのかHeroku CLI で有効にできます。$ heroku features:enable preboot -a YOUR_APP_NAME有効になりました。簡単ですね。$ heroku features:info preboot -a YOUR_APP_NAME=== prebootDescription: Provide seamless web dyno deploys [general]Docs:        https://devcenter.heroku.com/articles/prebootEnabled:     true使用する Procfile今回はこの Procfile を使います。release: bin/rails db:migrate db:seed# Web appweb: bin/rails s -p $PORT -e $RAILS_ENVworker: PROCESS_TYPE=worker bundle exec sidekiq -C config/sidekiq.yml# Ad appad_worker: PROCESS_TYPE=ad_worker bundle exec sidekiq -C config/sidekiq.ymlweb, worker, ad_worker という 3 つの process type を定義しています。今回注目する process type は web だけです。1 行目で release も定義しているため、デプロイ時には One-Off Dyno1 で release 処理をしたあと、生成した Slug が Dyno に展開されていきます。デプロイによる heroku ps の変化それでは $ watch 'heroku ps -a YOUR_APP_NAME' しつつ、デプロイの様子を観察していきましょう。macOS では watch コマンドは $ brew install watch 等でインストール出来ます。Web Dyno 1 台の場合、release Dyno の実行が終わると、web.1 Dyno のステータスが up → starting → up と変化します。starting 中にリクエストがあると、クライアントは待たされ、前述の H27 や H28 Error が発生します。Web Dyno 1 台でデプロイした様子Web Dyno 2 台かつ Preboot 無効でも、基本的に同じです。2 台同時に up → starting → up と変化します。Web Dyno 2 台かつ Preboot 無効でデプロイした様子Web Dyno 2 台かつ Preboot を有効にすると、期待した通り、1 台ずつ up → starting → up になりませんでした・・・！あれ？Web Dyno 2 台かつ Preboot 有効でデプロイした様子Logs will show new dynos starting soon after the slug compile is finished. The output of heroku ps will immediately show the state (for example, starting or up) of the new dynos. Old dynos are still running but won’t appear in heroku ps.Preboot のドキュメントによると、heroku ps では確認できないようです。ただしログでは確認できるそう。デプロイ時の挙動Approximately 3 minutes after the deploy completes (or 2 minutes after your app’s boot timeout limit, which defaults to 1 minute), HTTP requests will start routing to the new dynos and simultaneously stop routing to the old dynos.遅くともデプロイ完了から約 3 分後、もしくはアプリケーションの起動タイムアウトリミット 2 分（デフォルトは 1 分）後、HTTP リクエストは新 Dyno に Routing されるそうです。Shortly after the new dynos are fully live and serving user requests, the old dynos will be shut down. You’ll see them shut down in the logs as usual.新 Dyno が完全に起動し、ユーザリクエストを処理するとすぐに、旧 Dyno はシャットダウンされるようです。これもログで確認可能とのこと。では実際にログを見てみましょう。デプロイ時のログgit push 後まもなく、Web dyno の状態が up から starting に変わりました。Nov 28 14:58:51 heroku[web] notice RestartingNov 28 14:58:51 heroku[web] notice State changed from up to startingNov 28 14:58:51 heroku[web] notice RestartingNov 28 14:58:51 heroku[web] notice State changed from up to starting新 Web dyno 2 台で Puma (Web server) がそれぞれ起動を開始しました。Nov 28 14:59:17 heroku[web] notice Starting process with command `bin/rails s -p 24256 -e production`Nov 28 14:59:19 heroku[web] notice Starting process with command `bin/rails s -p 15417 -e production`Nov 28 14:59:21 app[web] info [heroku-exec] StartingNov 28 14:59:22 app[web] info [heroku-exec] StartingNov 28 14:59:31 app[web] info => Booting PumaNov 28 14:59:31 app[web] info => Rails 5.2.1 application starting in productionNov 28 14:59:31 app[web] info => Run `rails server -h` for more startup optionsNov 28 14:59:32 app[web] info => Booting PumaNov 28 14:59:32 app[web] info => Rails 5.2.1 application starting in productionNov 28 14:59:32 app[web] info => Run `rails server -h` for more startup optionsWeb dyno の状態が starting から up に変わりました。まだ Puma は起動しきっていません。Nov 28 14:59:33 heroku[web] notice State changed from starting to upNov 28 14:59:34 heroku[web] notice State changed from starting to up新 Web dyno 1 台で Puma が起動しきりました。問題はありませんが、手元で起動するよりずいぶん遅い気がします。Nov 28 15:02:03 app[web] info Puma starting in single mode...Nov 28 15:02:03 app[web] info * Version 3.12.0 (ruby 2.5.3-p105), codename: Llamas in PajamasNov 28 15:02:03 app[web] info * Min threads: 11, max threads: 11Nov 28 15:02:03 app[web] info * Environment: productionNov 28 15:02:03 app[web] info * Listening on tcp://0.0.0.0:24256Nov 28 15:02:03 app[web] info Use Ctrl-C to stop旧 Web dyno 1 台で Puma が終了しました。前述の意訳では約 3 分とあるため合っています。Nov 28 15:02:19 heroku[web] notice Stopping all processes with SIGTERMNov 28 15:02:19 app[web] info Gracefully stopping, waiting for requests to finishNov 28 15:02:19 app[web] info === puma shutdown: 2018-11-28 06:02:19 +0000 ===Nov 28 15:02:19 app[web] info Goodbye!Nov 28 15:02:19 app[web] info ExitingNov 28 15:02:19 heroku[web] notice Process exited with status 143もうひとつの旧 Web dyno 1 台で Puma が終了しました。これも意訳のとおりです。Nov 28 15:03:25 heroku[web] notice Stopping all processes with SIGTERMNov 28 15:03:25 app[web] info Gracefully stopping, waiting for requests to finishNov 28 15:03:25 app[web] info === puma shutdown: 2018-11-28 06:03:25 +0000 ===Nov 28 15:03:25 app[web] info Goodbye!Nov 28 15:03:25 app[web] info ExitingNov 28 15:03:25 heroku[web] notice Process exited with status 143もうひとつの新 Web dyno 1 台で Puma が起動しきりました。Booting Puma からずいぶんと時間がかかりました。Nov 28 15:05:08 app[web] info Puma starting in single mode...Nov 28 15:05:08 app[web] info * Version 3.12.0 (ruby 2.5.3-p105), codename: Llamas in PajamasNov 28 15:05:08 app[web] info * Min threads: 11, max threads: 11Nov 28 15:05:08 app[web] info * Environment: productionNov 28 15:05:08 app[web] info * Listening on tcp://0.0.0.0:15417Nov 28 15:05:08 app[web] info Use Ctrl-C to stopPuma の起動に時間がかかるのは謎ですが、Preboot のドキュメントと相違ないと理解しました。再起動時の挙動再起動時の挙動も確認します。Heroku ではアプリケーションの健全性を維持するため、Dyno は 1 日に 1 回、自動的に再起動します。Dyno は差し替えられるため、ローカルファイルシステムの変更はすべて破棄されます。The new dynos will start receiving requests as soon as it binds to its assigned port. At this point, both the old and new dynos are receiving requests.新 Dyno は割り当てられたポートにバインドされると、すぐにリクエストを受信し始めます。この時点で、新旧の Dyno がリクエストを受信するそう。Approximately 4 to 6 minutes after the restart is invoked, the old dynos will be shut down. You’ll see them shut down in the logs as usual.再起動が行われてから、約 4~6 分後に旧 Dyno がシャットダウンされるようです。これもログで確認可能とのこと。Preboot が GA になった 2014 年頃は、まだ 1 日 1 回の再起動には対応していないようでした。現在は対応しているようです。再起動時のログ結論から書くと、1 台目が再起動し終えてから 2 台目が再起動していました。再起動においても、ドキュメントの通りに Preboot が機能しているようです。Web dyno 1 台目Web dyno 1 台の状態が up から starting に変わりました。Nov 30 11:41:22 heroku[web] notice CyclingNov 30 11:41:22 heroku[web] notice State changed from up to starting新 Web dyno 1 台で Puma が起動を開始しました。Nov 30 11:41:42 heroku[web] notice Starting process with command `bin/rails s -p 32434 -e production`Nov 30 11:41:44 app[web] info [heroku-exec] StartingNov 30 11:41:50 app[web] info => Booting PumaNov 30 11:41:50 app[web] info => Rails 5.2.1.1 application starting in productionNov 30 11:41:50 app[web] info => Run `rails server -h` for more startup optionsWeb dyno 1 台の状態が starting から up に変わりました。まだ Puma は起動しきっていません。Nov 30 11:41:51 heroku[web] notice State changed from starting to up新 Web dyno 1 台で Puma が起動しきりました。相変わらず時間がかかります。Nov 30 11:45:45 app[web] info Puma starting in single mode...Nov 30 11:45:45 app[web] info * Version 3.12.0 (ruby 2.5.3-p105), codename: Llamas in PajamasNov 30 11:45:45 app[web] info * Min threads: 11, max threads: 11Nov 30 11:45:45 app[web] info * Environment: productionNov 30 11:45:45 app[web] info * Listening on tcp://0.0.0.0:32434Nov 30 11:45:45 app[web] info Use Ctrl-C to stop旧 Web dyno 1 台で Puma が終了しました。前述の意訳では約 4~6 分後とありましたが、間を置かずに終了しました。Nov 30 11:45:55 heroku[web] notice Stopping all processes with SIGTERMNov 30 11:45:56 app[web] info Gracefully stopping, waiting for requests to finishNov 30 11:45:56 app[web] info === puma shutdown: 2018-11-30 02:45:55 +0000 ===Nov 30 11:45:56 app[web] info Goodbye!Nov 30 11:45:56 app[web] info ExitingNov 30 11:45:56 heroku[web] notice Process exited with status 143Web dyno 2 台目もうひとつの Web dyno 1 台の状態が up から starting に変わりました。14 分くらい差があります。最大 216 分のランダムな差があるようなので、記載通りです。Nov 30 12:00:41 heroku[web] notice CyclingNov 30 12:00:41 heroku[web] notice State changed from up to starting新 Web dyno 1 台で Puma が起動を開始しました。Nov 30 12:00:59 heroku[web] notice Starting process with command `bin/rails s -p 16965 -e production`Nov 30 12:01:01 app[web] info [heroku-exec] StartingNov 30 12:01:08 app[web] info => Booting PumaNov 30 12:01:08 app[web] info => Rails 5.2.1.1 application starting in productionNov 30 12:01:08 app[web] info => Run `rails server -h` for more startup optionsWeb dyno 1 台の状態が starting から up に変わりました。まだ Puma は起動しきっていません。Nov 30 12:01:10 heroku[web] notice State changed from starting to up新 Web dyno 1 台で Puma が起動しきりました。今回は時間がかかりませんでした。Nov 30 12:01:15 app[web] info Puma starting in single mode...Nov 30 12:01:15 app[web] info * Version 3.12.0 (ruby 2.5.3-p105), codename: Llamas in PajamasNov 30 12:01:15 app[web] info * Min threads: 11, max threads: 11Nov 30 12:01:15 app[web] info * Environment: productionNov 30 12:01:15 app[web] info * Listening on tcp://0.0.0.0:16965Nov 30 12:01:15 app[web] info Use Ctrl-C to stop旧 Web dyno 1 台で Puma が終了しました。今度は時間がかかりました。前述の意訳では約 4~6 分後とあるため合っています。Nov 30 12:05:14 heroku[web] notice Stopping all processes with SIGTERMNov 30 12:05:14 app[web] info Gracefully stopping, waiting for requests to finishNov 30 12:05:14 app[web] info === puma shutdown: 2018-11-30 03:05:14 +0000 ===Nov 30 12:05:14 app[web] info Goodbye!Nov 30 12:05:14 app[web] info ExitingNov 30 12:05:14 heroku[web] notice Process exited with status 143注意事項Preboot を有効にしてデプロイすると、無効時よりも時間をかけて Dyno が差し替わります。全ての Dyno に反映されるまで数分かかることもあるようです。この点はチームで把握しておく必要があります。最大 3 分間、新旧の Dyno が重複するため、Add-on の同時接続数の制限に達する可能性があります。Heroku Postgres の場合は pgbouncer buildpack を使うことが奨励されています。新旧の Dyno が混在することにより、一度のデプロイで DB schema とアプリケーションコードを変更すると、不具合が発生する可能性があります。Preboot 機能を一旦無効にすることを勧めています。でも、Procfile に release process type を定義していれば同じなので、個人的には気にならないですね。Preboot 機能を使うためには、Dyno type が Standard-1X 以上である必要があります。Dyno type が Free と Hobby では Preboot を有効に出来ません。Preboot が適用されるのは Web Dyno だけです。自分で定義した worker Dyno 等は対象外です。まとめHeroku の Preboot 機能を深掘りし、有効にする必要性を理解しました。Web Dyno を 2 台以上にしたら Preboot を有効にすると良さそう前述の「注意事項」には留意したほうが良いしかし、こんなに長い記事になるとは...。明日の heroku Advent Calendar 2018 はまたまた id:masutaka26 です。Bitbar の便利な Heroku plugin を紹介してくれるみたいです。お楽しみに。このような使い捨ての Dyno を One-Off Dyno と呼びます。heroku run で使われる Dyno もそうです。↩]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[まだ .rubocop_todo.yml で消耗してるの？]]></title>
        <id>https://developer.feedforce.jp/entry/2018/12/05/140000</id>
        <link href="https://developer.feedforce.jp/entry/2018/12/05/140000"/>
        <updated>2018-12-05T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[どうも、バックエンドエンジニアのサトウリョウスケです ✌︎('ω')✌︎若干釣り臭いタイトルですが、先日 RubocopChallenger という gem の v1.0.0 をリリースしたので紹介させて頂きます 🙏github.com経緯僕が所属している ソーシャルPLUS は 2012 年頃から開発が始まりました。Rails のプロダクトとしては古株の方だと思います。ソーシャルPLUS に RuboCop が導入されたのは 2017/02 頃 1 で、それまで特に RuboCop を意識したコードで開発を進めてこなかったので、巨大な .rubocop_todo.yml が出力され、それが手付かずのままになってしまっていました。ちなみに当初は 1669 行 195 種類 の違反ルールがありました。このままでは RuboCop の恩恵が受けられないので、 RuboCop Challenge と称して (以前 Ice Bucket Challenge が流行っていたので) 週イチで .rubocop_todo.yml から違反ルールを一つ消して、 auto-correct で修正する、という事をやっていたのですが、数ヶ月（数週間だったかも）ですっかりやるのを忘れてしまいました 😇最近また RuboCop Challenge を再開しよう、という流れになったのですが、手動でコツコツやるのも精神的にしんどくなって来たので、なんとか自動化したいな、という気持ちになり、手動でやっていた Rubocop Challenge を Ruby スクリプトで動かせるようにしました。最初は単純な Ruby スクリプトだったのですが、せっかくなので gem 化しよう、という事になり、作成したのが RubocopChallenger です。ところで、弊社エンジニアの id:masutaka26 が circleci-bundle-update-pr という CI を利用した Bundle Update の自動更新 gem を作成しています。github.comこれに感銘を受けて (？) 自分の RubocopChallenger も CI から$ rubocop --auto-correct を実行した結果が PR として届くような仕組みになっています。使い方1. Gemfile に rubocop_challenger を追加gem 'rubocop_challenger', group: :development, require: false2. GitHub personal access token の作成RubocopChallenger が PR を作成するために GitHub personal access token が必要になります。Settings から Generate new token をクリックして access token を作成します。Select Scopes では repo にチェック ✅ を入れて下さい。3. CircleCI で環境変数の設定今回は CircleCI での利用例を紹介します 🙏ダッシュボード画面 に移動し、 RubocopChallenger を適用したいアプリケーションの Project Settings -> Environment Variables へと移動します。Add Variable をクリックして GITHUB_ACCESS_TOKEN という Key で先程作成した GitHub personal access token を設定します。4. .circleci/config.yml の編集以下に設定例を紹介します。# .circleci/config.ymlversion: 2jobs:  rubocop_challenge:    docker:      - image: circleci/ruby:2.5-node-browsers    working_directory: ~/repo    steps:      - checkout      - run:          name: Rubocop Challenge          command: |            bundle install            bundle exec rubocop_challenger go \              --email={RubocopChallenger が commit する際の user email} \              --name="{RubocopChallenger が commit する際の user name}"workflows:  version: 2  nightly:    triggers:      - schedule:          cron: "30 23 * * 1,2,3" # この設定の場合、火水木 の毎朝 8:30 に RubocopChallnger の PR が届きます          filters:            branches:              only:                - master    jobs:      - rubocop_challenge5. 作成された PR の確認ここまでの手順を終えると、 CircleCI に指定したスケジュールで PR がされるようになると思います。後は auto-correct の内容を確認して、 merge するだけです。中には適用したくないルールも出てくると思いますが、その場合は .rubocop.yml にルールを再定義して auto-correct されないようにします。どんな PR が作成されるのか？ちょっと RubocopChallenger のバージョンが古い頃の物ですが、 以下のような PR が自動的に作成されます。デフォルトの設定では、 .rubocop_todo.yml の中から Cop supports --auto-correct かつ Offense count が最大 のルールを消して、 auto-correct した結果が PR として作成されます。また、 auto-correct の後で $ rubocop --auto-gen-config を実行して .rubocop_todo.yml を再作成しています。 RuboCop のバージョンが変わった時とかに .rubocop_todo.yml に出力される内容も若干変わっていたりするのですが、毎回最新の状態に作り直してくれるのがちょっと便利だったりします。ちなみに PR の Description に表示されている内容は 本家 RuboCop の RubyDoc に記載されている内容と同じものです。これを表示するのに地味に苦労しました 😓高度な設定RubocopChallnger にはいくつかオプションが用意されているので、ご紹介します。--mode上述の通り、デフォルトでは auto-correct の対象は Cop supports --auto-correct かつ Offense count が最大 のルールが選択されますが、 mode に渡す値によって対象を変更することが出来ます。most_occurrence (デフォルト)Offense count が最大 のルールを選択するleast_occurrenceOffense count が最小 のルールを選択するrandom全体からランダムに選択する使用例$ bundle exec rubocop_challenger go \    --email=rubocop-challenger@example.com \    --name="Rubocop Challenger" \    --mode=random --labelsRubocopChallnger が作成する PR に付与される label を指定します。デフォルトでは rubocop challenge というラベルが付与されます。ソーシャルPLUS では waffle.io を利用していたりするのですが、レビュー待ち状態の label を付与するようにすると見落としがなくて便利です。スペース区切りで複数指定することが出来ます。使用例$ bundle exec rubocop_challenger go \    --email=rubocop-challenger@example.com \    --name="Rubocop Challenger" \    --labels="rubocop challenge" "in progress"--template作成される PR をカスタマイズしたい場合などあるかと思います。その場合は template に erb ファイルのパスを指定することが可能です。デフォルトでは以下のテンプレートが使用されるので、必要に応じてカスタマイズしてご利用下さい 🙏https://github.com/ryz310/rubocop_challenger/blob/master/lib/templates/default.md.erb使用例$ bundle exec rubocop_challenger go \    --email=rubocop-challenger@example.com \    --name="Rubocop Challenger" \    --template=./path/to/template.md.erb--no-regenerate-rubocop-todo上述の通り、デフォルトでは auto-correct の後で $ rubocop --auto-gen-config を実行して .rubocop_todo.yml を再作成しています。これが不要な場合は no-regenerate-rubocop-todo オプションを指定します。使用例$ bundle exec rubocop_challenger go \    --email=rubocop-challenger@example.com \    --name="Rubocop Challenger" \    --no-regenerate-rubocop-todo既知の不具合 (v1.2.0 で解消済み)RuboCop のルールの中には Cop supports --auto-correct と表記されているにも関わらず、部分的にしか auto-correct してくれないものがあります。例えば Style/Semicolon が auto-correct できるのは行末に ; が存在する場合だけのようです。puts 'hoge'; # => auto-correct されるputs 'fuga'; puts 'piyo' # => auto-correct されないこのようなルールが auto-correct 対象に選ばれると、RubocopChallenger を実行した後も違反が解決されず、後続の $ rubocop --auto-gen-config で再度 .rubocop_todo.yml に対象ルールが出てきてしまうので、 RubocopChallenger が機能しない状態になってしまいます。現状では Style/Semicolon のようなルールに遭遇した場合は、手動で .rubocop.yml にルールを移動させる必要があります。今のところ RubocopChallenger 側での対策を思い付いていないので、もし良いアイデアがありましたら教えて下さい 🙏2019/03/26 追記RubocopChallenger v1.2.0 で Ignore リスト機能が追加されました。RubocopChallenger を実行した後も違反が解決されず、後続の $ rubocop --auto-gen-config で再度 .rubocop_todo.yml に対象ルールが出てきてしまった場合、 .rubocop_challenger.yml というファイルが作成され、対象ルールが Ignore リストに追加されます。Ignore リストに追加されたルールは次回以降、 RubocopChallenger の対象ルールとして選択されなくなるので、特に何もしなくとも運用を続けることが可能となります。最後に当初は 1669 行 195 種類 の違反ルールが .rubocop_todo.yml に存在していましたが、 RubocopChallenger を導入してから 3 ヶ月で 1187 行 132 種類 まで減らすことが出来ました。 auto-correct 可能な違反ルールはあと 62 種類残っているので、あと半分くらいまでは減らすことができそうです。本記事では肥大化してしまった .rubocop_todo.yml を自動的に修正していく RubocopChallnger を紹介し、導入方法について解説させて頂きました。 .rubocop_todo.yml が肥大化して困っているプロジェクトで役立てて頂ければ幸いです 🙏また、利用してみてフィードバックなどあれば Issue にてご連絡下さい。 GitHub では頑張って拙い英語を書くようにしていますが、日本語でも大丈夫です 🙆 PR も大歓迎です。どうぞ宜しくお願い致します 🙇ちなみに導入したのは自分です↩]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[社内では開発チームマネージャーを名乗っています]]></title>
        <id>https://developer.feedforce.jp/entry/2018/12/05/110000</id>
        <link href="https://developer.feedforce.jp/entry/2018/12/05/110000"/>
        <updated>2018-12-05T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんばんちわ、 id:tmd45こと玉田です。この記事は feedforce Advent Calendar 2018 の 5 日目の記事です。昨日は dfplus.io ブログ担当 1 号こと id:tgacky さんによる『2歳になった dfplus.io 成長の記録』でした。dfplus.io 3 年目突入おめでとうございます🎉さて今回は予告どおり、たまには真面目に自分の仕事について書いてみたいと思います。所属と経歴インターネット上では本業：光の戦士1、散財部風紀取り乱し委員わいわい係として、おおむね tmd45 という ID で存在しています。ふざけたプロフィール（気に入ってます）ですが、現実のフィードフォース社内ではプロダクト『ソーシャルPLUS®』の "開発チームマネージャー" を名乗っています。注追記: 自分の場合、役職ではないので外向けにはマネージャーではなくリーダーとなっています。フィードフォースは社歴 2 社目で今年で勤続 5 年になりました。プロダクトが複数ある弊社内では珍しく（？）この 5 年間ずっとソーシャルPLUS 一筋にやっています。あんまりひとつのプロダクトに古参が長くいるのも良くはないんですが、今回そこは置いておいてもらって…フィードフォースの組織とマネジメント弊社の組織としての取り組みは、以下の記事が詳しいと思います。www.hito-link.jpまた、以前このブログでも取り上げた「キャリアパス」という仕組みを用いているのですがdeveloper.feedforce.jp現在エンジニアのキャリアパスは以下の３つに分けられています。エンジニアリング志向エンジニア（技術プロフェッショナル志向）マネジメント志向エンジニアビジネス志向エンジニア各エンジニアがどの志向であっても、その価値が認められるよう それぞれに ジュニア ～ エキスパート の 4 段階（さらに内側で 2 段階に分かれるので実際には 8 段階ある）をもって、求められるスキルと行動特性を定義しています。キャリアパスが定められる前、所属 4 年目くらいでいわゆる「リーダー職」として（端的に言えば管理職手当が付く）開発チームリーダーをやっていました。キャリアパスが作られてからは「マネジメント志向エンジニア」を選び、それを基準に目標を定めたり評価してもらったりしています。ソーシャルPLUS チームの構成いまこの記事を書きながら初めてきちんと調べましたが、ソーシャルPLUS のリリースは 2012 年 4 月 17 日とのことで、6 年半も続いているサービスです。自分が参加してからの 5 年でも、チームメンバーや人数、その構成は様々に変化してきました。現在は非エンジニアのプロダクトマネージャーが統括し、ビジネスチームと開発チームがそれぞれに仕事をしています。ソーシャルPLUSチーム構成（概要）図自分（水色の人影）は開発チームの一員ですが、プロダクトの経験年数が長いことや、マネジメント志向であることで「チームマネジメント」を担っています。現在のエンジニアの人数は自分を含め８名です。自分が "チームマネージャー" を名乗って意識しているのは、8 人のメンバーがチームとしてより良く力を発揮しながらイイ雰囲気で仕事をしたい、ということです。技術ももちろん好きですし、ソーシャルPLUS というプロダクトに強い思い入れも理想もありますが、それでも自分が一番放っておけないのがソコだったという感じです。開発チームマネージャーとしてやっていることかんたんにまとめると以下のようなことをやってます。すべてが上手くいってるわけじゃなくて、試行錯誤しながらだけど、自分のお気持ちを中心に書いてます。プロダクトマネージャーとエンジニアメンバー間の緩衝材衝突というと語弊がありますが「あ、ちょっと説明足りないな」みたいなところを双方に補足したりしてます。エンジニアメンバー同士でもそういうことはあるので、心理的安全性の確保とも言えるかも。プロダクト開発の優先度の最終決定たいていのものはプロダクトオーナーとチームで話し合って決めますが、これといった決め手に欠けるものには自分が 経験と勘で 決めたりします。プロダクト開発チームメンバーとの 1on1月初に 1 回、チームのエンジニア全員（7 名）と 30 分ずつ話しています。社内の 1on1 のなかでは頻度は低いほうですね。事前に以下の項目をアンケートで答えてもらい、それを確認しながら雑談しています。前月でやったこと・わかったこと（YW）前月の自分のパフォーマンス（1 ～ 5 選択＋コメント）今月やろうと思うこと（T）チームやプロダクトについて（1 ～ 5 選択＋コメント）会社について（1 ～ 5 選択＋コメント）その他 自由欄「プロダクトの開発」に関することを中心に、会社のことも含め割と広範囲に話しています。チームメンバーのお気持ち確認とか、各々の体調だったり生活だったりの少しプライベートなことを聞かせてもらうこともあります。個別に相談されることでも、コトによっては「You それチームに言ってみな Yo! (σゝ∀・)σ」とチームの問題にしてもらったり（たいていは週一でやっている振り返りKPTで出してもらいます）。開発マネジメント定例というマネージャーが集まる定例にエスカレーションして、もっと広い範囲で解決を試みたりもします。気をつけてるつもりでも、自分がおしゃべりな質なのでこちらが喋りすぎている気はします。毎度反省。けど 1on1 やってよかった、というフィードバックをちょくちょくもらえてるので私もありがたいです。プロダクト開発チームミーティングの司会進行上の緩衝材の話とも関連してますが、ミーティングの雰囲気づくりも兼ねてるので司会進行業は自分がやることが多いです。とはいえ、自分が居ないと定例が進まないというのも問題なので、ここは逆になんとかしておきたい状況です。プロダクト開発の落ち穂拾い計画しているタスク以外にも、毎日いろんなことが起こります。落ち穂拾いといっても、全部私が片付けているというわけではなく、メンバーの状況を見てタスクとしてお願いするということもします。拾いにくい落ち穂を拾う、というのが正しいかも。拾いやすいもの（それも人によって違う）は自主的に拾ってくれるメンバーなので、助かってます。おわり他にもいろいろやってることがあるんですが、なかなか文章にするのが難しいですね。今回はこのへんで。明日の feedforce Advent Calendar 2018 は id:funesan2 による『煮る！』お話です。いったい何を煮たのか…おたのしみに！オンラインMMORPG『FINAL FANTASY XIV』のプレイヤーの意↩]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[インク沼に足首らへん？まで浸かっている話]]></title>
        <id>https://blog.tmd45.jp/entry/2018/12/01/000000</id>
        <link href="https://blog.tmd45.jp/entry/2018/12/01/000000"/>
        <updated>2018-11-30T15:00:00.000Z</updated>
        <summary type="html"><![CDATA[この記事は 沼 Advent Calendar 2018 の 1 日目の記事です。ご参加予定の皆さまありがとうございます。こちらは自分が浸かってる沼について語りつつ、他の沼を覗いてみようみたいな趣旨のゆるい Advent Calendar です。まだ空きもたくさんありますので、お気軽にご参加ください。ここで言う「沼」とは、以下のようなスラングとしての沼を指します。あるジャンルが好きになり過ぎてハマってしまい抜け出せなくなること。沼落ち。さっそく私が最近ハマり始めた沼のお話です。万年筆インク沼これから始めてみたい人へのおすすめ万年筆PILOT カクノPILOT プレラ 色彩逢いTWSBI ECO / TWSBI ECO-Tこれから始めてみたい人へのおすすめインクインクがたくさん増えてきたらさらに沼へインク沼っていうか文房具沼が深いおわりに万年筆インク沼すでに過去の記事に書いてますが、9月から紙のシステム手帳を日記代わりに使い始め、それをきっかけに 10 月頃から万年筆を使うようになりました。以下がこれまでの成果です。blog.tmd45.jpblog.tmd45.jpblog.tmd45.jpblog.tmd45.jp現時点で持っているインクを一覧にしてみました。自分だけのインク手帳を作る愉しみこういうものを作ること自体が楽しい(*´ｪ`*)似たような色でも、濃く重ねたり、水で伸ばしたり、時間が経ったりすると違う味が出てくるのとこれから始めてみたい人へのおすすめ万年筆最近、万年筆やインク自体の流行りが来てるそうです。その流れもあってか、国内メーカーのサイトには万年筆の入門・紹介ページがあったりします。私も最初は PILOT の万年筆入門記事を読んでました。万年筆は「書く為の道具」 | 特集記事一覧 | PILOT LIBRARY | PILOTインクの入れ方や、お手入れの仕方などが動画になっていてわかりやすいと思います。PILOT カクノ文房具屋さんでの取扱いも多く、そもそも入門用として作られているので、どこでもおすすめされているのが PILOT カクノ。1,000円程度 で購入することができ、カートリッジインクとかんたんな説明書が同封されています。他の廉価万年筆に比べて、ペン先の太さのバリエーションが多いのも魅力でしょうか。手帳など細かく書くのに使うなら EF（極細字）や F（細字）、それ以外でインクの色をしっかり楽しみたいなら M（中字）以上の太さが良いかと思います。ボトルインクを楽しむときには、別売りのコンバーターを購入する必要があります。kakuno/万年筆・カクノ F細字【透明ボディ/ノンカラー】 FKA-1SR-NCF出版社/メーカー: パイロットメディア: この商品を含むブログを見るPILOT プレラ 色彩逢い3,000円台 でおすすめなのは PILOT プレラ 色彩逢い。全長が短く小ぶりなボディが可愛らしく、クリアカラーで中に入れるインクと色を合わせるのも楽しいです。ペン先のバリエーションは F（細字）と CM（カリグラフィ）というちょっと変わった組み合わせ。本格的なカリグラフィには向かないですが、太い線と細い線の両方を表現できる（明朝体のような感じ）のが面白くて、私は普段から使っています。こちらはコンバーターが付属しているので、インクがあればそれを補充して使うことができます。【PILOT】パイロット プレラ 色彩逢い(いろあい) PRERA 万年筆F （細字）ブルー出版社/メーカー: PILOTメディア: オフィス用品この商品を含むブログを見るPilot プレラ Iro-Ai カリグラフィー用のペン先 万年筆ボディ出版社/メーカー: Pilot発売日: 2014/10/01メディア: オフィス用品この商品を含むブログを見るTWSBI ECO / TWSBI ECO-T最近自分で使いはじめて気に入ってるのが、5,000円台 で入手可能な TWSBI ECO。TWSBI は台湾の万年筆メーカーです。こちらもペン先の種類が豊富です。珍しいところでは STUB 1.1 という極太のペン先が存在しています。プレラのカリグラフィと同様に、太い線と細い線を表現することができます。1,000円、3,000円 のものよりしっかりした感触で「万年筆を使ってるゼ」という満足感が得られる気がします（笑）TWSBI（ツイスビー） ECO 万年筆 F（細字） ホワイト M7444340出版社/メーカー: TWSBIメディア: オフィス用品この商品を含むブログを見るこれから始めてみたい人へのおすすめインク手に入りやすく、色も豊富、名前も素敵で、ミニボトルの価格帯もお財布に優しいということで、やっぱり PILOT 色彩雫はおすすめです。PILOT　色彩雫　イロシズク　ミニ　3色セット　15ml　専用ケース入り　INK-15　3色自由に選べます出版社/メーカー: PILOTメディア: この商品を含むブログを見る同様にセーラー万年筆の四季織シリーズも。セーラー万年筆 万年筆 ボトルインク 四季織 十六夜の夢 雪明 13-1008-210出版社/メーカー: セーラー万年筆発売日: 2017/09/29メディア: オフィス用品この商品を含むブログを見るこういったメジャーなインク以外にも、以前の記事でも紹介しましたが、HAPPY INK DAYS archives というブログで毎日いろいろなインクが紹介されています。見てるだけでも楽しいですし、物欲も刺激されます。自分の手で、この色で、好きなものを書いてみたいな～と妄想するのがたまりません。インクがたくさん増えてきたらすべてを万年筆に入れておくのは難しいです。万年筆は普段から使っていないと、ペン先のインクが乾いたりしてしまいます。万年筆の一番のメンテナンスは「毎日使うこと」とのことです。なので、普段使う万年筆には使いやすい色とか、とくにお気に入りの色とかを入れて持ち歩きます。そうでないインクは、つけペンやガラスペンなどを使って楽しむようにしています。ルビナート ガラスペン NOV/D アメジスト#6出版社/メーカー: ルビナート(Rubinato)メディア: オフィス用品この商品を含むブログを見るインク沼として楽しむのであれば、万年筆ではなくつけペンだけでもいいかもしれませんね。さらに沼へこのように、インクの魅力に惹かれていった結果、万年筆が何本も増えたり（まだお高いものには手を出していないですが^^;）、つけペンが増えたり…さらには「万年筆インクで書きやすい紙」についてもこだわりはじめます。もともと文房具そのものが好きということもあって、「万年筆インクで書きやすい紙」もいろいろな種類を買って試してみています。ミドリ メモ MDペーパーパッド A5 無罫 15235006出版社/メーカー: デザインフィル発売日: 2018/02/28メディア: オフィス用品この商品を含むブログを見る神戸派計画 GRAPHILO paper A5出版社/メーカー: 神戸派計画メディア: オフィス用品この商品を含むブログを見るこれ以外にもいろいろ買いましたが、それはまた別の機会にまとめたいと思います。インク沼っていうか文房具沼が深いTwitter や Facebook、Instagram などあらゆる場所で文房具に関するハッシュタグやグループが盛り上がっています。そういうのを眺めるのも楽しみのひとつです。#手書きツイート - Twitter#ゆる書写 - Twitter#万年筆イラスト部 - Twitterハコペンさんの書写ツイートがとても美しくて好きです。ゆる書写を書きました。江戸川乱歩 「鏡地獄」より。ブラックライトで光らせてみました。使用インク : Organics studio Nitrogen、Walden pond 、色彩雫 山葡萄 #ゆる書写 #書写 #手書きツイート #手書き #インク沼 #色彩雫 #Organicsstudio #濃淡萌え#深夜のゆる書写60分一本勝負 pic.twitter.com/plAwlgkLDk— ハコペン (@hakoppen2018) 2018年11月11日ほぼ同様のハッシュタグが Instagram にもあります。おわりにまだまだ紹介したいものがたくさんありますが、一記事にするにはいろいろありすぎるのでこのへんで。というか参加ハードルを下げるためにさらっと書いて終わろうと思ってたのに、めっちゃ書いてしまった。沼だわぁ。明日は maccha418 さんが『リアル脱出ゲーム（SCRAP）沼』について書いてくれるとのこと。私もリアル脱出ゲームは一度参加したことがありますが、素敵なエンタテイメントですよね！引き続き 沼 Advent Calendar 2018 をよろしくおねがいします☺]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[『Heroku と EC Booster と私』と『Heroku ちょこっと Deep Dive』という発表をした]]></title>
        <id>https://developer.feedforce.jp/entry/2018/11/14/140000</id>
        <link href="https://developer.feedforce.jp/entry/2018/11/14/140000"/>
        <updated>2018-11-14T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。増田（id:masutaka26）です。以前住んだ街を訪れるのが趣味です。週次の社内勉強会 FFTT で『Heroku と EC Booster と私』と『Heroku ちょこっと Deep Dive』という発表をしました。まさかのダブルタイトル・・・！ スライドを作っていたら別々の話のような気がして、こうなりました。発表の内容『Heroku と EC Booster と私』は主に社内向けの話です。最近の EC Booster のインフラを紹介する機会がなかったので、今回の発表の場を借りました。『Heroku ちょこっと Deep Dive』では Heroku の Slug について、ちょこっと調べました。Heroku では Dyno という軽量コンテナ上で、アプリケーションが動作します。アプリケーションは Slug というパッケージに固められています。Slug にはアプリケーションコードの他に Buildpack や、Ruby なら bundle install されたライブラリ等が含められています。最近 Heroku を Terraform で管理し始めた時に、heroku_slug というリソースを見つけました。使いはしなかったのですが、自分で Slug を変更できる感覚がよく分からなくて調べたことが始まりでした。まさか Scratch から作れるとは思いませんでしたし、Slug の中身やサイズの増減を把握する方法が分かったことは大きな収穫でした。発表では話し忘れたのですが、Heroku は Stack に日々セキュリティパッチが当たって、24 時間以内に適用されることは大きなメリットです。小さなチームで当てるべきパッチの把握や適用をするのは、相当しんどいですからね。それでは、発表後に書いてくれた付箋への返信です。発表の感想付箋紙に感想を書いてくれました。 は私からのコメントです。 いつもありがとうございますふだん Heroku Review App しか使ってなかったんですが、興味を持ちました Rails アプリの公開にも、小さなスクリプトの定期実行にも使えるので便利ですよマッスル マスタカ！💪 マッスル マッスル！💪Heroku 未だに HTTP 1.1 なんですよね そうなんですよねー 💦 さすがにそろそろ...早く AWS + CloudFront 使いたい assets は CDN に逃したいですねー(dfplus.)io はいまだにフロントは Heroku 使ってマスタカ そうでありマスタカPush された tar ball に build された file が入るんですか そんな感じですーEC Booster のリリース時に Heroku を徹底的につかいたおすことを目標にしていたので、この発表もマスタカさんのふだんの仕事もとても感謝しています もったいないお言葉、ありがとうございます！「Heroku は良いサービス」めっちゃわかる だよねーHeroku は良しなにやってくれるけど、その「良しなに」部分もちゃんとオープンになっているのすばらしい ですねー。ずっと謎だった Slug の正体が分かってよかったですプロダクション環境での Heroku 運用、勉強になりました まだまだ小さなサービスですからね。これから大きくなると、問題がたくさん出てきそうですなんか push したら動かしてくれる Heroku が裏で何やってるか知れてよかったです！ 最近便利な機能知って Heroku いいなーってなってます！ ありがとうございます。他にも Heroku CI や Promoting など、まだ試せていない機能がありますねー資料が分かりやすいです 安心安全な資料作りを心がけて参ります資料の文字数がちょうど良くてとても見やすいと思いました 本当はもっと減らしたいのですが、あとから読める資料としても作ったので、今回の長さになりましたボトルネックがないようなら Heroku 便利そう とは言え、パフォーマンスを突き詰めると厳しいと感じています。現在は割とバックグラウンド＆バッチ処理にフォーカスしていますさすが弊社で Heroku に一番詳しい男！！ 引き続きニッチな領域を攻めていきますマスタカさんは説明が上手だなと思いました（こなみ） ありがとうございます！Heroku..... なんだかんだ、難しいイメージがあります... 私も最初はそうでしたねー身近に仕事で Heroku 使ってる人だー いえす！初心者でも使えるけど奥が深い Heroku ですねー発表の感想の感想Heroku はバックエンドエンジニアには概ね好評ですが、フロントエンドエンジニアには割と不評な印象を受けました。HTTP/3 という話が出てきた今、未だに HTTP/1.1 なのはだいぶ遅れてますし、パフォーマンスを突き詰めると、どうしてもレイテンシー問題に直面してしまいます。逆に言えば、この辺どうにかなれば最強だと思うのですが。🌀Heroku の先を見据えてやっていきます。💪]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[CircleCI Orbs 入門]]></title>
        <id>https://blog.tsub.me/post/introducing-to-circleci-orbs/</id>
        <link href="https://blog.tsub.me/post/introducing-to-circleci-orbs/"/>
        <updated>2018-11-10T08:47:00.000Z</updated>
        <summary type="html"><![CDATA[とうとう待望の CircleCI Orbs がリリースされたので一通り触ってみました。Announcing CircleCI Orbs and our new Technology Partner Program今回作ったサンプルは以下のリポジトリにありますので手っ取り早く知りたい人は以下のコードを見ると良いかと思います。 tsub/circleci-orbs-sandbox]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Albert で GitHub リポジトリを開ける拡張を作った]]></title>
        <id>https://blog.tsub.me/post/create-albert-github/</id>
        <link href="https://blog.tsub.me/post/create-albert-github/"/>
        <updated>2018-10-28T09:35:00.000Z</updated>
        <summary type="html"><![CDATA[先日プライベートの開発マシンを Linux にしたのですが、macOS の時に一番重宝していたものがなにかというと、実は Alfred だったことに気づきました。Alfred がないとストレスフルです。ただ Linux には Alternative Alfred がいくつかあり、その中でも Albert が比較的良さそうだったので Albert を使っていますが、Alfred で言う Workflow にあたるものが全然充実していませんでした。特に Alfred から GitHub を開く操作が一番多い気がするので、まずはそれを Albert でもできるようにするために、今回拡張を作りました。 tsub/albert-github: Open GitHub repository in browser with Albert]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[弊社の新人エンジニア研修カリキュラムを惜しみなく公開してみる]]></title>
        <id>https://developer.feedforce.jp/entry/2018/10/18/140000</id>
        <link href="https://developer.feedforce.jp/entry/2018/10/18/140000"/>
        <updated>2018-10-18T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは！今年の4月にポテンシャル枠で入社した id:daido1976 です！引き続き Rails に弄ばれる日々を過ごしています。さて、今回は約4ヶ月間の新人エンジニア研修を受け終えた私が弊社の研修カリキュラムを公開し、まとめや実際に受けてみての感想を書いていきたいと思います。前提としてフィードフォースでは今年4月〜5月のほぼ同時期に e-Navigator というプログラムを通じて、私を含む3名の実務未経験エンジニアが入社しています。今回の研修は、e-Navigator でもレビュアーだった @sukechannnn がメンターとして上記3名の新人エンジニアをフォローする体制で進めました！研修の成果を3行で入社時に「プログラミング歴3ヶ月の超初心者エンジニア」だった私がフィードフォースで約4ヶ月間の新人エンジニア研修を受けて配属後にある程度自走してコードが書けるぐらいのスキルを身につけることが出来ました！ざっくりスケジュール4月プロを目指す人のためのRuby入門 言語仕様からテスト駆動開発・デバッグ技法まで の例題を解く & 読むわかばちゃんと学ぶ Git使い方入門 を読むVim 研修（vimtutor を2周する）5月〜6月パーフェクトRuby on Rails の1章〜5章を読む & 6章〜7章のアプリ作成メタプログラミングRuby 第2版 を読むWebを支える技術 を読む社内スクラム研修7月ローテーション配属研修（2チーム）シンプルな Todo アプリの API 開発社内フロントエンド研修社内インフラ研修各研修についてのまとめや感想『プロを目指す人のためのRuby入門 言語仕様からテスト駆動開発・デバッグ技法まで』の例題を解く & 読む所要期間: 約1ヶ月表紙の 「Rails をやる前に、 Ruby を知ろう」 の通り、Rails での開発を意識しながらピュアな Ruby の基礎を学び直せる良書。クラスやモジュールはふんわり知ってる程度でしたが、この本を読んで初めて自分で定義しました。各章の例題を GitHub の PR ベースでレビューしてもらったり、本書内の Minitest を RSpec に変えてテスト書いたり、 RuboCop 導入したり、とメンターである @sukechannnn のファインプレーが光りました。プロを目指す人のためのRuby入門 言語仕様からテスト駆動開発・デバッグ技法まで (Software Design plusシリーズ)作者: 伊藤淳一出版社/メーカー: 技術評論社発売日: 2017/11/25メディア: 大型本この商品を含むブログを見る『わかばちゃんと学ぶ Git使い方入門』 を読む所要期間: 約3日入門書の名の通り、Git の概要を掴むにはよかったです。ただ、研修としてはもう少し SourceTree に特化していない内容も知れるといいなと思いました。わかばちゃんと学ぶ Git使い方入門〈GitHub、Bitbucket、SourceTree〉作者: 湊川あい,DQNEO出版社/メーカー: シーアンドアール研究所発売日: 2017/04/21メディア: 単行本（ソフトカバー）この商品を含むブログ (3件) を見るVim 研修（vimtutor を2周する）所要期間: 約2日vimtutor がナイス教材過ぎました…（しかも無料 & インストール不要）Vim への恐怖心がなくなったのはだいぶ大きかったです。（vimtutor やるまでは Vim 怖かった…）『パーフェクトRuby on Rails』の1章〜5章を読む & 6章〜7章のアプリ作成所要期間: 約1.5ヶ月所要期間的に今回の新人エンジニア研修のメインだったように思います、 rails new のところからがっつりレビューしてもらい、ログイン、CRUD、多対多の関連、E2E のテストまで、一通りの Rails アプリケーション開発を学べました。プロダクトのコードはほぼ写経だったこともあり、RSpec でいい感じのテストを書くことに一番頭を使いました。このタイミングで当たり前にテストを書く習慣が身についたのはとても良かったです。パーフェクト Ruby on Rails作者: すがわらまさのり,前島真一,近藤宇智朗,橋立友宏出版社/メーカー: 技術評論社発売日: 2014/06/06メディア: 大型本この商品を含むブログ (8件) を見る『メタプログラミングRuby 第2版』 を読む所要期間: 約2週間メタプログラミングについて学ぶというよりも、基礎を学び終えた初学者がより深く Ruby 内部の構造について学ぶ書という意味合いが強かったような気がします。オブジェクトモデル、メソッド、ブロック、スコープ、クラス & モジュール定義について学び、この頃から irb や pry 上で出てくる Ruby 組み込みのエラーメッセージが怖くなくなりました。あと読書感想会は本当にやってよかったです。新人エンジニア達がメタプログラミング Ruby の読書感想会をやった話メタプログラミングRuby 第2版作者: Paolo Perrotta,角征典出版社/メーカー: オライリージャパン発売日: 2015/10/10メディア: 大型本この商品を含むブログ (3件) を見る『Webを支える技術』 を読む所要期間: 約1週間実は入社前に一度読んだことがあったのですが、『パーフェクトRuby on Rails』のアプリ作成と並行して進めることで、より理解が深まりました。2部の URI と 3部の HTTP がものすごく勉強になりました。特に HTTP のところは RSpec や cURL で値をいじって、返ってくる値も覗きながら読むと、ものすごく捗りました。Webを支える技術 -HTTP、URI、HTML、そしてREST (WEB+DB PRESS plus)作者: 山本陽平出版社/メーカー: 技術評論社発売日: 2010/04/08メディア: 単行本（ソフトカバー）購入: 143人 クリック: 4,320回この商品を含むブログ (183件) を見る社内スクラム研修所要期間: 約0.5日社内随一のアジャイルお兄さん id:pokotyamu によるスクラム研修。座学パート1時間弱、レゴスクラムによる実践パート2時間強の研修で、スクラムの「守」の部分をがっつり学ぶことができました！一緒に研修を受けた人事インターンの方が感想記事を書いてくれています。media.feedforce.jpローテーション配属研修（2チーム）所要期間: 各1週間ずつ各チームに1週間ずつ、お試し配属のような形で実際のプロダクトのコードを触りながら、軽めの改修や新機能の一部を実装しました！当時の雰囲気は以下の記事をご参照ください。developer.feedforce.jpシンプルな Todo アプリの API 開発所要期間: 約1週間@sukechannnn が鬼のようなスピードで Swagger で書いてくれた仕様書をもとにゼロから Rails の API サーバを作りました。後述するフロントエンド研修、インフラ研修と併せて、API の実装〜React で SPA 作成〜AWS でデプロイまで一貫して経験することで、イマドキの Web アプリケーション開発の全体像を掴むことが出来ました。超シンプルなアプリケーションでしたが、『パーフェクトRuby on Rails』がほぼ写経だったこともあり、自分の頭を使って開発を進められて楽しかったです。社内フロントエンド研修所要期間: 約0.5日id:hano_tea に基礎編、id:otofu-square に実践編を担当していただきました！基礎編では SPA 作成に必要最低限の ES6、JSX の書き方を教えていただき、実践編では自作した Todo アプリ API を使った SPA を React で実装しました。React を触るのは初めてで JS もほとんど書いたことがなかったので、途中ちょっと取り残されそうになりましたが、なんとか食らいついて完成まで持っていくことができました！社内インフラ研修所要期間: 約0.5日『Amazon Web Services 基礎からのネットワーク&サーバー構築』 を教本に自分で開発した Todo アプリ API を手動デプロイしてみようという研修です。事前の知識として EC2 と S3 ぐらいは聞いたことあったものの、VPC？Internet Gateway？Route Table？みたいな状態でした…が、諸先輩方に一つ一つ丁寧に教えていただきデプロイまで持っていくことができました。翌日 React で実装した SPA も Heroku にデプロイし、Web の世界に自作アプリが羽ばたいた時にはとても感動しました…！Amazon Web Services 基礎からのネットワーク&サーバー構築 改訂版作者: 玉川憲,片山暁雄,今井雄太,大澤文孝出版社/メーカー: 日経BP発売日: 2017/04/13メディア: 単行本この商品を含むブログを見るそして研修は続く…上記の研修に加えて、チーム配属後の研修図書として以下の本をプレゼントしていただきました！リーダブルコード ―より良いコードを書くためのシンプルで実践的なテクニック (Theory in practice)作者: Dustin Boswell,Trevor Foucher,須藤功平,角征典出版社/メーカー: オライリージャパン発売日: 2012/06/23メディア: 単行本（ソフトカバー）購入: 68人 クリック: 1,802回この商品を含むブログ (140件) を見るオブジェクト指向設計実践ガイド ~Rubyでわかる 進化しつづける柔軟なアプリケーションの育て方作者: Sandi Metz,?山泰基出版社/メーカー: 技術評論社発売日: 2016/09/02メディア: 大型本この商品を含むブログ (6件) を見るSQL 第2版 ゼロからはじめるデータベース操作 (プログラミング学習シリーズ)作者: ミック出版社/メーカー: 翔泳社発売日: 2016/06/17メディア: 単行本（ソフトカバー）この商品を含むブログを見るふつうのLinuxプログラミング 第2版 Linuxの仕組みから学べるgccプログラミングの王道作者: 青木峰郎出版社/メーカー: SBクリエイティブ発売日: 2017/09/22メディア: 単行本この商品を含むブログを見る体系的に学ぶ 安全なWebアプリケーションの作り方 第2版 脆弱性が生まれる原理と対策の実践作者: 徳丸浩出版社/メーカー: SBクリエイティブ発売日: 2018/06/21メディア: 単行本この商品を含むブログを見るちなみに私は『リーダブルコード』『オブジェクト指向設計実践ガイド』『SQL ゼロからはじめるデータベース操作』を読み終えて、今は『ふつうのLinuxプログラミング』を読んでいるところです。『オブジェクト指向設計実践ガイド』についてはとても良い本だったので、また社内で読書会を開催しようと思っています！最後に今回は弊社の新人エンジニア研修カリキュラムをご紹介しました！来年以降の新人エンジニア研修には自分自身が積極的に関わっていきたいという考えもあり、カリキュラムをよりブラッシュアップさせていくためにも記事としてまとめ、研修の振り返りを行いました。この記事をきっかけに、もっとこうした方がいい、うちの会社ではこんなことやっている、みたいな情報が集まるといいなと思っています。それでは、またお会いしましょう ＾＾]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[社内勉強会で『Markdown と学ぶ HTML 基礎』を発表しました]]></title>
        <id>https://developer.feedforce.jp/entry/2018/10/12/184014</id>
        <link href="https://developer.feedforce.jp/entry/2018/10/12/184014"/>
        <updated>2018-10-12T09:40:14.000Z</updated>
        <summary type="html"><![CDATA[虚構集めてますか？こんにちは、白魔の id:tmd45です。社内勉強会の担当で『Markdown と学ぶ HTML 基礎』という発表をしました。タイトルの通りごくごく基本的な内容ですが、ちょっとした部分で「知らなかった！」みたいなのが結構あったみたいで盛り上がりましたv(￣Д￣)vspeakerdeck.comスライドはほぼ初学者向けに書いていますが、これは 8 月末に開催した e-Navigator 勉強会#4 向けに作成したスライドの焼き増し（第二版）だからだったりします。一部 HTML の間違いを修正し、Markdown parser や linter の情報を追記しております。以下、FFTT 恒例の感想付せん返信です。感想付せん返信Markdown にまだ慣れてない感があるので警察こわい警察👮こわいこわくないよ〜 勉強会で発散したから しばらく こわくないよ〜ぜひこれを機会に文章構造と HTML を意識してみてください！HTML 使いこなすためにはまず文章構造をですね…それな勉強になりました！捕まらないように気をつけます…(ΦωΦ)ﾌﾌﾌ…ul タグと ol タグの意味を理解できましたやったね！はじめてプログラミングに触れた日、ドットインストールで HTML から始めた時を思い出しました！(　；∀；) ｲｲﾊﾅｼﾀﾞﾅｰtmd45 さんが楽しそうに話されていて楽しかったです！！照れる〜 🤑WHATWG HTML Living Standard もよろしくオネシャス！これは知らなかった！へ〜HTML5 や HTML 5.1 は W3C が標準化を進めていますが、W3C とは別に、Apple, Chrome, Opera が設立した WHATWG という団体が独自に策定を進めている HTML 仕様です。HTML Living Standard と呼ばれ、バージョン番号や、何年何月何日版という概念もなく、毎日改版、修正、強化が進められています。HTML Living Standard - とほほのWWW入門奥深き Markdown …GitHub Flavor とか方言や拡張がいろいろあるので、あっちでは上手く書けたのがこっちでは崩れるとか文章構造をシンプルにすれば、あまり悩まなくて済んだりもします 😇リストに - [ ] を混ぜるのによく困ってますねこれも HTML がどうなって「意図どおりでない」表示なってしまうのか把握できるといいですね 😇もしかしたら、インデントのスペースの数が合ってないとかも影響してるかも？基礎的な内容だと思ったけど知らないこともあって良かった！！そう言ってもらえるとありがたいです！😊総合職向け勉強会で話してもいい内容だと思いましたたしかに 🦀リストの中の複数行コードという積年の悩みが消えました。ありがとうございます！この質問に答えてくれたのは id:masutaka26 ですけどこれもやっぱり Markdown の拡張と、文章構造／HTML の話の絡み合いですねーMarkdown のリンク記法はめんどいから Alfred Workflow に頑張ってもらってます私は Chrome 拡張の Create Link を使ってます！はてな記法に触れるやさしさやさしさなんだろうか（笑）Markdown が台頭してくる前は、世の中の文章構造は全部はてな記法で書ければいいのにと思ってた時期もありました😏font color を red にするにはどうしたらよいですか？( ＾ω＾) おっ？マジレスすると「文字を赤色にする」のは文章構造ではないので、たとえば「強調」の構造に対して Style（CSS）に color: red を定義するなど、文章構造とスタイルは分けるほうが良いですね 😇気づいたら社内の派閥投票がされていた！私は「Shift キー押さなくて済むので - リスト派」です。CommonMark（0.28）の仕様でも、どちらかに制限されているわけではないので、どちらがジャスティスってことも無いみたいですね。勉強会後の雑談で「 - リストを書いてる途中で、リアルタイムプレビューがせっかちで一瞬 見出し 記法（レベル 2 見出し）として扱われる」ことに気がついて * リストに宗旨変えしてる人がいました！変換される HTML（というか文章構造）が把握できるとそんな気づきもあったりします！ぜひみなさんも Markdown から HTML、そして文章構造について知ってみてくださいね。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[terraform-provider-healthchecksio に足りない機能を実装したらコラボレータになれた]]></title>
        <id>https://developer.feedforce.jp/entry/2018/10/09/140000</id>
        <link href="https://developer.feedforce.jp/entry/2018/10/09/140000"/>
        <updated>2018-10-09T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、増田（id:masutaka26）です。秋は体が急に冷え、一年で一番体調を崩しやすいので、意識して風呂に浸かっています。元気です。id:critical_alert から https://healthchecks.io/ を教えてもらいました。cron 系の定期実行ジョブが本当に実行されたかを監視してくれるサービスです。以前、sidekiq-cron のバグで定期実行ジョブが実行されないことがあったので、なんらかの方法でお手軽に監視したいモチベーションがありました。terraform-provider-healthchecksio を発見コード化したいなーと思って調べたら、非公式の Terraform provider を発見。kristofferahl/terraform-provider-healthchecksioこういった設定は属人化しやすいため、Terraform でコード化できるのは助かります。ただ、この時点では schedule や timezone などに対応していなかったため、採用は断念しました。healthchecks.io 上でポチポチと監視用のエンドポイントを作成し、ビジネスロジックにも ping するコードを埋め込み、いい感じに動作していました。実装開始悶々とした日を送り続けたある日、調べてみたら意外と簡単に機能が実装できそうなことが分かりました。👇 それで送ったのがこの PR。可能な限りのパラメータを使えるようになりました。Add parameters by masutaka · Pull Request #1terraform import もしたいなーと調べたら、たったの 3 行で対応出来ることが判明。👇 2 つ目の PR を出して無事マージされました。Introduce terraform import by masutaka · Pull Request #2ついでにしれっとコラボレータに招待してくれて、commit 権を頂けました。やったぜ！👇 先ほど 3 つ目の PR を出しました。非公式の Terraform provider なので、自分でバイナリを設置する必要があるのですよね。参考になりそうな Makefile を作りました。Add examples/Makefile by masutaka · Pull Request #3今後の予定The database failover process is manual, and the ops team consists of a single person, so multi-hour or even multi-day outages are possible!While we can only guarantee a best effort availability, in practice the availability has exceeded 99.9% since the service publicly launched in July 2015.https://healthchecks.io/about/healthchecks.io は安定稼働しており、2015 年 7 月のサービス開始以来、稼働率は 99.9% を超えているそうです。ただあくまでベストエフォート型の可用性しか保証していないので、過信は禁物です。いわゆる 1 人開発プロジェクトでもあります。今後は Datadog で同じ監視が出来るか調査して、healthchecks.io と併用したいなーと思っています。Cronitor も頭の片隅にあるけど、少しオーバースペック感があります。おまけhealthchecks.io は OSS でもあり、GitHub 上にコードが公開されています。Django で実装されています。healthchecks/healthchecks]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go 1.11 の Modules (vgo) を CircleCI で使う]]></title>
        <id>https://blog.tsub.me/post/go111-modules-in-circleci/</id>
        <link href="https://blog.tsub.me/post/go111-modules-in-circleci/"/>
        <updated>2018-08-30T05:33:00.000Z</updated>
        <summary type="html"><![CDATA[個人プロジェクトにて、先日リリースされた Go 1.11 の Modules (vgo) を使ってみました。移行自体はスムーズにできたのですが、CircleCI でのキャッシュのやり方がそこそこ重要かも？と思ったので記事を書きました。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes.rb に講師役として参加してきました]]></title>
        <id>https://developer.feedforce.jp/entry/2018/07/23/140133</id>
        <link href="https://developer.feedforce.jp/entry/2018/07/23/140133"/>
        <updated>2018-07-23T05:01:33.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、エンジニアの id:tsub511 です。先日 Kubernetes.rb という勉強会があり、そちらの講師役として参加してきました。localhost.connpass.com.rb と言いつつ Ruby の話は一切ありませんでした。タイトルの伏線は回収されず 😁参加の経緯さて、今回自分としては初の勉強会の主催側 (?) としてお手伝いすることとなったのですが、その経緯について軽くご紹介します。もともと一からイベントを企画したわけではなく、主催の @yoshi_hirano さんが講師役を募集していたところに応募した形になります。ただ、応募の経緯としては先日ご退職された元フィードフォースの @284km さんから、「講師役やってくれる人を1名探しているんですが、tsub 氏どうですか？」というお誘いを貰い、やってみたいと思ったので繋いでいただいた感じです。ちなみに社内で Kubernetes について勉強していくぞ！！的なチャンネルが最近できました。実際の準備やると決まってからは当日まで 1 ヶ月半ぐらいあったのですが、そこからは Twitter でグループ DM しながら準備を進めていきました。とはいえ、お互い顔も分からず会ったことのない中で Twitter の DM オンリーで準備を進めていったので少々不安を感じながらも、@yoshi_hirano さんやサポート役の @katsuhisa__ さんから優しくして頂けたので問題なく進められました。準備に際しては以下のリポジトリのコミット権を貰い、そこにサンプルを自分が足していきました。github.comまた、Rails のサンプルアプリについては以下のリポジトリも用意してもらいました。github.comあとは当日までひたすら YAML を書く... 😇 という感じになります。最初に作った Rails 用の YAML には結構時間がかかったものの、残りのアプリはほとんどコピペでサクサク進んでいきました。ただ、Sentry などは自分で動かしたことがなかったので、Kubernetes で動かすというよりはアプリそのもののセットアップ手順や構成などを理解するのに時間がかかったように思います。また、Discourse と GitLab については Docker イメージの使い方が独特で、Discourse の方は独自のシェルスクリプトを使っていて読み解くのがが大変そうで、GitLab の方はコンテナを動かしたら Chef が動き始めて色々インストールしだしたので諦めました。代わりに Mastodon を動かすことになりました。動かし始めたらいけそうだったので、勢いで当日の朝も準備をしてました..勉強会当日の様子会場は株式会社万葉さんのオフィスをお借りしました (自分がその辺りを手配したわけではないです)。とても快適でした。万葉さんありがとうございました 🤗特に、Chrome Cast に繋がったプロジェクターが設置してあり、各自何か言いたいことがある時にサクッと画面共有できて良かったかと思います。全体としては 12:30 ぐらいからゆるっと始まり、17:30 ぐらいに解散しました。最初に @yoshi_hirano さんから流れの説明があり、各自簡単に自己紹介をした後はそれぞれ資料を見ながらもくもくやっていました。ただ、最初に Minikube で躓く人が多かったようです。以下の Issue を参考に、最終的に $ minikube start --vm-driver=hyperkit --bootstrapper=localkube で動いたようです。github.comちなみに自分は $ minikube start --vm-driver=hyperkit だけで動きました。Minikube が動かないので GKE で進める方も多かったようです (サンプルは Minikube と GKE 両方の手順を用意していました)。後は、イメージの Pull や DB のマイグレーションジョブの実行など、待ち時間が多かったためか、それなりにわいわい話しながら皆で進めてました。自分は講師役という立ち位置でしたが、感覚的にはどちらかというと大学の講義で手伝いをしていた感じです。質問があったら近くに行って答えるのを繰り返しつつ、何もない時は Mastodon の GKE 版のサンプル資料を作っていました。最終的には Mastodon まで動かせた方も多く、サンプルを用意した自分としては非常に嬉しかったです ✨また、最後に KPT 方式の振り返りをやったのですが、Trello を使ったやり方が個人的にはすごく良かったです。会社でもやってみたいなと思いました。振り返りの様子。なお、どうやら弊社で月一でやっているもくもく会がたまたま同日開催で場所も神保町と、会場のすぐ側でやっていたようです。id:masutaka26 がひっそりとリモートで参加していました 😎02_nginx でこんなエラーが出た #localhost9292$ kubectl apply -f k8s/deployment.yamlError from server (BadRequest): error when creating "k8s/deployment.yaml": Deployment in version "v1" cannot be handled as a Deployment: no kind "Deployment" is registered for version "apps/v1"— Takashi Masuda (@masutaka) 2018年7月21日minikube のアップデートで直った！— Takashi Masuda (@masutaka) 2018年7月21日資料の補足KPT の P にも上がっていましたが、途中実行待ちが長いときに「これは上手くいっていて単純に時間のかかる処理なのか、そもそも上手く動いていないのか」というお声を頂きました。それについてはログを見る方法についても明示しておけば良かったと思っています。Kubernetes でログを見るには $ kubectl logs コマンドを使います。$ kubectl logs -f <Pod Name>(tail と同じように -f でストリーミングができます)Job の実行時などにはログを見ながら今何が動いているのかを見るとより分かりやすかったと思います。また、今回は Pod, Deployment, Service などの概念についての説明をせずにとりあえず手を動かしてみるという会でしたが、その辺りについては Kubernetes.rb #2 が開催されるようなので、興味のある方はぜひご参加ください！ (自分は次回は参加しないですが 🙇)localhost.connpass.comやってみた感想など今回初めて勉強会の主催側として参加したわけですが、感想としてはやって良かった！！と思っています。そもそも Kubernetes についてはまだ仕事で使っているわけでもなく導入の検討段階ですし、個人で趣味レベルで動かした程度だったので今回の資料作成の中でかなり理解が深まったと思っています。また、メンタル的にも成長できた気がします。会ったことのない社外の人たちがいる場に飛び込むのは個人的にはなかなかハードルが高く、今まで勉強会に参加する際は懇親会にはあまり出ないタイプだったのですが、今回の体験で「あ、なんだ。こんな感じなのか」みたいな感覚を得られたので今後は懇親会などに参加して社外の人とも交流する勇気が少し出ました。それでは最後に改めて、@yoshi_hirano さんや @katsuhisa__ さん、会場を提供してくださった万葉さん、参加してくださった皆様、ありがとうございました  👋]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[新人エンジニア達がメタプログラミング Ruby の読書感想会をやった話]]></title>
        <id>https://developer.feedforce.jp/entry/2018/07/19/000000</id>
        <link href="https://developer.feedforce.jp/entry/2018/07/19/000000"/>
        <updated>2018-07-18T15:00:00.000Z</updated>
        <summary type="html"><![CDATA[はじめまして！今年の4月にフィードフォースに入社した id:daido1976 です！Rails に弄ばれる日々を過ごしています。フィードフォースでは今年の4月と5月に私を含む計3名の新人エンジニア（開発実務未経験者）が入社をし、現在研修を受けている真っ最中です。研修の一環として、『メタプログラミングRuby』を読んでいたのですが、初心者にはかなり難しい内容の本ということもあり、各自きちんと理解ができているかどうかチェックする意味も込めて社内で読書感想会を開催しました！メタプログラミングRuby 第2版作者: Paolo Perrotta,角征典出版社/メーカー: オライリージャパン発売日: 2015/10/10メディア: 大型本この商品を含むブログ (3件) を見るもちろん新人エンジニア達にとって技術書の読書感想会など初めてなので、過去社内で行われていた以下の記事を参考に。developer.feedforce.jpで、どうだったの？結論めっちゃよかったです。今回の進め方まず、1日でやるには文量的に難しいと判断し、5日に分けて毎日1時間、1章ずつ進めていくことに。（それでも時間をオーバーしてしまうことが稀によくありました）当日は以下のような進め方をしました。事前に書いてきた付箋を全員で貼るカテゴライズする各自書いてきた付箋の内容を説明していく議題の優先度を決めるディスカッションするまとめをする『メタプログラミングRuby』読書感想会のホワイトボード個人的に一番盛り上がった3章（メソッド）のホワイトボード良かった点理解が曖昧な部分が明確になった本を読んで自分では理解できたと思っていても、それを言語化して他人に伝えようとするといかに理解できていないかわかるもの。今回の読書感想会でもまさにその経験ができ、自分が曖昧に理解していた部分が明確になり、今後どこを学んでいくべきかをはっきりさせられたように思います。他メンバーの感想や見解を聞けた新人エンジニアとはいえ数ヶ月の研修を経て、それぞれの考え方や興味を持つポイントにも個性が出始めてきています。同じ『メタプログラミングRuby』という本を読んでも、Ruby の内部処理に興味津々なメンバー、用語の意味に執着するメンバー（私です）、具体的なコード例を知りたがるメンバー、などそれぞれ違った観点の感想や見解が出てきてとても面白かったです。先輩エンジニアの方々に知見を共有してもらえた読書感想会を開催する前週に社内向けに「ご興味ある方は覗きに来てください」と呼びかけたところ、当日先輩エンジニアの方々が参加してくださり、我々新人エンジニア達の疑問点を図や具体的なコード例を用いてどんどん解消してくれました。正直これが一番のメリットだったように思います。Qiita:Team で呼びかけてる様子反省点最初の頃はだいぶ時間をオーバーしてしまった（ちなみに初回は1時間予定のところ、2.5時間もかかってしまいました…）本を読んでから読書感想会を開催するまで少し期間が空いてしまい、読んだ内容をすっかり忘れてしまっていた（主に私の話ですが…）ただ、基本的には良いことずくめの会だと感じたので、今後も機会があれば積極的に開催していきたいと思います！最後に今回は入社したばかりの新人エンジニア達が主体となって開催された『メタプログラミングRuby』読書感想会のご紹介をしました！最後に私が本書を読んで一番感銘を受けた一文を引用して終わりたいと思います。「（前略）メタプログラミングというものは存在しない。すべてはただのプログラミングじゃ。（後略）」p. 171それでは、またお会いしましょう ＾＾新人エンジニア達のメタプロRuby]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[10 分でわかる Ruby Guild]]></title>
        <id>https://developer.feedforce.jp/entry/2018/06/25/100000</id>
        <link href="https://developer.feedforce.jp/entry/2018/06/25/100000"/>
        <updated>2018-06-25T01:00:00.000Z</updated>
        <summary type="html"><![CDATA[どうも、バックエンドエンジニアのサトウリョウスケです ✌︎('ω')✌︎RubyKaigi 2018 から早 3 週間。この記事を読んでいる方でも参加された方が沢山いるのではないかと思います。個人的な感想ですが、今年は例年以上に充実していたんじゃないかな、と大満足です✨感想記事はこちらの記事に詳しくまとめられています。developer.feedforce.jpさて、今回の RubyKaigi の発表では笹田さんから Guild の進捗についての発表がありました。この Guild の概要について社内のメンバーに解説したところ中々に好評だったので、今回の記事では Guild とはどういうものなのかを超ざっくりですがご紹介します。そもそも論。なぜ Guild が必要なのか？Guild と Thread の関係Guild 間のデータの受け渡しおわりにそもそも論。なぜ Guild が必要なのか？ご存知の方も多いかと思いますが、現在の Ruby のスレッド処理はマルチコアに対応していません。最近の PC の殆どにはマルチコア CPU が搭載されていると思いますが、１つの Ruby プロセスが利用できるコアは１つだけです。つまり、並列処理と言っても複数の処理を時間分割して実行しているだけという事になります。 Guild は Ruby でマルチコアを使った並列処理を実現するために必要な機能となります。ちなみに Guild という名称はあくまでコードネームであるため、リリース時には別の名称になると思われます。Guild と Thread の関係上図のように Guild に Thread が内包される関係になり、プロセス全体でいうと RubyVM → Guild → Thread → Fiber という関係になります。これを踏まえて、 Guild 導入の前後を図に起こすと以下のようになります（雑な図で恐縮です。。🙇）同一 Guild 内の Thread は従来どおりシングルコアで動作します。しかし、 Guild を複数定義することで、それぞれの Guild に所属する Thread は並列に動作するようになります。Guild が実装されていない現在の Ruby は RubyVM → Thread → Fiber という関係になりますが、言い換えればこれは単一の Guild で作られた Ruby プログラムとみなせると思います。このことから、Guild が実装されても後方互換性は保たれるのではないか予想されます。Guild 間のデータの受け渡しGuild 間でのデータ受け渡しについて、 Shareable Object と Non-Shareable Object という概念が出てきます。詳細は 発表スライド を見てもらうとして、大雑把に両者の違いを説明すると以下のようになります。Shareable ObjectGuild 間でデータ共有が可能なオブジェクト基本的に値が変化しない Immutable なデータがこれに該当するConst や Freeze したデータのことただし Array や Hash は Freeze しても内側のデータまで Freeze されないので、 Shareable とはならないので注意これについては Deep Freeze を実装するかも、とのこと。他にも Isolated Proc という、ブロックの外側の変数へのアクセスを禁止した Proc もこれに該当しますが、詳細は割愛Non-Shareable ObjectGuild 間でデータ共有が禁止されているオブジェクトデータの受け渡しができない訳ではない（詳しくは後述）主に一般的な変数のこと単一の Guild にのみ所属するShareable Object は Immutable (値が変化しない)オブジェクトなので、並列処理中にデータを共有しても問題ないことはイメージしやすいかと思います。重要なのは Non-Shareable Object の受け渡しで、以下に示す 2 つの方法があります。COPYデータを Guild 間で共有するのではなく、別の Guild にコピーして渡すMOVE別の Guild にデータを渡すと、元の Guild からは見えなくなるCOPY は参照渡しではなく実体渡しなので、１つのデータに対して同時アクセスすることにはならず、 Thread-Safe なやりとりになります。一方の MOVE は、データを別の Guild に渡すと元の Guild からはアクセスできなくなる、カットアンドペーストのような振る舞いになります。こちらも同時アクセスが発生しないため、 Thread-Safe なやりとりになります。ちなみに MOVE より COPY が利用されるケースが多いだろうとのことでした。おわりにRubyKaigi 2018 で発表された Guild について超ざっくりと説明してみました。間違ってるところがあればご指摘頂けますと幸いです🙏なお、開発中の仕様なので実装までにいくつかの変更があるかもしれません。想像ですが、 Sidekiq とか Puma のような Thread ベースのサービスは Guild が導入されたら劇的にパフォーンスが良くなるかもしれませんね。これからの Ruby の可能性に期待が高まります✨]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[先期の MVP に選ばれた]]></title>
        <id>https://developer.feedforce.jp/entry/2018/06/15/130000</id>
        <link href="https://developer.feedforce.jp/entry/2018/06/15/130000"/>
        <updated>2018-06-15T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[お元気にしてますか。増田（id:masutaka26）です。先週の金曜日にフィードフォースにおける 2017 年度（2017年6月 〜 2018年5月）の納会が行われ、その中で初の MVP を受賞することができました。🎉喜びのコメントを表明中のしがないますたかチーム賞ももらったヨ会社で賞を頂いたのは 4 年前のルーキー賞以来です。masutaka.net四年寝太郎としては、取るべくして取った賞だと思いました（ｷﾘｯ正直なところ、頑張ったで賞くらいはもらえたりして？とボンヤリ思っていましたが*1、まさか MVP とは思いもよらず、発表の瞬間は素で驚きました。2017 年度は以下のチームに所属していました。2017 年 6 月 〜 2018 年 3 月DF PLUS（社内では DF Maker と呼ばれる）2018 年 4 月 〜 2018 年 5 月EC BoosterDF Maker ではサービスの信頼性向上にひたすら努める毎日でした。ECS でとんでもない量のエラーが発生して、３連休の間は暇を見つけてエラーログを確認してたなー（遠い目）。EC Booster ではチームビルディングから始めました。👇️以下の記事にまとまっています。developer.feedforce.jpどちらにも共通することが、2 年目の優秀な若者と仕事ができたことです。私が若者を引っ張っていくぜーという感じではなくむしろ逆で、すでにサービスを知り尽くした若者から学ぶ日々でした（今もそうです）。その中で、「こうすればチーム開発がドライブするだろう」や「こうすればサービスの質を高められるだろう」というアイディアを出し、実践し、小さな改善と失敗を繰り返したことが、結果としてチームの状況を良くしたりサービスの質を高めることが出来たのだと思います（現在進行系）。もちろん、一緒に働くことができたチームの皆さんの協力があっての賞です。ありがとうございます。🙏それではまた 4 年後にお会いしましょう💤*1:そのような賞はありません]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[『開発基盤チームとして課題を探したらマネジメントをしていた』という発表をした]]></title>
        <id>https://developer.feedforce.jp/entry/2018/05/21/150000</id>
        <link href="https://developer.feedforce.jp/entry/2018/05/21/150000"/>
        <updated>2018-05-21T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。増田（id:masutaka26）です。発表が無事終わり、何かゲームでも買おうかなと思ってます。さて、フィードフォースでは毎週金曜日 16:00 から、社内技術勉強会を開催しています。当番制の持ち回りで技術に関連したことを発表します。先週金曜日は私の当番で『開発基盤チームとして課題を探したらマネジメントをしていた』という発表をしました。技術チーム以外の方もたくさん聞きに来てくれて、ありがたかったです。🙏発表の内容前回含めて、これまでの発表は全てコードが登場していました。今回のようなコードが登場しないマネジメントの話は、内容が拡散しがちで難しかったです。結局まとまりませんでした...。今回は敢えて後から読めるプレゼン資料にしました。なのでプレゼン資料としては冗長な書き方になっています。まとめるとこんな内容です。コミュニケーションの不確実性を解消することが、マネジメント（やチームビルディング）に必要なはずそのため自ら場に飛び込んで、情報の非対称性を解消していったこれからもソフトウェアを書くこと以外に課題解決の手段があればやっていくそして徐々にマネジメントにかける時間を減らし、コードを書く時間を増やしていく今思えば、話し忘れたことがポロポロと出てきます。そもそもチームメンバーは自走できる方々で、ホワイトボード整理の時も方針だけ話したら勝手に整理されていった私は方向性を決めただけチームに入りたてのときは少々オーバーリアクションを心がけ、徐々に普通に戻していく。何考えている人か分かってもらうことで、その後のコミュニケーションをスムーズにする完璧な発表などありませんね。発表の感想付箋紙に感想を書いてくれました。 は私からのコメントです。ジョハリの窓の紹介ありがとうございます。勉強になります！ 私も『エンジニアリング組織論への招待』で初めて知りました。数年前にプロダクトオーナーの学習をした以来ですから...自分はまだマネジメントからは程遠い存在なのですが、話しかけやすいオーラを出すことや、くだらないと思うことも質問することなどは、今すぐにでも実行出来るし、目指すことだと思いました。ありがとうございました！ はい、個々が意識することが自走するチームや、個々のリーダーシップに繋がると思います共感できる良い話でした あざす！初公開の画像が見られて良かったです お粗末さまでした...早くインフラ朝会にもどって来てください 今回の件が落ち着いたらインフラ朝会にもどりますよ。待っていてくださいね！必要な情報とそうでもない情報の整理はものすごい HP/MP を消費しそう そうですね...マネジメントの成果はすぐには結果に現れないかも？どの位の頻度で振り返るべきなのか？ イテレーションごとの振り返りで、チームを観察すれば良いです。マネジメントの成果かどうかは重要ではないと思います◯◯を支える技術 次回はコードの話をしよう...知識科学やり直さなきゃ・・・ 私は今回で終了ですwチームの問題点を発見して、それを解決する手段を選ばないスタンスがすごい もう必死ですよ（笑）「内心ではなく行動に注目する」は、認知行動療法などでも使われていますね（最近のトレンド？） へぇ〜なかなかボリュームある資料でしたが、作るのにどれくらいかかったのか気になります 先週土曜日の夜に草稿を作り「これでいけそう」という感触を得て、今週は平日の夜２日くらいで整理をしました。話す内容については一ヶ月くらい前から頭の中で考えたり、『エンジニアリング組織論への招待』を全部読んだりしてました最近疲弊してそうで心配でしたが、今日の発表で色々アウトプットしてもらって安心しました 気にかけて頂いてありがとうございます。これからも安心安全のマスタカをお届けして参りますうなずきすぎて首もげる ありがとうございます。参考になったことが各チームに還元されるとうれしいですマスタカさん自体はマネジメントよりコード書く方が好きなんでしょうか？ そうですね。引き篭もってコードを書きたいですますたかさんのおかげでチームがちゃんと回りはじめた気がします そう言って頂けるとうれしいです。今がスタート地点だと捉え、精進してまいります増田さんが参加されてから何かがスムーズになった実感があります（情報や知識だけでなく） うれしいですね。マネジメントは空気のような存在が良いと考えていますマネジメントってどうやるか？のテクニックの面もあるけど、誰がやるか？の面も大きい気がしてます そうですね。例えば入ったばかりの職場だと、信頼関係を築くことから始まると思います。今回は同じ会社ということと、過去に一緒に働いたメンバーがいたことに助けられましたEC Booster のこれからに期待しています！ 💪「場に飛び込む」「正しく伝わったと思わない」肝に銘じます！ よろしくお願いします！「エンジニアが発見した課題の解決フロー」読みたいです Slack で書いたとおり、割と普通のフローです。発表で話したエンジニア以外の方からの依頼と整合性を合わせてこうしました。まだ始めて間もなく 1 イテレーションも回してない状態ですがね... Issue を作ると EC Booster waffle.io の Planning カラムに入る 次の開発ミーティングで Planning カラムを空にする スプリントでは Sprint Backlog に集中するfeedforce だとマネジメントオンリーじゃなくて、開発+マネジメントが最強という話を昨日クリストフとした 最強かは分かりませんが、コードも分かったほうが捗ると思います仕事でもプライベートでも、コミュニティ内でオープンな発信をする人が多いと、円滑に物事が進みやすいなと考えられました。その逆もまた然りだったので...。かつ、人に伝えるときは具体的にするというのも大切かと思いました 他人は自分でないので、分かりやすく具体的なコミュニケーションが大事ですね え、誰？（困惑）最後にエンジニアリング組織論への招待 ~不確実性に向き合う思考と組織のリファクタリング作者: 広木大地出版社/メーカー: 技術評論社発売日: 2018/02/22メディア: 単行本（ソフトカバー）この商品を含むブログ (2件) を見る良かったぞ。「エンジニア組織論」ではないことに注目。プロダクトオーナーやエンジニア以外の職種の方にも是非読んでもらいたいです。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Datadog で dd-agent に root 権限を与えずにプロセスがオープンしているファイルディスクリプタ数のメトリクスを取得する]]></title>
        <id>https://developer.feedforce.jp/entry/2018/05/11/190000</id>
        <link href="https://developer.feedforce.jp/entry/2018/05/11/190000"/>
        <updated>2018-05-11T10:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、エンジニアの id:tsub511 です。ここ数日気温の寒暖差が凄いですね。昨日あまりにも寒すぎて一度しまった冬用の布団を引っ張りだしたら、また気温が上がってきたので片付けることになりそうです。最近、Datadog でプロセスがオープンしているファイルディスクリプタ数のメトリクスを取る必要があり、色々と考えた結果良い方法を思いついたため、今回ご紹介します。Datadog 標準の system.processes.open_file_descriptors メトリクスを取るには root 権限が必要Datadog では標準で、Process Check という機能を使うことで system.processes.open_file_descriptors メトリクスを取ることができます。https://docs.datadoghq.com/integrations/process/#metricsただし、説明文にも書いてある通り dd-agent ユーザーが実行したプロセスしかこのメトリクスを取得することが出来ません。そのため、例えば Rails アプリケーションを動かすために puma プロセスを dev ユーザーで動かしていた場合、以下のような設定を書いても system.processes.open_file_descriptors メトリクスを取得することが出来ません。init_config:instances:  - name: puma_worker    search_string: ["puma: cluster worker"]    exact_match: Falseこれは何故かというと、プロセスがオープンしているファイルディスクリプタ数を取得するためには /proc/<PID>/fd 以下にアクセスする必要があるためです。/proc/<PID>/fd ディレクトリはそのプロセスを実行したユーザーにしか read 権限がありません。$ ls -al /proc/1/fdls: cannot open directory /proc/1/fd: Permission denied$ sudo ls -al /proc/1 | grep fddr-x------.   2 root root 0 Apr 18 06:58 fddr-x------.   2 root root 0 May 11 06:12 fdinfoそのため、dd-agent はそのメトリクスを取得できないというわけです。ただし、dd-agent に root 権限を与えることで、閲覧は可能になります。公式ドキュメントではそのやり方が提示されていますが、セキュリティ的にリスクがあるため、推奨はされていません。docs.datadoghq.comさて、この記事の内容は dd-agent に root 権限を与えずに system.processes.open_file_descriptors メトリクスを取得するということでしたが、どうやれば良いのでしょう？DogStatsD を使うDatadog には DogStatsD という仕組みがあります。DogStatsD は任意のカスタムメトリクスを Datadog に送る方法の一つです。docs.datadoghq.com通常は以下のような言語毎のライブラリを公式が提供してくれているため、こちらを使うことで任意のカスタムメトリクスを送ることができます。github.comDogStatsD を通してメトリクスを送る際は、その送り側のプロセスは任意のユーザーで実行できます。そのため、上記の例にあったように dev ユーザーが puma プロセスを実行している場合は dev ユーザーで DogStatsD にメトリクスを送るプロセスを実行すれば、同じ dev ユーザーのため /proc/<PID>/fd への read 権限があります。思いついてみれば簡単なことでしたね。でもプログラミング言語で実装するのは面倒じゃない？少し本題とは反れますが、もう少しお手軽に DogStatsD にカスタムメトリクスを送りたいな、とも思います。そこで、調べてみたところ「DogStatsD には単純に専用のフォーマットで UDP パケットを送るだけで良い」ということを知りました。On Linux:vagrant@vagrant-ubuntu-14-04:~$ echo -n "custom_metric:60|g|#shell" >/dev/udp/localhost/8125orvagrant@vagrant-ubuntu-14-04:~$ echo -n "custom_metric:60|g|#shell" | nc -4u -w0 127.0.0.1 8125https://docs.datadoghq.com/developers/dogstatsd/#sending-metrics上記のように、DogStatsD のエンドポイントである localhost:8125 に custom_metric:60|g|#shell のようなフォーマットで UDP パケットを送ってやれば良いです。そのため、プロセスがオープンしているファイルディスクリプタ数のカスタムメトリクスを送るには、以下のコマンドを実行すれば良いです。$ echo -n "open_file_descriptors.puma_worker:$(ls /proc/$(pgrep -f -u dev 'puma: cluster worker' | head -1)/fd/ | wc -l):g" | nc -u -4 localhost 8125上記のコマンドを crontab などで毎分実行してやれば open_file_descriptors.puma_worker メトリクスを送ることができます。ただし、実際に本番で利用しているコマンドはそこまで単純ではなく、以下のようなシェルスクリプトを書いて実行しています。#!/bin/shif [ $# -ne 2 ]; then  echo "Require 2 arguments" 1>&2  exit 1fiPROCESS_NAME=$1USER=$2# pgrep でシェルスクリプト自身のプロセスがマッチしてしまうため `grep -v` で除外する# CentOS 6 では pgrep に -a オプションがないため注意## 複数のプロセスが見つかっても無視するPROCESS=$(pgrep -f -a -u "${USER}" "${PROCESS_NAME}" | grep -v "$0" | head -1 | cut -f 1 -d ' ')if [ -z "${PROCESS}" ]; then  echo "${PROCESS_NAME} does not exists" 1>&2  exit 1fils /proc/"${PROCESS}"/fd/ | wc -l#!/bin/bashif [ $# -ne 3 ]; then  echo "Require 3 arguments" 1>&2  exit 1fiMETRIC_NAME=$1VARUE=$2METRIC_TYPE=$3echo -n "${METRIC_NAME}:${VARUE}|${METRIC_TYPE}" | nc -u -4 localhost 8125$ crontab -l* * * * * /path/to/send-to-dogstatsd.sh open_file_descriptors.puma_cluster_worker $(/path/to/get-open-fd.sh "puma: cluster worker" dev) g > /dev/null]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[JAWS DAYS 2018 に行ってきた]]></title>
        <id>https://blog.tsub.me/post/jaws-days-2018/</id>
        <link href="https://blog.tsub.me/post/jaws-days-2018/"/>
        <updated>2018-03-17T06:16:55.000Z</updated>
        <summary type="html"><![CDATA[社内勉強会の準備などで忙しく、レポートを書くのが遅れてしまいましたが、先週の 03/10 (土) に JAWS DAYS 2018 へ行ってきました。今回が初参加でしたが、AWS ユーザーグループのお祭りという感じですごく盛り上がっていて楽しいイベントでした。会社の同僚も 4 人ぐらい参加してました。自分が参加したセッションと聞いた感想やメモをつらつら書いていきます。(ただし Keynote は省きます)]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[m3.medium のインスタンスの CPU 負荷が高かったため t2.medium へ移行した]]></title>
        <id>https://developer.feedforce.jp/entry/2018/03/02/155020</id>
        <link href="https://developer.feedforce.jp/entry/2018/03/02/155020"/>
        <updated>2018-03-02T06:50:20.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、エンジニアの id:tsub511 です。最近頭痛がするのでヨガを始めましたが、効果が出ているのかよく分かりません。今回は m3.medium のインスタンスの CPU 負荷が高かったため t2.medium へ移行したら解決した話をします。m3.medium のインスタンスの CPU 負荷が高かった年始あたりから、週に数回ほど決まった時間に Mackerel でアラートが出ていました。CPU の Steal 値が異常に高く、全体としての使用率が 90 % を超えていました。ずっと原因が分からず、最初は Meltdown と Spectre のパッチを適用した関係で性能が低下したんじゃないか、などを疑っていました。しかし、ある時全く別の作業をしていたときに別のロールのインスタンスで同様に CPU 負荷が上がり、どちらも m3.medium というインスタンスタイプが共通していたことからなんとなくググってみたところ、以下の記事に辿り着きました。toritori0318.hatenadiary.jpどうやら、m3.medium というインスタンスタイプのみ CPU の Steal が発生しやすいようです。他にも同様の報告をしている記事をいくつか見つけました。https://forums.aws.amazon.com/thread.jspa?threadID=146585High CPU steal on EC2 m3.medium – Bonobos Tech Blog情報が 2014 年と古いですが、現に同様の事象が発生しているため、当時と変わっていない可能性が高いです。そのため、インスタンスタイプを変更することを検討しました。他のインスタンスタイプを検討m3.medium から別のインスタンスタイプに変更するに辺り、どのインスタンスタイプを選択するか、まずはコスト面で比較しました。calculator.s3.amazonaws.com順当に行けば m4 ファミリーが妥当なところですが、m4 ファミリーは medium サイズは提供していないため、費用がそれなりに増えてしまいます。この中で、m3.medium よりも安い t2.medium に目を付けました。t2.medium は m3.medium に比べると、 vCPU が 1 コア増え、メモリも 0.25 GB 増える上に料金が安くなるというかなりお得なインスタンスタイプです。m3.mediumインスタンスファミリー  インスタンスタイプ  プロセッサアーキテクチャ  vCPU  メモリ (GiB)  インスタンスストレージ（GB）  EBS 最適化利用  ネットワークパフォーマンス汎用  m3.medium  64 ビット  1  3.75  1 x 4  -  中https://aws.amazon.com/jp/ec2/previous-generation/t2.mediumモデル  vCPU  CPU クレジット/時  メモリ (GiB)  ストレージt2.medium  2  24  4  EBS のみhttps://aws.amazon.com/jp/ec2/instance-types/ただ、ここで安易に t2.medium を選択してはいけません。t2 ファミリーは「バースト可能パフォーマンスインスタンス」という特別な性質があります。T2 インスタンスについてT2 インスタンスについて、今までふわっとした理解しかなかったため、この機会に AWS のドキュメントをちゃんと読んでみました。docs.aws.amazon.com結論から言うと、弊社のサービスの性質上、決まった時間に Sidekiq のジョブがまとまって大量に実行されるため、普段は CPU 使用率は低く、ある時間だけ CPU 使用率が高くなるというまさに T2 インスタンスがピッタリなケースでした。CPU クレジットT2 インスタンスには CPU クレジットという概念があります。docs.aws.amazon.com1 CPU クレジットは 100 % の CPU 使用率を 1 分間稼働させることができます。t2.medium は CPU クレジットが 1 時間あたり 24 なので、100 % の CPU 使用率を 24 分間、あるいは 50 % の CPU 使用率を 48 分間、40 % の CPU 使用率なら 60 分間稼稼働させることができることになります。(ただし、t2.medium は vCPU が 2 コアなので、実際には 20 % の CPU 使用率で 60 分間の稼働)実際の CPU 使用率は平均で 20 % 以下に収まっていることが多い (たまにスパイクはする) ので、CPU クレジットが 24 ならまず問題ないです。この 40 % (20 %) という値をベースラインパフォーマンスと呼び、これを超えて CPU を使用することを「バースト」と呼びます。また、ベースラインパフォーマンスよりも CPU 使用率が下回っていた場合、クレジットバランスというものに余分な CPU クレジットが保存されます。クレジットバランスに保存された CPU クレジットは、CPU 負荷がベースラインパフォーマンスを上回った時に消費されます。つまり、余分な CPU クレジットは蓄積されて後で使うことができるということになります (ただし t2.medium の最大クレジットバランスは 576)。注意点としてはインスタンスを停止するとクレジットバランスに貯まった CPU クレジットは破棄されるというところでしょうか。T2 Unlimitedただ、T2 インスタンスを使う以上、気にしなければいけないのは CPU クレジットがなくなった場合は CPU のバーストができなくなるということです。CPU のバーストができないということはつまり、ベースラインパフォーマンス (t2.medium の場合は 20 %) 以上の CPU が使えなくなるということになります。ただし、去年の Re:Invent にて発表された T2 Unlimited という機能を有効にすることで CPU クレジットがなくなった場合でも自動的に CPU クレジットを追加され、CPU 使用に制限がかからなくなります。docs.aws.amazon.com具体的には、T2 Unlimited を有効にすると、CPU クレジット及びクレジットバランスがなくなった場合、余剰クレジットというものから消費されるようになります。最初の余剰クレジットは 24 時間で獲得できるクレジットの合計値となります。例えば t2.medium の場合、1 時間辺りの獲得クレジットは 24 なので 24 時間で 576 のクレジットが余剰クレジットになります。この 24 時間分の余剰クレジットは前借りのようなもので、消費した分だけ次のクレジット獲得時に余剰クレジットの支払いに使用されます。24 時間分の余剰クレジットまで全て使い切ってしまった場合でも、その後に消費した余剰クレジット分は追加で課金され、CPU のバーストは継続することが可能です。つまり、T2 Unlimited を有効にすれば T2 インスタンス特有の CPU クレジットの枯渇による CPU 使用制限の問題が解決されることになります。ただし、常にバーストし続けて追加でお金が発生し続けるような場合は、T2 インスタンスでなく普通にインスタンスタイプを利用したほうが懸命ですね。CPU クレジットの監視T2 Standard (非 T2 Unlimited) であっても、T2 Unlimited であっても、普段からどの程度 CPU がバーストしているかは監視しておいたほうが良いです。そのために、CloudWatch で CPUCreditUsage, CPUCreditBalance, CPUSurplusCreditBalance, CPUSurplusCreditsCharged という 4 つのメトリクスが提供されています。個人的には T2 Unlimited の場合、基本的には CPUSurplusCreditBalance と CPUSurplusCreditsCharged を監視しておけば良いと思います。CPUSurplusCreditBalance は消費された 24 時間分の余剰クレジット数CPUSurplusCreditsCharged は 24 時間分の余剰クレジットを使い切った後で更に消費される余剰クレジット数実際の監視には Datadog を利用しました (現在監視ツールを Datadog へ移行途中なため)。閾値はまだ感覚を掴めていないため、とりあえず厳しめにしてあります。m3.medium から t2.medium へインスタンスタイプを変更する弊社のサービスのインフラでは、Blue Green Deployment が可能な体制が整っているため、インスタンスタイプの変更は非常に簡単です。新しい環境のインスタンスは t2.medium で作成し、ELB からコネクションが流れるようになったら、古い環境のインスタンスを削除するだけです。ただ、EC2 の Launch Configuration + Auto Scaling Group を使っていたため、少し工夫が必要でした。T2 Unlimited の有効化は Launch Configuration ではサポートされていませんでした。Auto Scaling グループで T2 インスタンスを無制限に設定して起動するには起動テンプレートを使用する必要があります。起動設定では、T2 インスタンスを無制限として起動することがサポートされていません。https://docs.aws.amazon.com/ja_jp/AWSEC2/latest/UserGuide/t2-unlimited.html#t2-auto-scaling-grpLaunch Template ならサポートされているものの、今から移行するのも大変ですし、何より Terraform がまだ Launch Template をサポートしていませんでした (2018/03/02 時点)。github.comどうしようと困っていたところ、以下の記事に助けられました。dev.classmethod.jpEC2 User Data を使って、インスタンス起動時に自身に対して T2 Unlimited を有効化する、という方法です。自分では全く思いつきませんでしたが、User Data も Terraform を使って管理できるのでかなりシンプルに実現できました。実際には以下の User Data を利用しました (CentOS を使っているため $ yum install aws-cli ができない)。#!/bin/bashset -x# Install aws-clicurl -L https://bootstrap.pypa.io/get-pip.py | pythonpip install awscli --upgrade# Enable T2 UnlimitedINSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)REGION=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone | sed -e 's/.$//')aws --region "${REGION}" ec2 describe-instance-credit-specifications --instance-id "${INSTANCE_ID}"aws --region "${REGION}" ec2 modify-instance-credit-specification --instance-credit-specification InstanceId="${INSTANCE_ID}",CpuCredits=unlimitedaws --region "${REGION}" ec2 describe-instance-credit-specifications --instance-id "${INSTANCE_ID}"これで、Auto Scaling Group によって起動したインスタンスに対して自動的に T2 Unlimited が有効になりました。インスタンスタイプを t2.medium に変更した結果実際に t2.medium のインスタンスを稼働させた結果、同程度の負荷がかかった際の CPU の Steal 値はほぼ 0 になりました。ちなみに t2.medium は vCPU が 2 つあるため、グラフの最大値は 200 % になっています。user 値が 90 % 程度なので、実質 CPU 使用率は 45 % 程度で、m3.medium の頃とほとんど性能は変わっていません。また、その他にも 5 分間のロードアベレージも全体的に下がっていました。まとめm3.medium のインスタンスを使っていて CPU 負荷に悩まされている場合はインスタンスタイプを変更すると解決するかもT2 インスタンスは適材適所で使えば費用を安く抑えられて非常に良いT2 Unlimited によって CPU クレジットがなくなる問題が解決されて安心して T2 インスタンスを使用できるようになったT2 Unlimited を Launch Configuration で有効化したい場合は User Data を使うと良いm3.medium が原因だったようで、解決して良かったです。日々のアラートに悩まされなくて良くなりました。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[続・Rails 5.2 開発環境を Docker で構築する]]></title>
        <id>https://developer.feedforce.jp/entry/2018/02/25/234556</id>
        <link href="https://developer.feedforce.jp/entry/2018/02/25/234556"/>
        <updated>2018-02-25T14:45:56.000Z</updated>
        <summary type="html"><![CDATA[どうも、バックエンドエンジニアのサトウリョウスケです ✌︎('ω')✌︎前回の記事では Docker を使って Rails 5.2 の環境構築をしました。現在も引き続き Docker についてのお話をします。developer.feedforce.jpその後も幾つか手を加え続けておりまして、現在この記事を書いている時点で v1.3.0 になりました 🎉明らかに初回のナンバリングを間違えていた感がありますが、少しずつインクリメントさせていく楽しみを実感できて良いです 笑前回からの変更点についてさて、前回の記事は v1.0.0 時点のものでしたが、ここで v1.3.0 になった現在の Dockerfile を見てみましょう。# DockerfileFROM ryz310/rails-on-dockerなんと！たったの１行ぽっちです！＼＼\٩( 'ω' )و //／／何言ってんだコイツと思われそうですが、これはどういう事かというと、大部分を base/Dockerfile に移動したためです。# base/DockerfileFROM ruby:2.5MAINTAINER ryz310@gmail.comRUN apt-get update -qq && apt-get install -y build-essential libpq-dev nodejsWORKDIR /myappENV BUNDLE_JOBS=32ONBUILD ADD Gemfile /myapp/GemfileONBUILD ADD Gemfile.lock /myapp/Gemfile.lockONBUILD RUN bundle installONBUILD ADD . /myappこの base/Dockerfile のイメージは 僕のDocker Hub に置いてあります。ONBUILD が付いたコマンドはこのイメージを継承したイメージで実行されるため、先ほどの Dockerfile には FROM しかありませんが、ビルドの際には以下の 4 つのコマンドが実行される事になります。ADD Gemfile /myapp/GemfileADD Gemfile.lock /myapp/Gemfile.lockRUN bundle installADD . /myapp感覚としては、 base/Dockerfile で Rails の起動に必要なサーバー環境を構築して、 Dockerfile で Rails そのものを構築していくような感じです。Rails の構築には RDS イメージなども必須となってきますので、それについては docker-compose.yml と組み合わせて構築していきます。# docker-compose.ymlversion: '3'services:  db:    image: mysql:5.7    volumes:      - mysql_data:/var/lib/mysql    environment:      MYSQL_ALLOW_EMPTY_PASSWORD: 'yes'    ports:      - "3306:3306"  web:    build: .    image: web_image    command: bundle exec rails s -p 3000 -b '0.0.0.0'    volumes:      - .:/myapp      - bundle:/usr/local/bundle    ports:      - "3000:3000"    depends_on:      - db    environment:      DB_HOST: db  spring:    image: web_image    command: bundle exec spring server    volumes:      - .:/myapp      - bundle:/usr/local/bundle    tty: false    stdin_open: false    environment:      DB_HOST: dbvolumes:  mysql_data:  bundle:ここも v1.0.0 の頃からいくつか変更がありまして、例えば web と spring で同じ内容になるように v1.0.0 では YAML のエイリアスを使っていたのですが、 docker image で共有する方法に変えました。この定義の記述について少し補足します。web の定義で build: . と image: web_image と言う記述がありますが、これは Dockerfile をビルドして web_image というタグをつける、と言う振る舞いになります。一方、 spring の定義の image: web_image では web_image というタグが付いたイメージを使用する、という振る舞いになります。この辺はなんだかややこしいですね 😓また、 bundle:/usr/local/bundle を volume に指定する事で、bundle install の内容を永続化するようにしてあります。これにより、$ docker-compose run --rm web bundle install というコマンドが有効になってくるので、Gemfile を更新した際に一から Build する必要がなくなります。同様に、 MySQL も mysql_data:/var/lib/mysql を volume に指定する事で、テーブルの内容を永続化するようにしています。まとめONBUILD を使用する事で、他の Rails アプリでも同じイメージを転用できるようになったのが一番の改善点ではないかと思います。ただ、Rails アプリ側の事情で何か $ apt-get install を加えたくなった場合にどうするのか、という課題感があります。base/Dockerfile は汎用的な環境構築を意識しているので、あまり一般的でないインストールは行いたくありません。理想としては「子イメージ」で $ apt-get install させて、「孫イメージ」で今回のように Rails の構築をする構成が実現できれば良いのではないかと思いますが、 ONBUILD を「子イメージ」でスキップさせるとか出来るんでしょうか？次回はこの辺について少し調べていければ、と思っています。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[201802 Twitter ポリシー変更]]></title>
        <id>https://developer.feedforce.jp/entry/2018/02/22/210931</id>
        <link href="https://developer.feedforce.jp/entry/2018/02/22/210931"/>
        <updated>2018-02-22T12:09:31.000Z</updated>
        <summary type="html"><![CDATA[ソーシャルPLUS 開発チーム id:tmd45 です。本日こんなニュースが Twitter 上で話題になっていました。ツイッター社、複数アカウントから同じ内容の投稿禁止 - ライブドアニュース公式発表は現地時間 2018/2/21 の開発者向けブログの記事のようです。blog.twitter.com国内ニュースでは一般のユーザアカウント（そのユーザが spammer であるかどうかは区別せず）の発言がただ対象になるかのように見えますが、実際には Twitter App を介して複数アカウントにツイートや Retweets、Like などの拡散を実行させることを禁止する ようです。ざっくり読み下してるので、もし読み違えている部分があればご指摘いただけると助かりますm(__)mDo not (and do not allow your users to) simultaneously post identical or substantially similar content to multiple accounts. For example, your service should not permit a user to select several accounts they control from which to publish a given Tweet.たとえば、あなたのサービスのなかから何かシェアさせたいときに、シェア対象に複数の Twitter アカウントを指定できるような実装は許可されません。代わりに１つのアカウントで Tweet させ、それ以外のアカウントは Retweet を利用してシェアすることが推奨されます。Please note that bulk, aggressive, or very high-volume automated Retweeting is not permitted under the Automation Rules, and may be subject to enforcement actions.ただし自動化された異常に大量な Retweets も、Twitter 側の自動検知ルールにひっかかって執行対象になることがあるので注意とのこと。Do not (and do not allow your users to) simultaneously perform actions such as Likes, Retweets, or follows from multiple accounts. For example, your service should not permit a user to select several accounts they control to follow a specified account.複数のユーザアカウントに Like, Retweets をさせるようにアプリケーションがユーザ（アカウント）を制御するのも禁止です。The use of any form of automation (including scheduling) to post identical or substantially similar content, or to perform actions such as Likes or Retweets, across many accounts that have authorized your app (whether or not you created or directly control those accounts) is not permitted. For example, applications that coordinate activity across multiple accounts to simultaneously post Tweets with a specific hashtag (e.g. in an attempt to cause that topic to trend) are prohibited.Twitter App を、同一または類似したコンテンツを投稿するため、あるいは Like や Retweet などのアクションを実行するための自動化（スケジューリングを含む）に用いるのは許可されていません。その Twitter App を認可したユーザが App の作成者と関係あるかどうかにかかわらずです。つまり拡散のために大量のユーザアカウントを用意し、拡散の自動化のために Twitter App で開発をすることは禁止されます。このケースの例外として、以下のパターンが挙げられています。While we continue to permit cross-posting outside information (such as weather alerts or RSS feeds) to Twitter using automation, you should only post this content to one account you control.As a sole exception to this rule, applications that broadcast or share weather, emergency, or other public service announcements of broad community interest (for example, earthquake or tsunami alerts) are permitted to post this content across multiple accounts who have authorized an app.天気予報や RSS フィードなどを自動的に投稿するような利用は許可される。ただし自分で管理する１つのアカウントにのみ投稿することこのルールの唯一の例外として、天気、緊急事態、地震・津波など広く公開されるべき公的な通知をするアプリケーションは、複数のアカウントに投稿することが許可されますアプリが複数のアカウントを選択して投稿、Like、Retweet など出来るようになっている場合は 2018/3/23 までに新しいポリシーに準拠するよう変更する必要 があります。これを守らない場合、Twitter App とアカウントを一時停止されます。…ということで、普段気軽に拡散させるようなアプリを作っている開発者の方も、きちんと Twitter のアプリ開発ポリシーに目を通し、準拠するようにしましょう。The Twitter RulesAutomation rules]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rails 5.2 開発環境を Docker で構築する]]></title>
        <id>https://developer.feedforce.jp/entry/2018/02/11/140012</id>
        <link href="https://developer.feedforce.jp/entry/2018/02/11/140012"/>
        <updated>2018-02-11T05:00:12.000Z</updated>
        <summary type="html"><![CDATA[どうも、バックエンドエンジニアのサトウリョウスケです ✌︎('ω')✌︎僕が所属している ソーシャルPLUS チームでは Rails の開発環境を Docker で構築しています。自分も日々お世話になっている Docker ですが、イチから Dockerfile 書いた事が無かったので、やってみました。こちらのチュートリアルを参考にしています。クイックスタート・ガイド：Docker Compose と Rails — Docker-docs-ja 17.06.Beta ドキュメントやってみた✨早速ですがこちらが完成品の GitHub リポジトリになります。github.comこの記事は v1.0.0 時点で書いていますが、リポジトリは今後も更新されていく可能性があります。環境Debian StretchRuby 2.5.0Rails 5.2.0.rc1MySQL 5.7つかいかたRails 5.2 で環境構築したと言いましたが、リポジトリには Rails のソースコードが一切含まれていません。これは rails new から Docker でやっていくためです。以下に手順を示しますので、興味ある方はぜひ記事を読みながら一緒にやってみて下さい 🙏0. リポジトリをローカルにクローンする$ git clone https://github.com/ryz310/rails-on-docker.git1. 以下のコマンドを実行する$ docker-compose run web rails new . --force --database=mysql --skip-bundle --skip-gitrails new によって必要なファイルがインストールされます。同時に Gemfile も Rails 用に更新されます。残念ながらアプリ名は指定できないので、必要に応じて変更して下さい。なお、後述の MySQL データの永続化のため MySQL のデータファイルを含めないようにリポジトリの方で .gitignore を用意しています。そのため --skip-git を付けて Rails に .gitignore ファイルを作成させないようにしていますが、永続化が不用な場合はオプションを外して下さい。2. 更新された Gemfile で以下のコメントアウトを外す# gem 'mini_racer', platforms: :ruby以前であれば therubyracer だったのですが、いつの間にか mini_racer に変わっていたのですね。techracho.bpsinc.jpTechRacho さんの週刊Railsウォッチにはいつもお世話になっております 🙏3. $ docker-compose build を実行するGemfile を変更したので再読イメージを作り直します。4. config/database.yml を以下のように変更するdefault: &default  adapter: mysql2  encoding: utf8  pool: <%= ENV.fetch("RAILS_MAX_THREADS") { 5 } %>  username: root  password: xxxxxx # <- ここを変更  host: db # <- ここを変更あくまでローカルの開発環境構築なのでパスワードを隠したりとかはしません。本番運用とかでやっちゃダメですよ！MySQL の Root ユーザーのパスワードは docker-compose.yml で指定しています。必要に応じて書き換えて下さい。  db:    image: mysql:5.7    volumes:      - .mysql_data:/var/lib/mysql    environment:      MYSQL_ROOT_PASSWORD: xxxxxx # <- ここ！5. $ docker-compose up を実行するコンテナを起動させます。6. $ docker-compose exec spring spring rake db:create を実行するコンテナは起動していますが、Rails で使用する DB テーブルはまだ作成されていないので作成します。何気に spring を使っている点に注目です。7. http://localhost:3000/ にアクセスして Rails が動いていることを確認ここまでの操作で Rails が正しく動作しているはずです。Yay! You’re on Rails!ポイントbundle install が毎回走らないように工夫しているDockerfile で WORKDIR に /tmp ディレクトリを指定しているところがポイントです。これをせずに WORKDIR /myapp からの ADD . /myapp をやってしまうと、 Rails のコードに変更がある度に bundle install が最初から実行されてしまいます。Gemfile と Gemfile.lock を /tmp に格納することで、これらのファイルに変更がない限り bundle install が実行されないようになっています。WORKDIR /tmpADD Gemfile GemfileADD Gemfile.lock Gemfile.lockRUN bundle installWORKDIR /myappADD . /myapp参考: DockerでRails + MySQLの開発環境を構築 | EasyRamblespring に対応Rails で開発する上で欠かせない spring も利用できるようにしてあります。前述の rake db:create の時にも出てきましたが、$ docker-compose exec spring spring rails console のようにして使います。spring が 2 回出てくるところがポイントです。一つ目はコンテナのサービス名です。毎回書くのはだるいので僕は alias にしています。fish shell 用なので良いように読み替えて下さい 🙏# ~/.config/fish/config.fish# docker-compose aliasesfunction fig  docker-compose $argvendfunction figspring  docker-compose exec spring spring $argvend# USAGE$ figspring rails c参考: 高速に開発できる Docker + Rails開発環境のテンプレートを作った - QiitaMySQL データの永続化イメージを作り直した時に MySQL のデータが失われてしまうのは辛いものがあります。毎回テスト用のデータを一から作り直したくないので、 docker-compose.yml で以下のように指定してMySQL のデータを永続化させています。  db:    image: mysql:5.7    volumes:      - .mysql_data:/var/lib/mysql # <- これ！    environment:      MYSQL_ROOT_PASSWORD: xxxxxx.mysql_data ディレクトリ以下に MySQL のデータが格納されています。リポジトリの .gitignore で無視させています。試していませんが、参考にさせて頂いた記事によると Docker for Mac 以外の環境では問題が発生するようですのでご注意下さい。対応方法も記事内で紹介されています 🙏参考: docker composeでMySQLのデータ領域をローカルにマウントする | WEB EGGまとめ週末に何かアプリでも書こうかと思った時に、ついでに Docker で環境構築もやっちゃうか、と思い立って書きました。そうこうしてるうちに週末の半分くらいが過ぎ去っていますが、きっと今後は捗るはず。。 😇何度も確認していますが、もし間違っている点、不便な点などありましたらそっと教えて頂けますと幸いです 🙏Fork も大歓迎ですし、誰かに使ってもらえると嬉しいです 😁]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS Lambda with Golang と SAM に入門した]]></title>
        <id>https://blog.tsub.me/post/introduce-aws-lambda-with-golang-and-sam/</id>
        <link href="https://blog.tsub.me/post/introduce-aws-lambda-with-golang-and-sam/"/>
        <updated>2018-01-31T14:15:00.000Z</updated>
        <summary type="html"><![CDATA[先日 AWS Lambda の Golang サポートがリリースされました。Announcing Go Support for AWS Lambda | AWS Compute Blog今回は AWS Lambda を Golang で書きつつ、SAM へも入門したのでその辺りの知見とか作ったものについて紹介します。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[mackerel のカスタムメトリックを echo でワンライナーしたった]]></title>
        <id>https://developer.feedforce.jp/entry/2017/12/31/143611</id>
        <link href="https://developer.feedforce.jp/entry/2017/12/31/143611"/>
        <updated>2017-12-31T05:36:11.000Z</updated>
        <summary type="html"><![CDATA[どうも、バックエンドエンジニアのサトウリョウスケです ✌︎('ω')✌︎突然ですがワテクシ、 Rails アプリはそこそこ書ける方なんですが、インフラはからきしだったりします。 フルスタックじゃないエンジニアが許されるのは小学生までだよねー、というちょっと懐かしみのある煽りが社内のからも聞こえてきそうなので、最近インフラを少しずつ触るようにしています。触ってみるとインフラも色々楽しいですね ✌︎('ω')✌︎とはいえ、インフラエンジニアレベル 1 の自分があれこれやっても障害に繋がるだけなので、まずはお手軽な雑用タスクから始めることにしました。さて、少し話は変わって、最近、弊社サービスで Sidekiq プロセスのファイルディスクリプタが急騰してサーバーがダウンする、という障害が何度か発生しました。現象としては Sidekiq プロセスのファイルディスクリプタが上限値 (初期値は 1024 ) に達すると、サーバーの CPU 使用率が 100 % 付近まで達してしまう、というものだったので、ファイルディスクリプタの上限を 65536 まで上げる、というワークアラウンドな対応で凌いでいました。現在は解決済み（この原因については、別途記事にします）ですが、当時はなぜファイルディスクリプタの数が上昇し続けてしまうのか原因が全くわからず、チームのエンジニアは安眠できない日々が続いていました。とりあえず Sidekiq のプロセスをリスタートすればファイルディスクリプタの数は一旦リセットされるので、監視してやばくなったらアラートを飛ばして再起動させる、という方法で一旦は凌ます。また、継続して監視することで、原因の解明にも繋がるかもしれません。弊社のサービスでは監視に mackerel を利用しているので、カスタムメトリックを使ってお手軽に監視させよう、ということになりました。mackerel のカスタムメトリックの投稿方法は こちらの記事 にまとまっています。要点だけ抜粋しますと、 mackerel-agent の設定ファイル に 以下の書式で標準出力を実行するコマンドを記述 すれば OK です。設定ファイルで指定するコマンドは、標準出力の各行に以下のフォーマットの出力をすることが期待されます（\t はタブ文字です）:{metric name}\t{metric value}\t{epoch seconds}そして以下がファイルディスクリプタを監視するための設定です（完成品がレンジから出てくるパティーン）[plugin.metrics.file_descriptor_count]command = '''  echo -e "file_descriptor.sidekiq\\t$(sudo ls /proc/$(pgrep -f -u {user_name} sidekiq | head -1)/fd/ | wc -l)\\t$(date -u +%s)"'''標準出力されれば OK なので、 echo で任意の文字列を出力するような方法でも実現可能です。内部で pgrep -f -u {user_name} sidekiq と書いて、プロセス ID を取得していますが、 -u でプロセスを実行しているユーザーを指定しないと、 pgrep のプロセス ID を取得してしまうケースがあるので注意が必要です。(mackerel-agent は root で実行される)同じ要領で、他にも以下のように書けば puma のファイルディスクリプタも取ることができます。[plugin.metrics.file_descriptor_count]command = '''  echo -e "file_descriptor.puma\\t$(sudo ls /proc/$(pgrep -f -u {user_name} puma.sock | head -1)/fd/ | wc -l)\\t$(date -u +%s)"  echo -e "file_descriptor.puma_cluster_worker\\t$(sudo ls /proc/$(pgrep -f -u {user_name} 'puma: cluster worker' | head -1)/fd/ | wc -l)\\t$(date -u +%s)"'''ちなみにこの記事を書くときに、もしやと思って調べてみたら、 mackerel-agent-plugins の中に 任意のプロセスのファイルディスクリプタを監視する奴 がありました 😓試していませんが、こっちを使った方が良いと思います 😇そんな感じで監視できたグラフが以下になります。ファイルディスクリプタ数がファイナルファンタジーの HP みたいになってますね。。。とりあえず 10,000 を超えたらアラートを飛ばすように設定しましたが、デプロイする度にリセットされるので、結局アラートが飛ぶことも障害が発生することもなく、問題は解決しました。本稿では echo を使ったワンライナーでカスタムメトリックを投稿する方法を紹介しました。mackerel-agent-plugins に載っていないけどワークアラウンドでとりあえず監視させたい、という時は便利だと思うので、どこかでご活用ください 🙏]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[GitHub 上でコミットハッシュにもとづく URL を得る]]></title>
        <id>https://developer.feedforce.jp/entry/2017/12/20/141220</id>
        <link href="https://developer.feedforce.jp/entry/2017/12/20/141220"/>
        <updated>2017-12-20T05:12:20.000Z</updated>
        <summary type="html"><![CDATA[ご存知のかたも多いかと思いますが、GitHub で管理しているコードについて、コードの "ある部分" を示したいときに貼り付ける URL には注意が必要です。ソースコードの参照リンクを貼るときに気をつけたいことたとえばこんなリンクを貼り付けた場合https://github.com/tmd45/tmd45.github.io/blob/source/Gemfile#L16これは source ブランチの最新の Gemfile ファイルの 16 行目を参照します。いま（記事執筆時点で）このリンクを開くと gem 'slim', '~> 3.0.2' が書かれている行を参照してることがわかりますが、この先 source ブランチのコードに変更があった場合「最新の」コードでは別のものを指してしまう可能性があります。なので "ある行" を参照する場合にはコミットハッシュを特定した URL を貼るのがベストです。https://github.com/tmd45/tmd45.github.io/blob/de6fb7dd0309465b936817e8452948084c8d7c76/Gemfile#L16この URL はコミットハッシュ de6fb7dd0309465b936817e8452948084c8d7c76 の Gemfile ファイルの 16 行目を参照しているので、コードに変更があってもリンク先の内容が変わってしまうことはありません。コードの "ある部分" を示したいときにはこの違いに気をつけましょう。GitHub の便利なショートカットというわけで、ここまでは自分も当たり前のように実践していたのですが、この「コミットハッシュにもとづく URL」取得のためにいちいち History などから最新のコミットハッシュを辿るということをしていました。面倒。最近同僚に教えてもらったのですが（そして感動してこの記事を書いているのですが）GitHub ページ上のショートカット機能により y キーを押すことで、現在見ているファイルのコミットハッシュにもとづく URL を簡単に得ることができます。以下のページを開いているときに y キーを押すとhttps://github.com/tmd45/tmd45.github.io/blob/source/Gemfile#L16アドレスバーで以下の URL に変わるのがわかります（コミットハッシュ部分は最新のものに変わります）。https://github.com/tmd45/tmd45.github.io/blob/de6fb7dd0309465b936817e8452948084c8d7c76/Gemfile#L16一発で便利！( ﾟДﾟﾉﾉ"☆ﾊﾟﾁﾊﾟﾁﾊﾟﾁﾊﾟﾁちなみにGitHub 上で利用できる便利なショートカット機能は ? キーを押すことで GitHub ページ上のどこでも確認できます。Expand URL to its canonical formというのが今回のそれですね。t と l はよく使ってたんですが、これは気づいていませんでした(；´∀｀)"Source code browsing" にあるショートカットは、GitHub 上でコードブラウジングを行う上で非常に便利なので、ぜひ試してみて下さい。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[結婚して半年が経ったので工夫していることとか]]></title>
        <id>https://blog.tsub.me/post/half-a-year-after-married/</id>
        <link href="https://blog.tsub.me/post/half-a-year-after-married/"/>
        <updated>2017-12-09T12:30:00.000Z</updated>
        <summary type="html"><![CDATA[この記事は feedforce Advent Calendar 2017 の 9 日目の記事です。昨日の記事は tmd45 さんの TypeScript 社内勉強会 完遂報告 - Feedforce Developer Blog でした。さて、本題ですがワタクシ今年の 4 月に結婚をしました。妻は Web コーダーで、割と Web サービスなどにも抵抗がなく普段から Slack や Kibela などを夫婦間で活用しています。今回はその辺りで色々と工夫している部分を紹介できればと思います。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[TypeScript 社内勉強会 完遂報告]]></title>
        <id>https://developer.feedforce.jp/entry/2017/12/08/090000</id>
        <link href="https://developer.feedforce.jp/entry/2017/12/08/090000"/>
        <updated>2017-12-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[この記事は feedforce Advent Calendar 2017 の 8 日目の記事です。昨日は id:phiitakes の『フィードフォースのスポーツ系活動紹介 - phiitakesのなにか』でした。弊社こんなに運動部あったんだ！？ｗこにゃにゃちわ（世代）id:tmd45 です。ちょうど今週、社内で開催していた TypeScript 勉強会が一段落したので、それについて書きたいと思います。TypeScript の話というよりは社内勉強会のやりかたのひとつという話になりますのでご了承ください。きっかけ今年の夏頃から担当プロダクトで AWS Lambda を利用するようになりました。当初開発に取り組んでくれたサーバサイドメンバーが JavaScript 学んでいたり、強力なフロントエンド開発メンバーがいたりということで実装に Node.js を選択しています*1。フロントエンドチームでは静的型付け（と関数型）の導入が進んでおり、とくに新しいプロダクトでは Flow を用いた開発が行われています。そんなこんなで Lambda function の実装でも静的型付けの可能な言語で開発したいという向きがあり、とはいえ JavaScript の基礎もあやふやなチームメンバー（主に私）がいる状況でしたので Pure JavaScript から大きく逸脱することがなく最新の仕様にも触れられる TypeScript でやってみようという選択になりました。仕事でやるからにはしっかり勉強せねばと思いつつ、自力だけで学ぶことになると最初に開発に取り組んでくれたメンバー以外（主に私）は意識が下がりがちになると思い、社内勉強会という名目で取り組むことにしました。やりかた仕事ですぐに使う技術を学ぶという名目でしたので、週一でゆっくりというよりは、短時間で刻んでどんどんやっていこうという気持ちでセッティングしました。導入を決めた自分のチーム（のサーバサイドメンバー）の参加は必須として、ミーティングの日程を避けて 1 イテレーション＝ 2 週間のなかでミーティングの無い日を中心にスケジュールしました。 曜日  予定  月  スプリントプランニング  火  なし  水  TypeScript 勉強会  木  なし  金  TypeScript 勉強会      月  TypeScript 勉強会  火  なし  水  TypeScript 勉強会  木  なし  金  スプリントレビュー、レトロスペクティブ、KPT 回数を増やしたぶん、1 回の実施時間を短めに 30 分〜 45 分 としました（それでも後半は白熱して 1 時間いっぱい使ってしまいましたが）。開催時間は毎朝の デイリースクラムを終えたあとの、午前中 にしました。夕方は日によって早く帰りたいこともありますが、朝は普段からチームで集まることが決まっているので時間確保がしやすかったためです。そんな感じでスケジューリングしつつ、参加したいひとはお気軽にご参加ください、というスタンスで進めました。結果、参加人数は 3 〜 7 人と幅がありました。はじめるまえにお気持ちを表明してみた教材は市販の書籍も検討しましたが   TypeScript Deep Dive というオンラインドキュメント を利用しました。浅すぎず深すぎず、最新の JavaScript のことも学べる適度な教材はなんだろう？と投げかけたところ、先に TypeScript を自主学習していたメンバーやフロントエンドの元老院の方からオススメしてもらいました。英語だけど、みんなで読むならこわくない。また Promise の部分では追加で   JavaScript Promiseの本 の前半も読みました。どちらのドキュメントでもこの勉強会では基礎部分だけ実施しました。仕事で使いながら、あとは自分で応用編も読み進めていく予定です。よかったこと学習した内容からとくに思ったのは、言語の設計思想を理解して学ぶと納得感が違うなということです。 Deep Dive の冒頭には TypeScript がどういうスタンスの JavaScriptTranspiler であるか？という解説がありました*2。また、参加メンバーは普段主に Ruby を書いているひとが多いので「Ruby ではここはこういうやりかたになってるね」という話をしたり、いろんな言語を触ってよく学んでいるメンバーから「別の言語ではこういう思想でこういう実装がされているよ」とか、たとえば「Flow では（思想が違うので）ここは勝手に推論してくれるんですよ」みたいな話をしてもらったりしました。Slack で疑問点を共有しながら進めていると「先生」になるような社内メンバーたちが説明をくれたり議事録*3にもコメントをもらったり、とても助かりました。Qiita:Team でかんたんな議事録を作っていました「先生」レベルの人たちが毎回勉強会に参加するのは、どうしても（お互いに）無駄に感じてしまうこともあると思います。興味のある章だったり時間のあるときには気軽に参加してもらって、そうでないときも助けてもらうことができて大感謝でした 🙏まとめ勉強会を実施した成果として、実際に TypeScript で書いているリポジトリが最低限読めるようになりましたし、他のフロントエンドのリポジトリでも「呪文かコレ」って思うことがなくなりました！開発の入り口に立つことが出来たように思います。大成功 😊年明けには業務で利用している外部の JS SDK の TypeScript ソースコード・リーディング会をやってみたいと考えています(๑•̀ㅂ•́)و✧最終回の参加メンバー！おつかれさまでした！！〜〜〜さて、明日は弊社には結構多い（！）既婚者勢の id:tsub511 による 結婚して半年が経ったので工夫していることとか です。おたのしみに〜*1:隣のチームでは Python を利用していますね。開発のメインは Ruby ですが、それ以外の言語はチームによって検討している感じです*2:あくまでも Pure JavaScript がそのまま記述できる（ように設定できる）ように設計され、既存の JavaScript コードを部分的に移行していけるものである、とか*3:業務時間中にやっている勉強会だったので「真面目に勉強してますよ〜」と報告する理由で書いていたのですが（笑）あとで見返すこともできて良かった]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS でコンテナを動かすためのサービスまとめ]]></title>
        <id>https://developer.feedforce.jp/entry/2017/11/30/133525</id>
        <link href="https://developer.feedforce.jp/entry/2017/11/30/133525"/>
        <updated>2017-11-30T04:35:25.000Z</updated>
        <summary type="html"><![CDATA[こんにちは、バックエンドエンジニアの tsub (id:tsub511) です。本日 AWS Re:Invent 2017 でコンテナ実行環境として新たに AWS Fargate と Amazon Elastic Container Service for Kubernetes (EKS) が発表されました。昨日は発表が待ち遠しくて気が気じゃなかったですが、無事に予想通りマネージド Kubernetes サービスが発表されて大喜びです。今回は AWS でコンテナを扱う上で、今までのサービスと合わせて選択肢がいくつかあって混乱すると思うので簡単にまとめました。AWS でコンテナを動かすためのサービス新しく 2 つのサービスが追加されたことで、これだけあります。(見落としがなければ)Amazon Elastic Beanstalk (EB)Amazon Elastic Container Service (ECS)AWS BatchAmazon Elastic Container Service for Kubernetes (EKS) new!AWS Fargate new!それぞれの特徴について説明していきます。Amazon Elastic Beanstalk (EB)※ EB については自分は全く触ったことがないので分からないままで書きます。EB は Heroku のような PaaS です。本来は Heroku と同じようにアプリケーションのコードをそのままデプロイして動かしますが、Docker もサポートしていて、コンテナとしてデプロイして動かすことができます。小規模なサービス、チームなどでインフラの管理をしたくないというユースケースで使うことが多いと思います。Amazon Elastic Container Service (ECS)ECS は AWS が独自に開発しているマネージドなコンテナのオーケストレーションサービスです。(コンテナのオーケストレーションについては おまけ: コンテナのオーケストレーションについて を参照)EC2 の上で ecs-agent を動かすことで、ECS のクラスタとして認識させることができるため、非常にシンプルです。ecs-agent が動いていれば、ECS を通して EC2 の上でコンテナを簡単に動かすことができます。ECS 向けに awslogs という Docker 用の logging driver を提供していて、CloudWatch Logs との連携も簡単に行えます。ECS 上でのコンテナは、ECS Task や ECS Service として動かします。ECS Task には IAM Role を使った権限管理や VPC ネットワーク上で直接 Task を動かすことができ、セキュリティグループなどを利用することもできます。ECS Service は ELB との連携があり、コンテナが起動したら自動的に ELB に紐付けたり、コンテナが終了したら ELB から外したり、といったことをやってくれます。上記のように、他の AWS サービスとの連携がスムーズにできる点は非常に魅力的です。AWS BatchBatch はコンテナを使ったバッチコンピューティングに特化したサービスです。バッチコンピューティングと言っても、決まった時間に決まった処理をするという cron のようなものではなく、機械学習やスーパーコンピュータなどで利用するような大量の計算処理を必要とする場合に利用されるような基盤となります。そういう背景もあり、主に CPU ベースでのコンテナの割り振り、ホストのオートスケーリングなどに強いです。また、Batch はバックエンドで ECS を使っており、ジョブを動かすと実際に ECS Cluster が作られその上で Task が動いている様子を見ることもできます。ECS を使っていることもあり、ログは自動的に awslogs logging driver によって CloudWatch Logs に送られたり、CloudWatch による ECS のメトリクスを見ることが可能です。また、ECS と違って Job Queue も提供されていて、ジョブの状態管理なども可能となっています。CloudWatch Events によりジョブの状態変化によるイベントドリブンな処理も可能です。Amazon Elastic Container Service for Kubernetes (EKS) new!aws.amazon.comEKS はマネージドな Kubernetes を提供してくれるサービスです。(現在はプレビュー版のみの提供)ECS は AWS が独自に開発しているコンテナのオーケストレーションサービスでしたが、Kubernetes は Google が開発している OSS プロジェクトのコンテナのオーケストレーションツールです。(コンテナのオーケストレーションについては おまけ: コンテナのオーケストレーションについて を参照)Kubernetes は OSS ということもあり、コミュニティが非常に活発で多くの開発者・企業が開発に協力しています。Kubernetes を動かすための基盤はかなりたくさんの選択肢があり、あくまで EKS はそのうちの一つです。特に、今まで AWS 上で Kubernetes 環境を構築するために様々なツールがありました (私が知っている範囲で)。kubespraykube-awskopskubeadmTectonicRancher実際、今まで AWS 上で Kubernetes を利用していた人が多いようで、おそらくその方々は何らかのツールを用いて自前で構築していたと思います。https://aws.amazon.com/jp/blogs/news/amazon-elastic-container-service-for-kubernetes/AWS 上で Kubernetes を利用している多くのお客様がいます。実際、Cloud Native Computing Foundationによると、Kubernetes のワークロードの63％が AWS 上で動作しています。AWS は Kubernetes を実行するうえで人気の場所これらのツールを使って、Kubernetes クラスタの構築・管理を簡単にすることができますが、やはりマスターノードを管理する必要はでてくると思います。そこを AWS 側で管理・提供してくれるのが EKS となります。EKS としては Kubernetes クラスタの管理だけでなく、ELB や IAM、VPC、Private Link、CloudTrail などとの連携も提供してくれているため、自前で Kubernetes クラスタを構築するよりも便利になっています。後述の Fargate との連携も今後できるようになるとのことです。また、既存の Kubernetes 用ツール群を使えるのも大きな強みです。AWS Fargate new!aws.amazon.comFargate は単体のサービスではなく ECS や EKS の上で使うことのできるサービスです。(現在は北部バージニアリージョンのみの提供)前述した ECS や EKS と違い、コンテナを動かすホストについて意識せず、コンテナそのものを動かすことだけに集中することができます。どういうことかと言うと、Fargate では事前にホストを動かしておく必要はありません。AWS VPC 環境と ECS のクラスター (名前空間的な意味で) を作っておけば、後はコンテナを起動するだけで自動的にホストを用意し、コンテナを実行してくれます。また、ECS の上に乗っかっているので、使い方は簡単で ECS Task として起動する時に launch type として Fargate を指定するだけです。その他、ECS Service でも使うことができます。(EKS との連携についてはまだ情報が公開されていないため不明です)ホストをこちら側で管理しないため、ホストの監視の方法などは気になるところですが、CloudWatch を通してホストのメトリクスは取れるようです。https://aws.amazon.com/jp/blogs/news/aws-fargate-a-product-overviewFargateではアプリケーションのログをCloudWatch Logsに送ることができます。サービスのメトリクス(CPUとメモリの利用率)もCloudWatchメトリクスとして利用可能です。可視化や監視、アプリケーションパフォーマンスの領域での我々のパートナーである、DataDog、Aquasec、Splunk、Twistlock、そしてNew RelicもFargateタスクをサポートしています。また、Fargate に似たサービスとして Azure Container Instances (ACI) や Hyper.sh といったサービスも AWS 以外で提供されています。おまけ: コンテナのオーケストレーションについてコンテナを本番環境で動かそうとした時、コンテナをどのインスタンスで動かすか、どのインスタンスでコンテナが動いているのか、などコンテナの管理方法でいくつか問題が出てきます (あくまで一例です)。そういった問題を解決するため、コンテナのスケジューリングやマネージングをするツールを用意する必要がありますが、それらを解決するためのツールが ECS や Kubernetes, Docker Swarm などと言ったオーケストレーションツールとなります。ただし、どこにコンテナのスケジューリングやマネージングをする人が必要になってきます。その人をマスターノードなどと呼び、次はこれを管理・冗長化などしなければいけないという問題がでてきます。そのマスターノードの管理までマネージドで提供してくれているのが、ECS や EKS, GKE (Google Kubernetes Engine), AKS (Azure Container Service) などになります。まとめEB小規模なサービス・チームで使うと良さそうECSAWS 独自のコンテナのオーケストレーションサービスBatchバッチコンピューティング基盤として使うEKSKubernetes を使ったコンテナのオーケストレーションサービスFargate検証環境・本番環境・ちょっとした処理など幅広く使える機能的に ECS と EKS の使い分けは難しいですが、学習コスト・導入コストの面で ECS に軍配は上がると思います。ただ、コンテナを使う以上コミュニティが巨大な Kubernetes を使うことも大きなメリットです。個人的には EKS を推していきたいです。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamoid の使い方【global_secondary_index 編】]]></title>
        <id>https://developer.feedforce.jp/entry/2017/11/26/195509</id>
        <link href="https://developer.feedforce.jp/entry/2017/11/26/195509"/>
        <updated>2017-11-26T10:55:09.000Z</updated>
        <summary type="html"><![CDATA[どうも、バックエンドエンジニアのサトウリョウスケです ✌︎('ω')✌︎前回に引き続き、 Dynamoid 第3弾です ✌︎('ω')✌︎Rails で DynamoDB を利用する際の ORM として dynamoid があります。今回は dynamoid から Global Secondary Index (GSI) を利用する方法について紹介します。Global Secondary Index (GSI) ってなんぞ今回も名称の整理をしておきますGSI は検索のためのインデックスLocal Secondary Index (LSI) もあるやでどういう用途で便利なのかDynamoid での利用方法テーブル定義#where を使えば GSI を使って自動的にクエリで検索してくれる昇順・降順を入れ替えたい場合まとめdynamoid の導入方法については以前書いたこちらの記事を参考にしてみて下さい。tech.feedforce.jpGlobal Secondary Index (GSI) ってなんぞ今回も名称の整理をしておきます文中に Hash Key やら Range Key という名称が出てきますが、現在は名称が異なります。しかし、 dyanmoid では相変わらず旧名称のまま (hash_key, range_key) でパラメータを指定するので、今回も最初に対応表を記載しておきます。 旧名称  現名称  Hash Key  Partition Key  Range Key  Sort Key GSI は検索のためのインデックスDynamoDB にはプライマリキーの指定方法として、単一の Partition Key を使用する方法と、Partition Key と Sort Key を組み合わせて使用する方法があります。これについては前回の【range 編】の記事でも触れました。developer.feedforce.jpプライマリキーに指定されたカラムに対してであれば、レコードの抽出や範囲検索などが実行可能となる訳ですが、プライマリキーに指定されていないカラムに対しては検索が実行できず、テーブルのフルスキャンを実行することになってしまい非効率です。そこで、別のカラムに対しても検索を行いたい場合は GSI を設定して、フルスキャンすることなく効率的にデータを抽出できるようにします。 GSI もプライマリキーの指定と同様に、単一の Partition Key のみで指定することも、 Partition Key と  Sort Key の組み合わせで指定することも可能です。ただし、 プライマリキーにはユニーク制約が設定されますが、 GSI にはユニーク制約が存在しない ので、その点には注意が必要です。Local Secondary Index (LSI) もあるやでLSI の Partition Key はプライマリキーと共通です。Sort Key の部分だけ別に設定したい場合に使用します。その点を除けば GSI とよく似ていますが、こちらはテーブルの作成時にしか定義することができないようです。 なお、LSI もdynamoid から指定することは可能 (local_secondary_index を使用する) ですが、本稿では触れません。どういう用途で便利なのかあまり複雑なテーブル設計が推奨されない DynamoDB ですが、簡単なテーブル間の関連付けを行いたいシーンが出てきます。親テーブルの ID を結合キーとして子テーブルに設定したい場合などに GSI は便利です。以下に User Table と User Comment Table の例を示します。User Comment Table には親テーブルである User Table の ID が GSI の Partition Key として設定してあります。また、コメントの投稿日時 (Posted at) を GSI の Sort Key として設定しています。これで User 毎に投稿日時順にソートしたコメントを取得することができるようになります。User Table ID  name  1  John  2  Marry  3  Taro User Comment Table ID (Primary Partition Key)  User ID (GSI Partition Key)  Posted at (GSI Sort Key)  Comment  1  1  1509529916  Hello  2  1  1509530052  I am John  3  1  1509530085  How do you do?  4  2  1509523925  Thanks a lot!  5  3  1509527628  こんにちは  6  3  1509527101  どうも ちなみに、プライマリキーと GSI を逆に設定してもほぼ成立するのですが、前述したように GSI にはユニーク制約が存在しないので、User Comment Table では ID をプライマリキーとして、重複が生じないように設定してあります。Dynamoid での利用方法テーブル定義ここからは前述の User Table と User Comment Table を dynamoid から利用する例を示していきます。class User  include Dynamoid::Document  table name: :users, key: :id  field :name, :string  # 現在のユーザーに紐付くコメントを作成する  def create_comment!(attributes = {})    attributes[:user_id] = id    UserComment.new(attributes).tap(&:save!)  end  # 現在のユーザーのコメント一覧を取得する  def comments    UserComment.where(user_id: id)  end  # 現在のユーザーの最終コメントを取得する  def latest_comment    comments.scan_index_forward(false).scan_limit(1).all.first  endendclass UserComment  include Dynamoid::Document  table name: :user_comments, key: :id  field :user_id, :string  field :posted_at, :datetime  global_secondary_index hash_key: :user_id,                          range_key: :posted_at,                          projected_attributes: :allend※ ちなみに dynamoid には has_many を利用して関連テーブルを実現する方法があるのですが、結合キーを親テーブルに持つ設計になるのがあまり好ましくなかったので、自前で実装しています。いくつか注意する点があって、 global_secondary_index で使用する hash_key と range_key は field で定義されている必要があります。また、 projected_attributes: :all というオプションをつけないと後述の #where でインデックスを利用した検索が行われません。一旦これが無い状態でリリースとしてしまうと、射影される属性が限定された GSI が作成されてしまい、実行時にエラーになります。その場合は AWS マネジメントコンソールから直接 GSI を作り直す羽目になりますのでご注意ください 🙏One or more parameter values were invalid: Select type ALL_ATTRIBUTES is not supported for global secondary index#where を使えば GSI を使って自動的にクエリで検索してくれる#comments というメソッドの中で #where を使用した検索が登場しますが、 GSI が設定されていれば特別な記述がなくとも自動的にクエリ検索が行われます。UserComment.where(user_id: id)# => [#<UserComment:0x00007f44c86183e8>, ...]前述の通り projected_attributes: :all が指定されていないとフルスキャンされてしまうのでご注意ください。昇順・降順を入れ替えたい場合#latest_comment というメソッド内で使用していますが、 #scan_index_forward(false) と指定すると降順でソートされた状態で結果が返ってきます。未指定の場合は昇順でソートされます。また、 #scan_limit(n) と指定することで、先頭から n 件の結果に限定して取得が可能です。#latest_comment ではこれらを組み合わせて最終のコメントを取得しています。まとめ本稿では GSI と LSI とプライマリキーの違い、具体的な利用用途を紹介しました。前回の記事でも触れましたが、 dynamoid は初回実行時にテーブルや GSI が存在していないと作成する、という挙動になるため、後で設計を変えたくなった場合に GSI や最悪テーブルを作り直す羽目になります。特に初めて利用する場合は設計の勘所を掴むのが難しいので、リリース前に入念に設計を見直すことをお勧めします。その点では RDS 以上に慎重な設計が求められるように感じています。色々と気を付けなければならない点も多いですが、並列動作性は非常に高いので、利用したくなるシーンが必ず出てくると思います。その際に本稿が少しでもお役に立てば幸いです 🙏]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go で Datadog の Alfred Workflow を作った]]></title>
        <id>https://blog.tsub.me/post/create-alfred-workflow/</id>
        <link href="https://blog.tsub.me/post/create-alfred-workflow/"/>
        <updated>2017-11-26T07:00:00.000Z</updated>
        <summary type="html"><![CDATA[最近会社の同僚が Alfred Workflow を Go で書いたという LT を発表していて面白そうだったので、自分も書いてみました。以下のリポジトリで配布しています。 tsub/alfred-datadog-workflow: A Alfred workflow to open Datadog pagesWorkflow のダウンロードリンクはこちらから最新バージョンのものをどうぞ。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[VSCode ファイル検索の除外設定]]></title>
        <id>https://developer.feedforce.jp/entry/2017/11/24/195644</id>
        <link href="https://developer.feedforce.jp/entry/2017/11/24/195644"/>
        <updated>2017-11-24T10:56:44.000Z</updated>
        <summary type="html"><![CDATA[2019/10/15 追記 現在（Version 1.39.1）の VSCode では詳細検索オプションの内容が異なっております。詳しくは 公式ドキュメントをご参照ください。本文記事中の VS Code バージョンはもっと古いです。ポケ森 ⛺キャンパーレベル 18 の引きこもり id:tmd45です。鉱石掘りのお手伝いはお気軽にお申し付けください。VSCode のファイル検索の恒久的な除外条件は「基本設定」から行います。// 検索でファイルとフォルダーを除外するために glob パターンを構成します。// files.exclude 設定からすべての glob パターンを継承します。"search.exclude": {  "**/node_modules": true,  "**/bower_components": true},// 新しいワークスペースでテキストを検索するときに、// 既定で .gitignore ファイルと .ignore ファイルを使用するかどうかを制御します。"search.useIgnoreFilesByDefault": false,この設定をすると Cmd + P によるファイル移動の候補から、指定したものを除外することができます。後者の .gitignore .ignore による除外を有効にしておけば、search.exclude を個別に書かなくてもいい感じに除外されてハッピーです。この設定を Explorer の「検索」 Cmd + Shift + F でも有効にするには「詳細検索の切り替え」（検索ボックス右下の  マーク）で詳細検索オプションを開き「無視設定ファイルを使用します」を ON にします。このボタンに気づくまでに時間がかかってしまいました…( ´・ω・`)]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[VSCode の Multi-root workspace]]></title>
        <id>https://developer.feedforce.jp/entry/2017/11/14/121634</id>
        <link href="https://developer.feedforce.jp/entry/2017/11/14/121634"/>
        <updated>2017-11-14T03:16:34.000Z</updated>
        <summary type="html"><![CDATA[元 Vim 使いの id:tmd45 です。今年諸事情で VSCode に乗り換えたばかり。Version 1.18.0 の新機能 Multi-root workspace が仕事で便利というだけのお話。code.visualstudio.com関わってるプロダクトの関連するリポジトリがたくさんあるので（バックエンド、フロント、インフラ、その他…）まとめられるのはありがたい。 git 関連の拡張をいろいろ入れてるので、どこまで標準機能か分からなくなってますが、このソース管理タブもいい感じですね。最近さわってないリポジトリが一目瞭然だずぇ…]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[【2017/11/16 に訂正を追記しました】 社内 LT 大会で「ここがつらいよ ECS」というタイトルで発表しました]]></title>
        <id>https://developer.feedforce.jp/entry/2017/11/13/183623</id>
        <link href="https://developer.feedforce.jp/entry/2017/11/13/183623"/>
        <updated>2017-11-13T09:36:23.000Z</updated>
        <summary type="html"><![CDATA[[追記] この記事の内容について訂正この記事内、及び Speaker Deck に投稿したスライドの中で誤っていた箇所があったため、訂正致します。「ECS Optimized AMI では ecs-agent のバージョンが固定されない」という内容ですが、そういった問題はありませんでした。AWS の方から直接アドバイスを頂いたところ、弊社が使用していた User Data のスクリプト内で $ yum update を実行していたことが原因となっていました。$ yum update によりインスタンスを新規に立てた際に常に最新の ecs-agent や Docker がインストールされていました。そのため、ECS Optimized AMI によってインストールされる ecs-agent と Docker のバージョンは以下のドキュメントで提示されているバージョンが常にインストールされることになります。docs.aws.amazon.comスライド中でも紹介しているように、一番困っていた問題が解消されたため AWS のサポートの方には非常に感謝をしております。誤った情報を公開してしまい、申し訳ありませんでした。こんにちは、バックエンドエンジニアの tsub (id:tsub511) です。先日の社内 LT 大会にて、「ここがつらいよ ECS」というタイトルで発表してきました。社内 LT 大会の記事についてはこちらをご覧ください。developer.feedforce.jp私が発表したスライドはこちらです。せっかくですので、スライドにて紹介している「第一位 ecs-agent と Docker のバージョンが勝手に上がる」についてもう少し詳しく解説をしたいと思います。ECS を用いたバッチシステムの運用について弊社では Amazon ECS を用いたバッチシステムを運用しています。Amazon ECS を用いたバッチシステムについての詳細は以前弊社の新卒エンジニアが書いてくれたので、こちらの記事をご覧ください。www.wantedly.comecs-agent についてecs-agent とは、Amazon ECS にインスタンスを認識させるために動かす必要のあるエージェントです。github.comDocker イメージが配布されていて、通常はコンテナとして立ち上げます。ECS Optimized AMI を利用していれば、インスタンスを起動したタイミングで勝手に立ち上げてくれるので、特に意識せずとも ECS を使えると思います。docs.aws.amazon.comただし、ECS においてはこの ecs-agent がコンテナの配置、監視などを行っているため、かなり重要な役割となりますので、無視してはいけない存在です。ecs-agent のバグによりいくつかのタスクが起動しなかった以前、以下の Issue で取り上げられている ecs-agent v1.14.2 のバグにより ECS でいくつかのタスクが起動しなくなっていました。github.com2017/06/08 に ecs-agent を全コンテナインスタンスで v1.14.2 にアップデートしたことにより、06/08 から 06/13 にかけて 6 つのタスクが PENDING 状態のまま止まっていてインシデントが起きてしまいました。この時は、上記 Issue でも書かれているように、一旦 amazon/amazon-ecs-agent:latest イメージにバグが発生する以前の v1.14.1 を Push し直してくれたことで、バージョンをロールバックすることはできました。ただ、このような問題を再度起こさないためにも ecs-agent のバージョンは固定したいところですが、固定はできないという問題がここで発覚しました。ECS Optimized AMI では ecs-agent のバージョンが固定されないECS Optimized AMI を使っていれば ecs-agent を自動的に立ち上げてくれますが、これが少々曲者です。ECS Optimized AMI を使ってインスタンスを立ち上げた時、起動する ecs-agent のバージョンは常に最新のものが使われるのです。しかも、AMI の中に ecs-agent がパッケージングされているかと思ったら、AMI をアップデートせずとも、インスタンスを新しく起動したら最新の ecs-agent が自動的に使用されます。更に言うと、この ecs-agent のバージョンをユーザーが固定することはできず、最新バージョンしか選択肢がありません。そのため、上述したようなバグが ecs-agent に含まれてしまった場合に回避不可能になります。新たにインスタンスを立ち上げず、手動で ecs-agent をアップデートしなければ今動いてるもののバージョンが変わることはありませんが、オートスケーリングの設定をしていた場合、スケールアウトしたらそのインスタンスからは最新の ecs-agent が使われてしまう、という状況です。この回避不可能な仕様に日々悩まされています。ちなみに、Docker のバージョンも ecs-agent と同じようにバージョンが固定されていません。どう運用しているかでは、弊社ではどう運用しているかというと、一部のコンテナインスタンスにカナリアリリース的にアップデートし、しばらく最新バージョンの ecs-agent をクラスタの中に紛れ込ませて稼働させておきます。例えば 10 台のコンテナインスタンスを動かしていたとして、その内の 2, 3 台だけ ecs-agent をアップデートします。アップデート自体は AWS コンソールから可能ですので簡単です。数台だけアップデートした後 1, 2 週間ほど経ってから ecs-agent の Issue を確認して、特に大きな問題が起きてなさそうなら全台アップデートする、というような運用をしています。これで今のところ ecs-agent のバグを踏む確率は多少減ったかな、という印象です。まとめecs-agent のアップデートによりバグが入り込む可能性があるECS Optimized AMI における ecs-agent と Docker のバージョン固定はできず、新しいインスタンスを起動すると最新が使われる一部のコンテナインスタンスだけアップデートし、しばらく経って問題なければ全台アップデートする、という運用をしているというわけで、今後も ECS による運用を続けていきますが、何か良いソリューションがあれば教えていただきたい次第です。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Serverlessconf Tokyo 2017 に参加してきました]]></title>
        <id>https://developer.feedforce.jp/entry/2017/11/05/111553</id>
        <link href="https://developer.feedforce.jp/entry/2017/11/05/111553"/>
        <updated>2017-11-05T02:15:53.000Z</updated>
        <summary type="html"><![CDATA[どうも、バックエンドエンジニアのサトウリョウスケです ✌︎('ω')✌︎先日のおとうふ先生の記事にもあったように、Serverlessconf Tokyo 2017 というイベントが都内で開催されておりました。developer.feedforce.jp11/3(金) のメインカンファレンスには弊社からも5名ほど参加しており、みんな大学生の頃の100倍くらい意識高く勉強して参りました ✌︎('ω')✌︎お昼ご飯に弁当出たのが嬉しかったです ✌︎('ω')✌︎あと、馴染みあるメンツでカンファレンス行くと、終わってから飲みに行けるのも良いですね ✌︎('ω')✌︎さて、今回の記事では当日の発表内容についていくつかダイジェストと感想を書いていきたいと思います。スライドはこちらのサイトでまとめられているようで大変助かります 🙏www.n-novice.comサーバレスアーキテクチャによる時系列データベースの構築と監視Mackerel という監視サービスをどのように監視・管理しているのか、というお話時系列データベースの構成Kinesis Streams へ保存Lambda で Redis へ保存Redis に一定件数溜まったら DynamoDB へ保存一度 Redis を挟んでいるのは書き込みコストを抑えるためDyanmoDB の TTL を超えるデータは S3 へ保存データの参照性に合わせて書き込み先を変更しているのはナルホド監視についてまとめメトリックを可視化して眺めよう監視の基礎は平常状態を知ること系全体の可用性を監視しようServerless を使った具体的な設計例として、とても参考になります。時系列データベースの実装として、複数のストレージを上手く組み合わせて設計されているのは色々なシーンで応用できる設計例ではないでしょうか。Java チームが選択したTypeScript による AWS Lambda 開発Slides | Riotz Works固定 IP を実現するには VPC lambda が必要VPC の lambda はすごく遅い固定 IP に対する需要は現在も一定数あるようなので。。。（日本では特に）マイクロ化が過剰で複雑になったどの程度の粒度でサービスを切り分けていくか、というのは相変わらずセンスが問われるな、という印象言語毎に実行速度がずいぶん違う一度 Java で実装して、スピードが出ずに TypeScript で実装し直したJava は初回実行時はオーバーヘッドが大きいバッチ処理のように計算量が多い処理であれば Java の方が速いようですAWS Lambdaの処理性能を言語毎に測ってみたhttp://acro-engineer.hatenablog.com/entry/2016/08/02/120000Serverlessの世界に特別なことなんて何もなかったslideship.comServerless でよくある課題と解決Functionの適切な分割・統合Functionやサービス間のデータの受け渡し外部サービスの呼び出しとエラーハンドリングテストスライドに色々な Tips が詳しく書かれているので一読すると吉ただ、紹介されている方法だと Lambda Function の粒度がかなり細かくなるので、その辺の管理は大丈夫なのか気になりましたマイクロ化しすぎ問題とかは大丈夫でしょうか？どういうサービスが Serverless に向いているのか、という話も出てくるので参考になります個人的には特性を押さえた上で、従来の Rails のようなアプリケーションと Serverless をハイブリッドに組み合わせて使うのが良いと考えていますServerlessとか言う前に知ってほしいDBのことslideship.com一個前のと同じ登壇者の方こちらは DB についての tipsいい感じに煽られていたので、来週弊社で開催される LT大会 でもこんな感じのノリを期待しています ✌︎('ω')✌︎社内LT大会準備中 - Feedforce Developer Blog非同期で並列数を制限すれば RDS を Lambda から利用しても問題ない同時接続数が爆発しないように調整して使えば OKLambda から RDS を使ってはいけない、というのがセオリーだったので、使えると言い切る人がいたのはインパクトあったまあ確かに。例えば AWS Aurora の db.r3.large だと 最大接続数が 1,000 あるhttp://docs.aws.amazon.com/ja_jp/AmazonRDS/latest/UserGuide/Aurora.Managing.html#Aurora.Managing.MaxConnections同時に 1,000 を超える Lambda が実行されなければ理屈の上では大丈夫なはず用法用量を守って正しくお使いください、というやつか。。DynamoDB でフルスキャンしたら負けこの辺は最近自分でも勉強していたので再確認しながら聞いていました（宣伝）developer.feedforce.jp真のサーバレスアーキテクトとサーバレス時代のゲーム開発・運用他の発表と被ってしまったので、当日の講演は見られなかったのですが、ブログの方を見たらとても興味深い内容だったのでご紹介しておきます🙏gs2.hatenablog.com実は、私もSaaSはサーバレスなのか？という事に対しては、ちょっと思うところがあります。私はフルマネージドサービスはサーバレスだと思いますが、マネージドサービスはサーバレスではない。と思っているためです。また、別の言い方をすると、スケールに限界があるモノはサーバレスではない。と思っています。つまり、使用方法さえ間違えなければ《勝手に》《無限に》スケールするフルマネージドサービスこそがサーバレス。と言えるのではないか。と思っています。SaaS の運用・開発してる人だと結構重要なテーマなのではないか、と思います。SaaS なんだからリクエストどんだけ送っても向こう側で良きに計らってくれるやろ。そう思ってた時期が俺にもありました 😇とは言え、SaaS 利用者からそう見えるようなサービスにしたい、という思いはあります。SaaS 利用者としても SaaS の裏側のことは一切考えずに利用したいと思うので。。弊社の ソーシャル PLUS も SaaS ですが、利用して頂いているサイトがイベントなどでアクセスが急騰するケースがありますので、開発・運用ではそういう点に気を遣っています。コールドスタート対策コールドスタートとはLambda は初回呼び出し時やしばらく呼ばれなかった後に呼ばれたときは response time が長くなる1 つの Lambda Function に全ロジックを入れるAPI Gateway のエンドポイント毎にどのロジックを実行するかパラメータで渡しているコール比率の低いエンドポイントでもコールドスタートを回避できるパラメータで動作が変わるRails の Routing のようなものと佐藤は解釈しました一定間隔で Lambda を起こすように Invoke させる方法もあるが、個人的には Routing やらせる方式の方が良いのではないか、という気がするLambda Function が大量に作られてしまう（マイクロ化しすぎ問題）と管理が難しくなるのではないか、という思いもあってこの方法は実際に試してみたいです所感Serverless に限ったことではありませんが、近年登場する新技術はトレードオフな側面が強いように感じています。一昔前は今までは解の無かった技術的課題を解決する形で新しい技術が登場する、というケースが多かったのではないでしょうか？対して今は、既存の技術でもできなくはないけど、特定のケースで困るから、それを解決する新しい技術が登場する、というケースが多いような。そして、その特定のケースを解決するために、一部のことは許容しなければならない、という印象です。（まあ単純に僕も歳をとって、保守的な考え方が強くなってきただけなのかもしれません。。）今回のカンファレンスは実際の開発者からどういうトレードオフがあるか、という話が出てきたことで、自分の中で改めて Serverless と向き合う覚悟というか、モチベーションが出てきたように思います。とはいえ、個人的には、従来の Rails アプリと Serverless をハイブリッドに使った設計に取り組んでいくのが現時点での最適解ではないかと感じています。もちろん、設計を考えた上で Full-Serverless が最適となれば、そういうアプリを作って行くつもりですが、それなりに複雑なロジックを考えるにはまだ Full-Serverless は早いのではないかな、と思います。やはり並列性の高さが Serverless の魅力なので、アプリケーションの基本的な部分は従来通り Rails で作成して、アクセス数が急にスパイクするような場所を局所的に Serverless にするような設計をこれから色々試していこうと考えています。ただ、Serverless のコンセプトとしては、ソフトウェア開発の生産性そのものを向上させることが目的とのことだったので、将来的にはハイブリッドよりも Serverless に振り切った設計がベストになっていくかもしれません。今後の発展に期待しています。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamoid の使い方【range 編】]]></title>
        <id>https://developer.feedforce.jp/entry/2017/11/04/235323</id>
        <link href="https://developer.feedforce.jp/entry/2017/11/04/235323"/>
        <updated>2017-11-04T14:53:23.000Z</updated>
        <summary type="html"><![CDATA[どうも、バックエンドエンジニアのサトウリョウスケです ✌︎('ω')✌︎最近こうして 弊社の tech ブログが移転した 訳ですが、自社で管理してるブログだと投稿フローがめんどくさいと僕がボヤいたのが移転理由の一端だったりします 😎でも移転作業したのは僕じゃなくて、球だけ投げてどっか行きました 😎移転ありがとうございます 🙇移転して一発目の投稿なので張り切って参ります 💪さて、Rails で DynamoDB を利用する際の ORM として dynamoid があります。今回は dynamoid から Hash-Range Table (Partition Key と Sort Key の複合) を利用する方法について紹介します。dynamoid の導入方法については以前書いたこちらの記事を参考にしてみて下さい。tech.feedforce.jpHash-Range Table ってなんぞその前に名称の整理をしておきますタイトルに 【range 編】と書いているのですが、これは Sort Key の事を指します。どうやら DynamoDB は初期の頃と現在で一部の名称が変化したようです。しかし、 Dyanmoid では相変わらず旧名称のまま (hash_key, range_key) でパラメータを指定するので、対応表を記載しておきます。 旧名称  現名称  Hash Key  Partition Key  Range Key  Sort Key DyanmoDB には 2 種類のプライマリキーがあるこちらのスライドが分かりやすいのですが、 DynamoDB のテーブル定義として Hash Table と Hash-Range Table というものがあります。  Hash TableHash Key (Partition Key) という一つのカラムの値でプライマリキーを表現するテーブルこの構成だと Hash Key は 重複させることができないHash-Range TableHash Key と Range Key (Partition Key,  Sort Key) の二つの値でプライマリキーを表現するRange Key が異なっていれば、同一の Hash Key を持つレコードが複数存在しても良いスキャンより高速なクエリ 1 で複数のレコードを取得することが可能スキャンだと物凄くコストが高いので、基本的にクエリだけでデータ取得できるように設計すべきRange Key での昇順・降順でのソートが可能Range Key に対しての 範囲検索 も可能dynamoid での利用方法テーブル定義class User  include Dynamoid::Document  table name: :users, key: :hash_key  range :range_key, :string # <= これendrange :(フィールド名), :(データ型) で Range Key の定義が可能です。ちょっと試せていないのですが、AWS コンソールからだとテーブル作成時にしか Range Key (ソートキー) を定義できないので、既に存在しているテーブルに途中で range の定義を加えても動作しないと思います。使い方Dynamoid にも ActiveRecord と同じように #where というメソッドが実装されています。ドキュメントでは内部でどのような動きをするのかが見当たらなかったので、実装から確認したのですが、検索条件に Hash Key や Range Key が含まれているかどうかを判断して、クエリが使える場合はクエリで検索してくれるようです。User.where(hash_key: 'hash_key') # クエリで検索されるUser.where(hash_key: 'hash_key', range_key: 'range_key') # クエリで検索されるUser.where(name: 'name') # Hash Key が無いのでスキャンが実行されるただし、引数の指定方法や定義の仕方が少しでも間違っていると #where でスキャンが実行されてしまっているケースがあります。本当にクエリ検索されているか、念のため Rails のログ出力を確認し、スキャンが実行されていないかどうか確認するようにして下さい ⚠️#where の使い方は ActiveRecord とほぼ同じです。User.where(hash_key: 'hash_key').all # => [#<User:0x000000076ed848>, #<User:0x0000000779abb0>, ...]User.where(hash_key: 'hash_key').each do |user|  user # => #<User:0x000000076ed848>endUser.where(hash_key: 'hash_key').first # => #<User:0x000000048cb050>User.where(hash_key: 'hash_key').last # => #<User:0x000000048cb050>そして、 range_key に対して gt, lt, gte, lte, begins_with, between の演算子が使用できます。User.where(hash_key: 'hash_key', 'range_key.gt': 123)User.where(hash_key: 'hash_key', 'range_key.lt': 123)User.where(hash_key: 'hash_key', 'range_key.gte': 123)User.where(hash_key: 'hash_key', 'range_key.lte': 123)User.where(hash_key: 'hash_key', 'range_key.begins_with': 'range_')User.where(hash_key: 'hash_key', 'range_key.between': [100, 200])ハマりポイントここからは Range Key を dyanmoid を使っていてハマった点をいくつか紹介したいと思います。range を定義していると #find_by_id の動作が変わる# Hash Table として利用class User  include Dynamoid::Document  table name: :users, key: :hash_keyend# OK!User.find_by_id('hash_key')# => #<User:0x000000048cb050># Hash-Range Table として利用class User  include Dynamoid::Document  table name: :users, key: :hash_key  range :range_key, :stringend# Error!User.find_by_id('hash_key')# => Aws::DynamoDB::Errors::ValidationException: The provided key element does not match the schemaんん？？ってなったのですが、こういう事らしいです。#find_by_id は内部的には Aws::DynamoDB::Client#get_item を呼び出している#get_item は結果が一意に定まる検索条件を指定しないとエラーになるつまり range_key (primary sort key) を定義している場合は引数一つだとエラー引数に range_key を指定すれば OKLine::User.find_by_id('hash_key', range_key: 'range_key')# => #<User:0x000000048cb050># OK!has_many は Hash-Range Table に対応していないdynamoid では ActiveRecord のような has_many has_one belongs_to が定義されているのですが、 Hash-Range Table だと上手く動作しません。内部の実装を見てみましたが、 Hash Table の状態で利用することが前提となっているようでした。Hash Table であればこんな感じで利用することができます。class User  include Dynamoid::Document  table name: :users, key: :hash_key  has_many :talks, class: Talkendclass Talk  include Dynamoid::Document  table name: :talks, key: :hash_key  belongs_to :user, class: Userenduser = User.create(name: 'Taro')user.talks.create(content: 'Hello world')まとめHash Table と Hash-Range Table の違いから、 Dynamoid における実装方法についてを紹介しました。Dynamoid を利用した場合は migration を明示的に実行する訳ではないため、Rails のソースコードと DyanmoDB のテーブルの実態が必ずしも一致していないケースがある点がハマりどころのような気がします。本稿で紹介した Hash-Range Table が DynamoDB と Dynamoid 両方で正しく設定されているかをリリース前に入念にチェックした方が良いでしょう。クエリとスキャンのベストプラクティクス↩]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[github-nippou という gem を golang で書き直したという発表をした]]></title>
        <id>https://developer.feedforce.jp/entry/2017/10/16/150000</id>
        <link href="https://developer.feedforce.jp/entry/2017/10/16/150000"/>
        <updated>2017-10-16T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[こんにちは。増田（id:masutaka26）です。社内勉強会の順番が来ると、２ヶ月前くらいからソワソワしてきます。そんなわけで今回は『日報を golang で支える技術』というお題で発表しました。発表の内容以下、スライドからの抜粋です。背景など以前の社内勉強会で github-nippou という gem を紹介したひとつの ruby スクリプトからそこそこ作り込み、毎日便利に使っている先日も同僚の @ryz310 から pull request をもらって、さらに便利になったからの、golang への書き換えをした。その場でライブリリースモチベーション以前 hub コマンドが ruby から golang に移行したのを見て、一度やってみたかったrubygems の CLI を Dockerize してポータビリティを高めるのは何かが違うという気持ち@ryz310 からもらった元気当たり前だけど、全て代わりの方法を調べて実装しなければいけなかったoctokit → go-github, thor → cobra など。Assets の取り扱いも変わる良かったこと１バイナリになったのでインストールやアップデートが容易になったパフォーマンスも予想外に上がった（当社比約２倍）以前調べた時は GitHub への API アクセスがボトルネックだったので、そこまで変わらないと思っていた結果的にいろんなライブラリを使えて、良い素振りになった困ったこととにかく関数が長くなりがちgetXXX() 多くなりすぎファイル分割の意味書けば書くほど golang っぽい書き方が分からなくなるOOP ではないことへの戸惑いruby が柔軟すぎて移植は難しかった。オススメはしない疑問などclass設計で便利だった、private method の代わり設計し直すべきかclassを 使わなかった頃の JS の知見を知りたいビジネスロジックのパッケージ名はどうすれば今回は lib にした感想今後、rubygems で CLI ツールを作ることはないと思う。継続利用を考えると勧めづらいgolang は単一ファイルでの実装と実行に一番ハードルが低い気がしているもっと楽な言語あったら教えて今後の予定golang っぽい書き方にするもっとテストを書くgolang on Lambda や API サーバも学習する『スターティングGo言語』を読み切る結果↓https://github.com/masutaka/github-nippou発表の感想付箋紙に感想を書いてくれました。 は私からのコメントです。クラス設計周りの話は自分も悩んでいます ライブラリとか読んで学習します！GraphQL 対応の PR 出すぞ お待ちしております！Go の GraphQL のクライアント側はまだなさそう github.com/graphql-go/graphql を見つけました。API リクエストを 1 回に出来るのかな？CLI は ruby < golang わかる。mruby も良いかも mruby はエコシステムが大変そうで...Makefile 参考にします あざす！Makefile 全く分からない... 勉強しないと！！ 基本だったら１日くらいで分かりますよ。make はビルドを必要な時だけに抑えられるので良いです流石ますたかさん とても面白く熱量ある発表でした ありがとうございます。熱量は大事にしてます神回ですね もっと技術レベル上げていきます！刺激を受けるいい話だった 刺激を与えるのがエンジニアとして使命のひとつだと思っていますスピード up は nice 発見 ですねやっぱりはやくなるものなんだなー   速いだけでテンションが上がりますgolang 熱の高まり・・・！！ やりましょう！go モチベーションの高さの理由が分かりました！！ もう必死ですよずっと Go 書いてたのはこれだったのかー（棒） そうなんですよー（棒）Go 書きたい 今書きましょう！ゴー書いてく！！   ｱ､ﾊｲざんねん！！わたし（@ryz310）の PR はここでおわってしまった！！ またお待ちしております！（笑）バイナリ配布うれしい！ ユーザの皆様のことを第一に考えておりますCLI は golang Web やる気にはなら… 配布を考えるとそうですねー。今は GitHub でリリースしておけば、Homebrew や zplug など配布手段はいろいろありますしディストリビューション大変だなー gox で簡単に並列ビルドできるので、それほど大変ではなかったですね知見 ライブデプロイ is つらい もうハラハラドキドキですよライブデプロイメント やはり初回の CI は絶対失敗しますね...（社内 Slack の）#golang を今後ともよろしくお願いします こちらこそ！最後にとにかく設計が目下の悩みです。良い情報があれば教えてください！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[開発者ブログを移行しました]]></title>
        <id>https://developer.feedforce.jp/entry/2017/10/12/100000</id>
        <link href="https://developer.feedforce.jp/entry/2017/10/12/100000"/>
        <updated>2017-10-12T01:00:00.000Z</updated>
        <summary type="html"><![CDATA[これまで Middleman Blog + GitHub Pages で運営していた弊社開発者ブログを、はてなブログに移行しました。過去の記事は移行前のアドレス http://tech.feedforce.jp からご覧いただけます。旧ブログのホットエントリー移行をきっかけに心機一転、これまでよりカジュアルな情報公開を目指していきます。フォードフォース【新生】開発者ブログを、どうぞよろしくお願いします！]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go で s3-edit という CLI アプリケーションを作った]]></title>
        <id>https://blog.tsub.me/post/create-s3-edit/</id>
        <link href="https://blog.tsub.me/post/create-s3-edit/"/>
        <updated>2017-09-05T13:30:00.000Z</updated>
        <summary type="html"><![CDATA[最近 Rust を少し学んでいたが、難しくて少し挫折しかけたのと、結局仕事への導入を考えるなら Go のほうが既に書ける人が何人かいる、というのもあり Go を書き始めた。手初めてに欲しい CLI アプリケーションがあったのでそれをサクッと Go で書いてみた。 tsub/s3-edit: Edit directly a file on Amazon S3]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[pecoからfzfに移行した]]></title>
        <id>https://blog.tsub.me/post/move-from-peco-to-fzf/</id>
        <link href="https://blog.tsub.me/post/move-from-peco-to-fzf/"/>
        <updated>2017-05-03T00:30:00.000Z</updated>
        <summary type="html"><![CDATA[今までずっと peco を使ってきたが、そろそろ別のツールに変えてみるか…と思い立ったので fzf に移行した。junegunn/fzf: A command-line fuzzy finder written in Go自分は基本的に飽き性なので、定期的に環境を変えたくなる時期が来るのだが fzf が思ってたより良かったので紹介したい。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ブログをGKEでの運用に移行した]]></title>
        <id>https://blog.tsub.me/post/operate-blog-server-on-gke/</id>
        <link href="https://blog.tsub.me/post/operate-blog-server-on-gke/"/>
        <updated>2017-04-16T05:29:33.000Z</updated>
        <summary type="html"><![CDATA[このブログはGitHub pagesを使って公開していたが、GKEに移行することにした。はてなブログからHugo on Github Pagesに移行しましたこれを聞いて、99%の人が、HugoでHTMLファイルを生成して公開しているならわざわざサーバーなんて必要ないんじゃないか？金の無駄じゃないか？と思うかもしれない。自分もそう思う。今回GKEを使ったのはGKEとk8sでのコンテナ運用を経験したかったことが非常に大きい。会社ではECSを本番運用しているが、ECSに比べてk8sの方が良さそうな雰囲気しかないのでGKEの方も触っておこうかと思って移行した。また、今のところブログ以外に個人で運用しているWebサービス等はないため、ブログがちょうどいい題材だった。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ぼくの情報収集方法]]></title>
        <id>https://blog.tsub.me/post/how-i-collect-information/</id>
        <link href="https://blog.tsub.me/post/how-i-collect-information/"/>
        <updated>2016-12-17T07:50:00.000Z</updated>
        <summary type="html"><![CDATA[この記事はfeedforce Advent Calender 2016の17日目の記事です。前回の記事はpokotyamuによるHHKBを掃除した話でした無刻印のキーだからといってどのキーでも当てはまると思って適当にやるとものすごい罠に引っかかっちゃうんですね。さて、今回は多くのエンジニアにとって重要なキーワードである情報収集についてです。自分は多分社内ではわりと情報収集よくやってる方だと思っているのですが、自分が普段どんな方法で情報収集してるかを共有したかったので今回まとめてみました。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[フトらない話]]></title>
        <id>https://blog.tmd45.jp/entry/2016/12/10/135046</id>
        <link href="https://blog.tmd45.jp/entry/2016/12/10/135046"/>
        <updated>2016-12-10T04:50:46.000Z</updated>
        <summary type="html"><![CDATA[この記事は feedforce Advent Calendar 2016 の 10 日目の記事です。昨日は弊社営業チームから初めて Advent Calendar に参加してくれた黒い伝道師・喜多の『クリスマスまでまだ間に合う！日焼けマシンを使えば誰でも黒くモテ肌を手に入れる事ができる』でした。クリスマスに向けた Advent Calendar にぴったり（？）の記事。日焼けマシーンってサイバーな感じしますね（・∀・）今年の弊社 Advent Calendar は "エンジニア" に限らない、ということで「健康オタク」枠として、去年参加した 糖質制限 Advent Calendar 2015 のその後の話を書きます。blog.tmd45.jpダイエットではない。断じて。昨年のあらすじ 時期   2015年3月  一人暮らし再開。人生最重量 58 kg、体脂肪率 32 %。体重記録開始  2015年4月  "カロリー記録を始めて 19 日で 2.8 kg 落ちた"*1  同頃  Withings Activité（活動量・睡眠計測）利用開始*2  2015年5月  糖質制限に興味を持ちはじめて「糖質軽減」をはじめる    だいたい 1kg/月 で体重が減り続けて 調子に乗る  2015年9月  約半年の結果。体重 50.7 kg、体脂肪率 24.4 % 🎉  2015年12月  さらに3ヶ月経過。体重 51.1 kg、体脂肪率 25.4 %。冬ゆえの微増 そういえば、その後 社内で『シリコンバレー式 自分を変える最強の食事』が一斉を風靡してました。実際に社長が体質改善に成功してました。人それぞれ体質的・性格的に合うものを選ぶのが一番良いですね（感想）。以前の記事でも書きましたが、カロリー記録は続けていません。だいたい普段食べるもののカロリーや成分（炭水化物／糖質）の傾向が見えてきたとこで終わりにしました。カロリーの制限もしていません。むしろカロリーオフの商品とか困る。活動エネルギーはとりつつ、糖質を控えたい。ただ糖質（炭水化物）のコスパが良すぎるんですよね。食事にコスパを求めると必然的に糖質メインに摂取することになるかと思います。ゆえにこの二年、食費は以前よりかかってる気がしますヽ(；´Д｀)ノ一年ぶりの報告記事ですさて前置きが長くなりましたが、その後のご報告です。【祝】リバウンドなし【維持】褒めてくれていいのよ( ･´ｰ･｀)＋去年サボった健康診断を、今年はちゃんと行ってきたんですが、看護師さんたちに「この体重は意図して減らしたんですか…？」「なにかありました…？（心配」「これ以上減らさないでくださいね（叱」って言われました。わーい（？）安心してください、ブラックじゃないです（意味深）なにより血液検査（代謝）の結果、脂質の項目がすごく良くなった。2年前との比較ですが中性脂肪: 143 mg/dL → 40 mg/dL（むしろ減りすぎ）HDL-コレステロール: 59 mg/dL → 67 mg/dL（善玉コレステロール微増👍）LDL-コレステロール: 156 mg/dL → 89 mg/dL（悪玉コレステロール大幅減🎉）これはびっくりした。あと糖質軽減してても血糖値はほとんど変わりなしでした。意外。健康診断は毎年受けましょうね( ◠ ◡ ◠ )考察糖質軽減で自分が成功したのは、もともとの体重増加の原因が、不必要な糖質の取りすぎによるものであったからだと思います。先にも書きましたが、体質は人それぞれなので、太っていると感じるならその太っている原因も人それぞれ。原因に対する対策も人それぞれです。ダイエット 体質改善は流行り廃りに乗ってやっても、当たりハズレに振り回されるだけになりかねないので、自分の体質をよく観察して自分に合うものを選びましょう（真面目）最近のこと体重の減少があまりなくなって、維持の時期になってからは去年よりも多少糖質摂取量は増えました。ガンガン減ってるときはそれが楽しくて「せっかく体重減ってるのにいまここで食べちゃったらもったいないな」みたいに思って、あまり「食べたい！」みたいな気分にもならなかったですし。最近はご飯とか甘いものとかも食べたくて食べるようになってきました。そこで気をつけているのは、とにかく先に野菜をお腹にいれておくということです。まず野菜。野菜。野菜。ここ最近は「糖質警察」より「野菜先に食えババア」になりつつあります。野菜ジュースでもいいけど、砂糖や果実で味を整えてるために糖質の高いものもあるので気をつけて😉筋トレも暇な時にやるような習慣になりつつあります。まぁ真面目にはやってないので筋肉がつくほどではないですが… 通勤で歩く距離を伸ばしたり、地味にやっております。おわりによくあんな仕様が曖昧でログ出力も無くて運用で回避している箇所がたくさんあるシステムの面倒を見てくれているよなって思いますよね…— かせいさん (@kasei_san) 2016年12月10日健康診断は定期的に受けましょうね（2度目）。いかがでしたでしょうか。明日は Twitter アカウントは持ってるけど発言数が少なくて基本的に「おもしろいこと」しか公開しないタイプの @Lorentzca による 2 記事目『分散惑星間データセンター(Grid Interplanetary Data Center)時代のデータ通信技術』の話だそうです。真面目か…？！乞うご期待。追記こやつｗｗｗｗｗｗｗ（いい話でした ☺）去年より個人ブログのポスト数を2倍くらい増やせたのでなぜなのか書く*1:ちなみにカロリー記録は iOS アプリの『MyFitnessPal』を使っていました。*2:その前は『fitbit one』ユーザでした。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blox Introduction]]></title>
        <id>https://blog.tsub.me/post/blox-introduction/</id>
        <link href="https://blog.tsub.me/post/blox-introduction/"/>
        <updated>2016-12-08T15:00:00.000Z</updated>
        <summary type="html"><![CDATA[この記事はDocker Advent Calendar 2016の9日目の記事です。先日AWSのre:Invent 2016でBloxが発表されました。BloxはEC2 Container Service(ECS)関連のオープンソースのツール群のことです。そしてそのツールとは主にECSのカスタムスケジューラを指しますECSはマネージドなスケジューラとマネージャを標準で備えていますが、Bloxはそれとは別に自分でホスティングする必要があります。しかし、ECSに足りない機能を補ってくれるため導入するメリットは大きいでしょう。先日リリースされた、CloudWatchEventsのECSイベントストリームを利用することで、よりスムーズにECSのクラスタの状態を監視してカスタムスケジューラを作ることができるようになりました。Bloxはこれを使った一例と言えますこの記事ではBloxについて試してみて分かった内容や所感について書いていきます]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[はてなブログからHugo on Github Pagesに移行しました]]></title>
        <id>https://blog.tsub.me/post/created-blog-by-hugo/</id>
        <link href="https://blog.tsub.me/post/created-blog-by-hugo/"/>
        <updated>2016-08-11T16:01:16.000Z</updated>
        <summary type="html"><![CDATA[はてなブログをやめて、Hugo on Github Pagesに移行しました。といっても、走りだしのブログであまり記事は多くないんですが..移行した理由は、以前のブログを構築した際に、調子に乗ってはてなブログProに登録して独自ドメインを使っていたのですが、思ったよりも記事を書かずお金がちょっと勿体無いなーと思い始めてきたのでGithub Pagesに移行しました。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[neovimのterminal emulatorが便利すぎた]]></title>
        <id>https://blog.tsub.me/post/neovim-on-terminal-emulator/</id>
        <link href="https://blog.tsub.me/post/neovim-on-terminal-emulator/"/>
        <updated>2016-07-02T13:08:23.000Z</updated>
        <summary type="html"><![CDATA[少し前にvimからneovimに移行したのですが、vimよりさくさくな気がする、程度でneovimの機能を特に活用していませんでした。実はneovimにはterminal emulatorという機能があり、vimの中でshellを起動することができます。例えばコードを書きつつ、rspecを実行したりpryやtigを使ったりなど、非常に便利です。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[tokyo.ex #3 参加してきた]]></title>
        <id>https://blog.tsub.me/post/tokyo-ex-3-entry-report/</id>
        <link href="https://blog.tsub.me/post/tokyo-ex-3-entry-report/"/>
        <updated>2016-06-30T14:56:16.000Z</updated>
        <summary type="html"><![CDATA[tokyo.ex #3 に参加してきました。tokyo.ex #3前々からtokyo.ex #1, #2と気にはなっていたんですが、気づいた時には定員が埋まってまして今回やっと参加できました。と思ってたらわりと席空いてたりキャンセル多かったり、定員超えてるからといって諦めなくても良かったみたいですね参加してみての全体的な感想ですが、正直最近elixirを触ってなかったのでいい刺激になりました。話の内容は非常にレベルが高く、大半は理解できませんでしたが、その分elixirの勢いとコミュニティの熱さは十分伝わってきました。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[serverspecで複数のdocker containerに対してテストしたい]]></title>
        <id>https://blog.tsub.me/post/serverspec-for-several-container/</id>
        <link href="https://blog.tsub.me/post/serverspec-for-several-container/"/>
        <updated>2016-06-25T14:36:05.000Z</updated>
        <summary type="html"><![CDATA[前回の記事でdocker containerに対してserverspecでテストができるようになりました。serverspecでdocker containerに対してテストしたいdockerを扱う以上、containerは複数立てるのが普通です。今回は複数のcontainerを立てた時にそれぞれのcontainerに対してテストする方法について書いていきます。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[serverspecでdocker containerに対してテストしたい]]></title>
        <id>https://blog.tsub.me/post/serverspec-for-docker/</id>
        <link href="https://blog.tsub.me/post/serverspec-for-docker/"/>
        <updated>2016-06-25T13:25:08.000Z</updated>
        <summary type="html"><![CDATA[仕事でこれからdockerを使い始めるので、dockerを触りつつメモがてら記事に残していきます。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[goshiTKG]]></title>
        <id>https://blog.tmd45.jp/entry/2015/12/23/153236</id>
        <link href="https://blog.tmd45.jp/entry/2015/12/23/153236"/>
        <updated>2015-12-23T06:32:36.000Z</updated>
        <summary type="html"><![CDATA[本日3本目の更新になります。この記事は TKG Advent Calendar 2015 の 23 日目です。昨日は daruyanagi さんの『文明の利器をフル活用して oTKG に挑戦する』でした。温玉おいしそうでした。かくいう私は最近毎日お弁当に固茹で玉子を入れておりまして、それはフライパンで茹でて 10 分くらい（＋お湯にいれたまま放置 10 分くらい）です。半熟も嫌いじゃないのですが作るの難しいですね。製造機素敵です。さてさて、それではアラサー毒女の普通の TKG をどうぞ。材料うちに炊飯器はありません。諸般の事情で普段あまり白米を食べませんので。胡麻と塩昆布は正義です。構築レンジで温めた直後の輝くご飯と、ミックスサイズの卵です。お茶碗でいい感じにご飯を崩して、卵を割り入れました。軽くご飯と混ぜつつ醤油をかけます。用意した薬味類を盛り付けます。あと隠し味的にごま油をすこーーーしだけたらしました。胡麻のいい香りです。実食いつもどおりの美味しいお味です。醤油が多少古くても、塩昆布の風味のおかげで美味しい醤油っぽい味になります。胡麻の香りも最高。胡麻と塩昆布は正義です（2回目）おそまつさまでした。おわりに（おまけ）普段は朝ごはんか晩ごはんにしか TKG しないので、昼 TKG はちょっと新鮮でした。また子供の頃に食べていた TKG はこんなにいろいろ乗せたりしておらず、この辺は大人になって自分で工夫し始めた点かなと思います。胡麻と塩昆布は正義です（３回目）。普段は木綿豆腐をレンジで温めたものに、これらの薬味を乗せて食べることが多いです。というわけでタイトルは Goma と Shio-konbu の TKG でした。ひねりなし。TKG 自体、だいぶ久しぶりに食べましたが、やはり美味しいですね。おまけとして、ちょっと食べ足りなかったので酒のつまみに作ったベーコンチーズも載せておきますね。厚切りベーコン 100g（三割引）チーズ on the ベーコン and レンジで温めバジルと粗挽き胡椒 on the hot ベーコン大変美味しゅうございます。オーブンで焼いたほうがきっともっと美味しいです、が面倒なのでレンチンで済ます。明日は osapon さんが極々普通の TKG について書いてくださるようです。というか毎日違う Advent Calendar に参加・更新されててすごい。ではでは。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[さらに3ヶ月経過]]></title>
        <id>https://blog.tmd45.jp/entry/2015/12/23/130639</id>
        <link href="https://blog.tmd45.jp/entry/2015/12/23/130639"/>
        <updated>2015-12-23T04:06:39.000Z</updated>
        <summary type="html"><![CDATA[本日2本目の更新。この記事は 糖質制限 Advent Calendar 2015 の 23 日目です。前の回は Miki Shoji さんのはずですが、その前の記事は yancya さんの『yancya の糖質制限日記』でした。理性に完全に従う強い心がないと糖質警察にはなれないな、と思いました。素人（？）には 1日 40g 制限でもかなりキツイです。私は以前とりすぎていた糖質*1を減らしているだけなので、つらい思いはしていません。TKG Advent Calendar 2015 の記事執筆のためにひさびさに TKG（たまごかけごはん）を食べる予定です。前回の報告からさらに3ヶ月が経過しました。時が経つのは早いですね。約半年の結果 - TMD45'β'LOG!!!近況報告さて現在どうなってるかといいますと【悲報】冬眠準備【微増】目標値はキープしてるので、季節的にもとりあえず増加分はあまり気にしないことにします。できれば体重 50kg 切りたかった＆体脂肪率 22 % 達成したかったですが、忘年会だー年末進行だーオフィス移転だーってね。オフィス移転が、何の関係があるのかとお思いでしょうが、引っ越し祝いで良いお菓子をたくさんいただいているのです。ありがたいです。お土産の糖質はカウントしません。ありがたくいただきます*2。あと体重が増えてきたのは若干の筋肉増があったからとも考えられます。前回宣言したとおり、少しずつ筋トレ的なことも始めました。筋トレ的な話通っている鍼灸院の先生曰く、運動は歩く量を増やすのが一番健康に良いとのこと。早歩きで1日20分くらい歩きまわるのがいいそうです。そのへんは通退勤で意識しています。体重が減ったおかげか、いつしか辛くなっていた階段の利用も苦じゃなくなっておりまして、駅では階段を使うようにしたりと初歩的なことをやってます。あと同じく鍼灸の先生に言われたのが、腰痛などを防止するために脚〜お尻の筋肉をつけるとよいという話。なのでお尻の筋トレをやってます。iPhone アプリで週に2,3回。Runtastic Butt Trainer- Runtastic これが短時間でできるわりに、軽く筋肉痛になるくらいで自分にはちょうどよかったです。室内で出来る程度の動きなのもありがたい。飛んだりはねたり、単身者の賃貸で出来ないですからね…あ、先生といえば、鍼灸の先生に「筋肉少ししっかりしてきましたね」って褒められた！まだまだ頑張るぞ！— たま●2日目東ウのどこか (@tmd45) 2015, 11月 14あとお腹の筋肉も良い感じになってきてると褒められた_(:3 」∠)_— たま●2日目東ウのどこか (@tmd45) 2015, 11月 28他人からフィードバックを貰えるのは励みになります。隔週で通ってるのですが、もう先生に褒めてもらうために続けているようなものです。おわりにはい、そんなわけで。まとまり無いですが、自分の「糖質制限」は無理なく続けていきたいと思います。冬になったのでまた「きのこ＋葉物野菜＋もやし＋豚肉」アンド「豆腐」の日替わり出汁鍋習慣が始められそう。簡単だし美味いしいっぱい食べられるのでおすすめです。明日は…あれもう終わり？まだ明日・明後日（それ以外にも…）空きがありますので、興味持たれたかたは参加してみてはいかがでしょうか！糖質制限 Advent Calendar 2015 - Adventarではでは。追記24 日目 snaga さんが更新してくださいました！『A Hacker's Memorandum — 減量生活2015』です。*1:主にじゃがいもとパスタと菓子類。あと白米の常食はやめましたけど、たまに食べてます。*2:以前、人様からいただいたものに「糖質だ糖質だ」と言ってましたが大変失礼だったと反省しております。まぁいまも言うだけは言うんですが（大変失礼）]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[受付システムつくってみた（仮）]]></title>
        <id>https://blog.tmd45.jp/entry/2015/12/23/112220</id>
        <link href="https://blog.tmd45.jp/entry/2015/12/23/112220"/>
        <updated>2015-12-23T02:22:20.000Z</updated>
        <summary type="html"><![CDATA[この記事は フィードフォースエンジニア Advent Calendar 2015 の 23 日目の記事です。昨日は kano-e  さんの３本目の記事で『自分が文章を書く時と、開発してる時って、実は同じようなこと考えて作業を進めてる』というお話でした！わかる（迫真）「半年で約8kg痩せて筋肉ムキムキ(予定)になった話 OR (運用されていれば)受付システムの話」という予定でしたが、前者は後ほど 糖質制限 Advent Calendar 2015 に寄せる記事で書こうと思いますので、ここでは後者の話をすることにします。ただ残念なことにまだ運用は始められていないので、とりあえず作ることにした経緯などエモーショナルい話をしたいと思います。オフィス移転するで（したで！）blog.feedforce.jp今年の 10 月下旬、オフィスの移転の話がはっきりしてきた頃にこんなメモをガーッと書いて、社内の Slack に URL を放流。弊社には社内のエンジニア有志が続けている『もくもく会』*1がありまして、自分もそこで作ってみよう (๑•̀ㅂ•́)و✧ と先のメモをもって宣言してみました。Qiita のメモを公開した日（もくもく会開催の木曜日）は「とりあえず Rails で作ろうかなー」「まずは Heroku で動けばいいや」「社員を管理するモデルとー」みたいな感じで想像を膨らませていただけだったのですが、その翌日 。できてる…だと…？！ (； ･`д･´)ゆるふわ銃殺系フロントエンド芸人 @habu0104 兄貴により、だいたいの実装が出来上がっていました。なんということでしょう。スクショが無いんですが、ちゃんと画面側もいい感じに出来上がっていました。いやー、ほんとフロントエンドすごい。JavaScript 界隈すごい。自分の手が遅いのは多分にあるんですが、このスピード感はほんとすごいなぁ。…と関心するとともに、ちょっと心が折れた（笑）わたしのかんがえる さいきょうのスピード感は完全に負けたわけですが、いろいろと細かいところで「わたしのかんがえる（ry」イメージがありまして、それを叶えるために遅ればせながら自分でも実装を開始。Rails セットアップして〜 URL とモデル設計して〜…なんてちまちまこっそり、private repository で作り続けていたのです。が。Kitayon | オフィス向け受付アプリ Kitayon（キタヨン）ぎゃああああああああああああああ :(；ﾞﾟ'ωﾟ'):風呂グラマー masuidrive 氏のプロダクトβ発表…なんというタイミング…*2これはもう私個人がひとりで作ってるヘボ受付システムなんていらないじゃん。もうこれでいいじゃん。心が折れた（2回目）わたしのかんがえる…さい…きょうの…（涙）スピード感、大事。でもでも「わたしのか（ry」イメージを叶えたい一心で実装を続けました。俺得…俺得でいいんや…というわけで、移転には間に合いませんでしたが、現在なんとか「受付システム」っぽい感じには仕上がっております。  まだまだ思い描いている機能の半分もできてないのですが、とりあえず受付できるレベル。恥ずかしながらソースコードも公開プレイ始めました。tmd45/knock-on年明けくらいには本番環境を作って、社内でテストを始めたいなと思っております。 その前にはやく Kitayon のβ運用始まらないかな。目下の悩みはいま社内に転がってる iPad が初代で iOS 5.1.1 からバージョンアップできないことですかね。とりあえず動きはしましたが Safari で CSS Animation が動かなくて悲しい気持ちになりました。iPad を買ってもらえるように頑張りたいと思います (๑•̀ㅂ•́)و✧おわりにシステムのつくりの話には全く触れませんでしたが、いかがでしたでしょうか。明日はフィードフォースの年中サンタクロース hoshinotsuyoshi（@hoppiestar）が書いてくれるみたいです。ではでは。*1:“FFもくもく会” のご紹介！ | feedforce Engineers' blog*2:ちなみに弊社もβ応募中だったと思う…運用のこと考えたら自前で作るより外部サービス利用したほうが後が楽なんですよねー。私もその後のメンテのこと考えると、こっち使いたい（ぇ]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ドラ娘論]]></title>
        <id>https://blog.tmd45.jp/entry/2015/12/13/000000</id>
        <link href="https://blog.tmd45.jp/entry/2015/12/13/000000"/>
        <updated>2015-12-12T15:00:00.000Z</updated>
        <summary type="html"><![CDATA[この記事は Kosen Advent Calendar 2015 - Adventar の 13 日目の記事です。昨日のカレンダーは空白だったようですが、前回の記事は Kosuke Ohmura さんの『高専をだらだら堕落しながら６年かけて卒業した話』でした。ボン・ソワール みなさま、@tmd45 です。今日は 4, 5年前に数回ドラ娘を勤めただけ*1で、いまは「娘」ですらなくなったアラサー独女の私が思う「ドラ娘」について書き散らしたいと思います。いつか LT か何かで話そうと思って数年経ってしまったのでこの辺で消費しておこうかという感じです*2。銅鑼は権力イベントが始まる前に事前に依頼される場合もあるだろう、会場でボァー…っと発表と聞いてたら突然スカウトされることもあるだろう。やりたいと思ってたんだ！と意気込んで銅鑼を握ることもあれば、なんで自分は発表もしないのに壇上に上がらされてるんだ…と思うこともあるだろう。LT で一番重要なルールは、とにかくどんなにイイ話でも、どんなに本人がまだ話したくても、制限時間がきたらその発表を中止しなければならないという部分である。昨今は少し余裕を持って終えたり、時間ピッタリで綺麗に終わる発表がやたら増えてきているような気がする。それはそれで素晴らしいかもしれないが、「なんか最後までよくわからんかったがガンバったな…っ」とか「言いたいことが／聞きたいことがいっぱいあるのに無情の銅鑼…！」という空気が LT の醍醐味だと、私は思う。その無情の銅鑼を鳴らすのは他ならぬドラ娘である。発表者はどんな権力や身分を持っていようと、銅鑼が鳴ったら発表を切り上げなければならない。たとえそれが先輩であろうと、校長であろうと、著名な講演者であろうと、LTの銅鑼の音の前で彼らは等しく平等であり、（タイマーと）銅鑼の音が権力を持つのだ。発表者の息の根を止める気の抜けた銅鑼の音では、発表者や聴衆の "情熱" は止められない。そんな屁のような銅鑼より、この発表をやりきりたい！面白い話を聞きたい！…そんな彼らの思いが勝ってしまう。しかし LT はテンポが重要である。それはイベントのタイムスケジュールにも影響するし、次に次にと控えている発表者たちのリズムを崩し、結果的になぁなぁな雰囲気の原因となってしまう。ドラ娘は、発表を中断するために発表者の息の根を止める、次の発表に向けるために聴衆の息の根を止める、この一打で仕留める…！そんな気持ちで銅鑼を叩くのだ。ドラ娘は楽しい権力と殺意を秘めつつ、LT はやはりその発表を楽しむ場だ。ドラ娘として壇上に上げられて緊張するだろうが、タイマーを見るのに気を取られすぎて全く発表を聞いていなかった…ということではやはり勿体無い。タイマーを忘れて本末転倒になるのはまずいが、発表自体を楽しむことはドラ娘にも許されることだ。ラスト30秒くらいから少し気を張り、10秒前になったら静かに構えに入り、時間になったら殺意を込めて打ち鳴らせばよい。また、壇上は発表者の（だいたいは）一番近くで発表を聞ける特等席である。発表を存分に楽しんで聞こう。おわりに以上が、ドラ娘を経験し、他のドラ娘たちを見てきた私の「ドラ娘論」である。今後、銅鑼を握る諸君に何か伝わるものがあれば嬉しい。…なんてね〜〜ウィルキンソンのジンジャーエールウォッカとさけるチーズうめぇ〜〜〜。うへへぇ。というわけで明日は myu_mx さんが就職して感じたことというテーマで書いてくれるそうです。*1:参考記事:「ドラ娘」て知ってる？ | 非モテタイムズ http://himo2.jp/4338000 まだこの記事残ってる(^q^) 山本ユウカ先生その節は記事ありがとうございましたフヒヒ*2:一部界隈で「ドラ娘」の存在がハラスメント問題として取り沙汰されているようですが、本記事では「LT（ライトニングトーク）の制限時間が来たら大きな音を出してトークを終了させる係のひと」という意味で伝わりやすい名詞として「ドラ娘」と表記しているとご認識ください。「ドラ娘」という存在が気に入らない場合はそっとブラウザを閉じるか、適宜お好きな表現に脳内変換してお読みください。っていうかアドベントカレンダーにそんなヘビーな世論持ちだされても困るでござるよニンニン。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sphero と遊ぼう！にゃんこ編～僕にペットはいないけど #sphero #gosphero]]></title>
        <id>https://blog.tmd45.jp/entry/2013/12/08/135230</id>
        <link href="https://blog.tmd45.jp/entry/2013/12/08/135230"/>
        <updated>2013-12-08T04:52:30.000Z</updated>
        <summary type="html"><![CDATA[この記事は Sphero Advent Calendar 2013 のエントリーです。前回は 12 月 5 日、jishiha さんの『SpheroのイケてるPV動画7連発』 でした。jishiha さんにはその前日2日間も、Scratch から Sphero を動かす記事を投稿していただいてます！ありがとうございました。Sphero を Scratch(スクラッチ)から動かせるようにしたのでこどもでもプログラミングできるよ - 僕は発展途上技術者スクラッチからSpheroをブログラミングできるScratch2Spheroを改良しました - 僕は発展途上技術者さて今回は、せっかくの休日担当ですし Sphero を持ってピクニックにでもと思ったのですが、肝心の Sphero を会社に忘れてくるという大失態をやらかしましたので、撮ってくるはずだった動画をネット上から探してお茶を濁したいと思います。やだ苦～い。ちなみに「わんこ編」はありませんヨ、猫派だから。犬動画は犬派な方がまとめてくれればいいと思う。まずは公式からSphero + Cats - YouTube＼にゃうぉー／ウカカカしてる子もいますね。Sphero 本体これはプロトタイプ版か何かなんでしょうか、全部透明ですね。やっぱりスケルトンは面白そうです。世界の猫たちSchnurrli and the Sphero - YouTubeだらーん…ケリケリケリケリ…だらーん。Kittens playing with Sphero! - YouTube子猫も夢中。You Tube で sphero cat で検索して癒やされよう。癒やされた。ちょっと絞り込みが難しいのでここに貼るのは断念しましたが、instagram でもたくさんの Sphero 動画が上がってます。猫動画もちらほら。INK361 というサイトです。tags #sphero photosさらにふたたび公式から…Nyan！Sphero Nyan Cat SpaceParty - YouTubeNYAN-CAT…いったい何者なんだ…！NON-STOP NYAN CAT!猫だけど、ペットではないな。動画にもあるとおり、Sphero のゲームで NYAN-CAT を使ったものがありますよ。Nyan Cat - Space Party! on the App StoreSphero Nyan Cat スペースパーティ! - Google Play の Android アプリさて次回は、1日空いて() kenji.horie さん、かな？]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[ころころ可愛い♡なのに高性能！Orbotix Sphero 2.0 が面白い #sphero #gosphero]]></title>
        <id>https://blog.tmd45.jp/entry/2013/12/01/001418</id>
        <link href="https://blog.tmd45.jp/entry/2013/12/01/001418"/>
        <updated>2013-11-30T15:14:18.000Z</updated>
        <summary type="html"><![CDATA[この記事は Sphero Advent Calendar 2013 の1日目のエントリーです！最初の一日目そしてこれで最後になるかもしれない一日目は、Spheroについてご紹介をしたいと思います。念のため言っておきますがステマ記事では無いですよ～。個人的なガチマです。Sphero ってなに？Orbotix社製、ポケットに入るボール状のモバイルゲームロボットです。公式サイト（英語）公式サイト（日本語）キャプチャは日本語サイトのトップから。見た目にも未来を感じます。…英語のキャッチコピー、日本語にするとすごく胡散臭くなるのはなんででしょうか。英語でも胡散臭いのかな。無断階で変化するカラーLED。非接触式充電。階段から落ちたり、水の中に入れても大丈夫な衝撃・防水対応。Bluetoothで接続してiOS/Androidアプリから操作が可能です。2.0 は旧Spheroに比べてスピードアップ・機能性アップがされたとのこと。手に入れる前は、どうせ停止動作とかぐだぐだで、自転に負けてすぐに止まったりはしないだろうとゆるく考えていたのですが、これがなかなかピタッと止まってびっくりしました。スピードもかなり出せる（ゆっくりもできる）ので、おもいっきり遊ぶには広い室内や公園などの外がおすすめかも。ペットを飼っているひとには、ペットのいい遊び相手になってくれるかもしれません（そんな動画もいっぱいあります）。ちなみに、手に入れるきっかけは。自社の勉強会でちょこっと名前が出てきた程度だったのですが、真っ白でカッコイイ見た目が気になって、PVを見て、気づいたら買ってました٩(๑❛ᴗ❛๑)۶どんなふうに遊べるの？公式PVもカッコイイのですが、こちらのレビュー動画のほうが機能などはわかりやすいかも。Review: Sphero 2.0 is a brighter, faster, smartphone-controlled ball of fun | Macworld公式アプリはiOSのもので25種類あります。Andoroid版は見れてないのでわかりませんが、同じだけあるはず。単なるコントローラーだけでなく、画面に線を書いてそのとおりに走らせたり、画面内でAR（仮想現実）と組み合わせたり、Sphero自体をコントローラーにしてシューティングゲームをしたりできるアプリが揃っています。Sphero公式アプリ一覧この中で『orbBasic for Sphero』というアプリはスマートフォン上で orbBasic という言語を使ってSpheroの動きをプログラミングできます。Sphero MacroLab』を使いましょう。こちらは「用意されている定型文を組み合わせる」だけで好きな動きに調整することができます。さらにカスタマイズしたい人は！開発者向けページから Sphero の SDK が入手できます。公式で iOS, Android, Unity, Windows 8.1。非公式のもので Node や Windows Phone、Ruby や Python のライブラリもあります。Bluetooth接続できる環境があればよく、私も十数分で MacBook Air から Ruby のコードをポチポチして動かすことができました！その辺については次の記事で先輩が書いてくれるはず…！いろいろ夢がひろがりんぐガジェットです。どこで手に入るの？日本ではAmazonやSoftbank Selection、AppBank Sroreから購入できます。その他は販売店舗のページからチェックしてみてください。お値段、約1万5千円…高いと取るかはひとそれぞれ…。さらにApple StoreではApple限定バージョンとして、本体（と充電クレードル）の一部がスケルトンになっているモデルが販売されています。AppleモデルのPVはなんかズルい！Sphero 2.0 Revealed: Exclusively at Apple - YouTubeお子様をお持ちのお父さま・お母さま、息子さんがギークに育ちますよ。どうですか。私はAmazonで購入しましたが、会社の先輩がApple版を買っていて、大変悔しい思いをしました。中身のモーターなどが見えるので面白い！つい最近、日本でもSphero用のゴム製カバー Nubby Cover が発売されました。室内で遊ぶときにカーペットのシワに負けて（ツルツルすべってしまって）進まないことがあるのですが、でこぼこのついたカバーでそういう心配もなくなりますね。屋外で遊ぶときの傷防止にも！最後にいかがでしょうか。みなさん欲しくなってきたんじゃありません？2日目の記事は kano-e さんの『Ruby - Sphero gem で Sphero を操作してみる』です。おたのしみに。]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[超速！『FuelPHP Advent Calender 2012』が電子書籍化されました！ #FuelPHP]]></title>
        <id>https://blog.tmd45.jp/entry/2012/12/26/152345</id>
        <link href="https://blog.tmd45.jp/entry/2012/12/26/152345"/>
        <updated>2012-12-26T06:23:45.000Z</updated>
        <summary type="html"><![CDATA[前回の記事で参加させていただいた『FuelPHP Advent Calender 2012』が昨日、12月25日に無事完走しました！参加されたみなさまお疲れさまでした。そんな昨日の今日というすごいスピードで、達人出版会から電子書籍化までしていただきました！FuelPHP Advent Calendar 2012【電子書籍】FuelPHP Advent Calendar 2012参加有志達人出版会発行日: 2012-12-26対応フォーマット: EPUB, PDF詳細を見る 取りまとめをされていた @kenji_s さん、達人出版会の高橋さん、本当に有難うございました！この書籍は、ブログの記事を Markdown 化、Markdown からツールで ReVIEW（電子書籍のためのマークアップの１つ*1）化して、GitHub 上で原稿を取りまとめて作業するという方法で作成されました。各執筆者は GitHub 上で pull request を利用して原稿の修正が可能です。私も記事の一部に修正があったのでその方法で反映していただいています。 興味があるかたは、GitHub 上の原稿リポジトリを眺めてみても面白いかもです。kenjis/fuelphp-advent-calendar-2012 · GitHub 主題の FuelPHP の記事も、いい感じの Tips がたくさん詰まっていて使い手あります。去年よりも、実際に動かして云々系の記事が多い気がします。無料です。DL して損はないです！ちなみに去年の『FuelPHP Advent Calendar 2011』は技術評論社から電子書籍化されています。こちらは EPUB のみですがやっぱり無料です。FuelPHP Advent Calendar 2011 | Gihyo Digital Publishing まだまだ継続してる Advent Calendar もあるようですが（笑）クリスマスも過ぎてもうすぐ年の瀬です。みなさまお風邪など召されませんように、良いお年をお過ごしください！ ('ω`)ノ関連記事電子書籍『FuelPHP Advent Calendar 2012』が達人出版会より出版されました！ - A Day in Serenity @ kenjisFuelPHP Advent Calendar 2012 が出版されました『FuelPHP Advent Calendar 2012』が無料電子書籍になりました | H2O Space.*1:参考：『書籍制作フローを変える。「ReVIEW」という解。〜マークアップと自動組版と、時々、電子書籍〜』 http://kmuto.jp/events/page2012/page2012.pdf]]></summary>
    </entry>
    <entry>
        <title type="html"><![CDATA[さくらのレンタルサーバで FuelPHP を使ってはてなハイクブログを作る－ViewModel を使ってみる編 #FuelPHPAdvent2012]]></title>
        <id>https://blog.tmd45.jp/entry/2012/12/18/101053</id>
        <link href="https://blog.tmd45.jp/entry/2012/12/18/101053"/>
        <updated>2012-12-18T01:10:53.000Z</updated>
        <summary type="html"><![CDATA[この記事は FuelPHP Advent Calendar 2012 の 18 日目の記事です。昨日は @ootatter さんによる『fuelphpで既存DBをあつかうとか』でした。本日の担当は、毎年クリスマスには仕事で問題が起こりクルシミマス恒例になりつつあります、@tmd45 です。FuelPHP 標準のサーバキャッシュと「はてなハイクAPI」、ViewModel を利用したブログのようなものを作ってみましたので、その全貌を公開したいと思います。さくらのレンタルサーバーで FuelPHP を動かすViewModel を含めて MVC をひととおり使ってみるはてなの API を使ってみるなどを行っています。長いですが、マイナーな組み合わせすぎて需要が無いことをひそかに期待してます（笑）。では、よろしくお願いします。【修正履歴】1. さくらのレンタルサーバで FuelPHP を準備する利用しているのはスタンダードプランです。なおスタンダードより下のライトプランだと SFTP や SCP が使えなくてちょっと不便なので、動的なサイトを作りたい場合はスタンダードプラン以上で契約されるのがオススメです。さらに言えば、レンサバより VPS や Cloud なんちゃら系のホスティングサービスを使ったほうが利便性は良いです。さくらのレンサバを使った理由は「そこに放置中のレンサバがあったから」以外のなにものでもないのであしからず。PHP のバージョンを確認するお使いのレンサバで動く PHP のバージョンを確認します。FuelPHP を動作させるために PHP 5.3 以上を利用しましょう。必要であれば下記の画面で設定を変更してください。php.ini の編集さくらのレンサバはコントロールパネルから php.ini の編集が可能です。cgi.fix_pathinfo = 1 の記述が必要です。リクエスト URL の取得に、cgi 版 PHP ではこの設定を行って $_SERVER["ORIG_PATH_INFO"] の利用を有効にする必要があるためです。と、説明しておきますが、FuelPHP Core 内で $_SERVER["ORIG_PATH_INFO"] を利用しているのは私が見た限りでは Input クラスだけみたいです。とりあえずこの設定はやっておきましょう。以下の記事を参考にさせていただきました。さくらレンタルサーバーにFuelPHPをインストール - 2hoursPHPフレームワークはどのようにリクエストされたURLを取得しているか? - localdiskFuelPHP を用意するFuelPHP 公式サイトから最新版（執筆時点でv1.4）の FuelPHP アーカイブ（zip）をダウンロードします。アーカイブを展開して、ドキュメントルート内の構成を以下のようにしました。アーカイブ内（変更前）のディレクトリ構成/fuelphp-1.4  |-- /docs  |-- /fuel  |-- /public  |  |-- /assets  |  |-- .htaccess  |  `-- index.php  |-- oil  |-- README.md  `-- ...ドキュメントや README、使わない oil などを削除します。また、public ディレクトリの中身をレンサバのドキュメントルートに合わせるため、www/hhblog フォルダを作ってそこへ移動します。変更後のディレクトリ構成/hhblog  |-- /fuel  `-- /www     `-- / hhblog        |-- /assets        |-- .htaccess        `-- index.phpディレクトリ構成を変更したので、index.php に記述されているパスも修正します。diff だとわかりにくいですが、/../fuel から始まるパスが /../../fuel になります。$ diff fuelphp-1.4/public/index.php hhblog/www/hhblog/index.php16c16< define('APPPATH', realpath(__DIR__.'/../fuel/app/').DIRECTORY_SEPARATOR);---> define('APPPATH', realpath(__DIR__.'/../../fuel/app/').DIRECTORY_SEPARATOR);21c21< define('PKGPATH', realpath(__DIR__.'/../fuel/packages/').DIRECTORY_SEPARATOR);---> define('PKGPATH', realpath(__DIR__.'/../../fuel/packages/').DIRECTORY_SEPARATOR);26c26< define('COREPATH', realpath(__DIR__.'/../fuel/core/').DIRECTORY_SEPARATOR);---> define('COREPATH', realpath(__DIR__.'/../../fuel/core/').DIRECTORY_SEPARATOR);Web API を使用する際にはタイムスタンプなどが重要になってくる場合もあるので、タイムゾーンの設定もしておきました。役に立っているのかはいまいち分かりませんが･･･ファイル：~/hhblog/fuel/app/config/config.php<?php/* 中略 */return array(  'default_timezone' => 'Asia/Tokyo'    // タイムゾーンを設定);またセキュリティの観点から、Web から見えてしまう FuelPHP のエラー表示は OFF にしておくほうが良いです（参考：FuelPHP でのセキュリティ対策(3) - A Day in Serenity @ kenjis）。自分の場合は開発環境と公開環境を一緒くたにするという危険極まりない遊びをしていたのでエラーは出しっぱなしです･･･(;^_^)ファイル：~/hhblog/www/hhblog/index.php<?php/** * Set error reporting and display errors settings.  You will want to change these when in production. */error_reporting(-1);ini_set('display_errors', 1);    // この引数を 0 にする。レンタルサーバへの配置ですが、今回はレンサバのアカウントホーム /home/{user-id} 直下に fuel ディレクトリを、レンサバのドキュメントルート /home/{account-id}/www 直下に www ディレクトリの中身（hhblog ディレクトリ）を配置します。公開ディレクトリは /home/{user-id}/www/hhblog です。アクセス時の URL は http://{user-id}.sakura.ne.jp/hhblog/ になります。この配置であれば、.htaccess ファイルを修正する必要はありません。アーカイブに在ったものをそのまま使用すれば Mod_Rewrite が仕事してくれます。私は動作環境を本番（production）にするため、以下の部分だけ変更（コメント化を解除）しました。$ diff fuelphp-1.4/public/.htaccess hhblog/.htaccess2c2< # SetEnv FUEL_ENV production---> SetEnv FUEL_ENV production    FuelPHP を放り込む準備したFuelPHP 一式を、SFTP でも SCP でも好きな方法で、レンタルサーバに放り込みましょう。fuel ディレクトリは /home/{user-id} へ、www/hhblog ディレクトリは /home/{user-id}/www/hhblog となるように配置します。手動インストール（oil を使わないで配置する）を行ったので、ディレクトリのパーミッションの変更も自分で行います（参考：Instruction - Installation - FuelPHP Documentation）。以下の４つのディレクトリのパーミッションを 755（rwxr-xr-x）にします。「所有者」に書込み（w）権限があればよいので、700（rwx------）のほうが安心です*1。FTP/SCP ツールで変更しても良いですし、さくらのコントロールパネルからファイルマネージャーを使用して変更することも可能です（ディレクトリを 右クリック → プロパティ で属性の変更が可能）。以下の４つのディレクトリを書込み可にする/home/{account-id}  |-- /fuel  |  |-- /app  |  |  |-- /cache  |  |  |-- /config  |  |  |-- /logs  |  |  |-- /tmp  ...これでインストールは完了です。http://{user-id}.sakura.ne.jp/hhblog/ にアクセスすれば、おなじみの Welcome ページが表示されます。おめでとうありがとう。やることが多く見えますが、結構単純です。さくらのレンタルサーバでも FuelPHP は（とりあえず）簡単に動作します。2. はてなハイク API の利用フレームワークの準備が出来ましたので、ここから実装に入ります。コードはとくに省略せずに貼り付けてますので、長いです。補足：また、筆者は Java 屋なので、PHP 的におかしな書き方をしているところがあったらごめんなさい。はてなハイクの API を利用して記事を取得する処理を Model に作成します。API は以下の２種類を使うことにしました。詳しくは API のドキュメントをご確認ください。はてなハイクのトップページ（パブリック・タイムライン）引数はとくに無く、はてなハイクのトップページに表示されるタイムラインを取得します。キーワードのエントリーページ（キーワード・タイムライン）引数にキーワードを指定（必須）し、そのキーワードのタイムラインを取得します。以下が実装です。ちなみにテストはありません（!!?）。また、一部の引数は API に合わせて定義しているだけで、利用していないです（汗）ファイル：~/hhblog/fuel/app/classes/model/api/hatena/haiku.php<?phpclass Model_Api_Hatena_Haiku extends Model{	/**	 * Using statuses/public_timeline (Japanese Domain).	 * 	 * AUTHORIZATION : no	 * HTTP METHOD   : GET	 * 	 * @link http://developer.hatena.ne.jp/ja/documents/haiku/apis/rest/timelines#public_timeline	 * 	 * @param string  $body_formats reaponse text format, see default value	 * @param integer $count        getting entries number, API default 20 max 200	 * @param integer $page         getting page number, API default 1 max 100	 * @param string  $since        no getting older than rfc1123-date("Mon, 26 Nov 2012 15:53:43 GMT")	 * 	 * @return string JSON	 */	public static function getPublicTimeLine(		$body_formats = "api,haiku,html,html_mobile,html_touch"		, $count        = "20"		, $page         = "1"		, $since        = "Mon, 26 Nov 2012 15:53:43 GMT"	) {		// リクエストURL		$url = "http://h.hatena.ne.jp/api/statuses/public_timeline.json";		// Option（UTF-8で符号化必要）		$body_formats = urlencode($body_formats);		// URL 組み立て		$filename = $url . '?body_formats=' . $body_formats;		// file_getコンテキストのオプション		$opts = array(			'http' => array(				'method' => "GET",				'header' => "Host: hoge.sakura.ne.jp\r\n" .							"Accept-language: ja\r\n" .							"User-Agent: " . Input::user_agent() . "\r\n"			)		);		$context = stream_context_create($opts);		// API の結果を json で取得		$json = file_get_contents($filename, false, $context);		// 取得した json 文字列をそのまま返却		if($json != false)		{			return $json;		}		// 取得失敗したら空文字を返却		else		{			return '';		}	}	/**	 * Using statuses/keyword_timeline.	 * 	 * AUTHORIZATION : no	 * HTTP METHOD   : GET	 * 	 * @link http://developer.hatena.ne.jp/ja/documents/haiku/apis/rest/timelines#keyword_timeline	 *	 * @param string  $keyword      seatch keyword [required]	 * @param string  $body_formats reaponse text format, see default value	 * @param integer $count        getting entries number, API default 20 max 200	 * @param integer $page         getting page number, API default 1 max 100	 * @param string  $sort         getting entries sort, API default "new" is newer, "hot" is populer	 * @param string  $since        no getting older than rfc1123-date("Mon, 26 Nov 2012 15:53:43 GMT")	 * 	 * @return string JSON	 */	public static function getKeywordTimeLine(		$keyword		, $body_formats = "api,haiku,html,html_mobile,html_touch"		, $count        = "20"		, $page         = "1"		, $sort         = "new"		, $since        = "Mon, 26 Nov 2012 15:53:43 GMT"	) {		// リクエストURL		$url = "http://h.hatena.ne.jp/api/statuses/keyword_timeline.json";		// Option（UTF-8で符号化必要）		$word         = urlencode($keyword);		$body_formats = urlencode($body_formats);		// URL 組み立て		$filename = $url . '?word=' . $word . '&body_formats=' . $body_formats;		// file_getコンテキストのオプション		$opts = array(			'http' => array(				'method' => "GET",				'header' => "Host: hoge.sakura.ne.jp\r\n" .							"Accept-language: ja\r\n" .							"User-Agent: " . Input::user_agent() . "\r\n"			)		);		$context = stream_context_create($opts);		// API の結果を json で取得		$json = file_get_contents($filename, false, $context);		// 取得した json 文字列をそのまま返却		if($json != false)		{			return $json;		}		// 取得失敗したら空文字を返却		else		{			return '';		}	}}/* End of file haiku.php *//* Location: app/classes/model/api/hatena/haiku.php */file_getコンテキストのオプションに指定する Host 名は、ご自分の環境にあわせて修正してください。重要（2012-12-25）：3. ViewModel を使ってブログ（表示部分のみ）を作るここからやっとブログ（っぽく）表示する実装を行っていきます。View は贅沢（？）に Template を使ってみます。さきに書いてしまいますが、最終的なディレクトリ構成は以下のようになりました。実装のディレクトリ構成/home/{user-id}    |-- /fuel    |    |-- /app    |    |    |-- /classes    |    |    |    |-- /controller    |    |    |    |    `-- hhblog.php    |    |    |    |-- /model    |    |    |    |    `-- ･･･    |    |    |    `-- /view    |    |    |         `-- /hhblog    |    |    |              `-- /article    |    |    |                   |-- keyword.php    (1)'    |    |    |                   `-- public.php     (2)'    |    |    |-- /views    |    |    |    |-- /hhblog    |    |    |    |    |-- /article    |    |    |    |    |    |-- keyword.php    (1)    |    |    |    |    |    `-- public.php     (2)    |    |    |    |    `-- template.php    ･･･うーん、機能名失敗したな･･･（hhblog の hhblog コントローラ･･･orz）。View と Controllerでは、どんどんコードを晒していきます。ファイル：~/hhblog/fuel/app/classes/controller/hhblog.php<?php/** * Hatena-Haiku Blog Controller. *  * hhblog's root controller. *  * @package app * @extends Controller */class Controller_Hhblog extends Controller_Template{	public $template = 'hhblog/template';		/**	 * Get Hatena-Haiku Public Timeline in ViewModel	 * and show blog style page.	 */	public function action_public_timeline()	{		$this->template->set('title',       'Hatena-Haiku Public Timeline');		$this->template->set('description', 'はてなハイク Public Timeline を取得してブログっぽく表示させます。');		$this->template->set('active_api',  1);				// 本文部分に ViewModel を利用		$this->template->article = ViewModel::forge('hhblog/article/public');			}		/**	 * Get Hatena-Haiku Keyword Timeline in ViewModel	 * and show blog style page.	 * Now 'keyword' is Hatena user id 'id:hoge' style.	 */	public function action_keyword_timeline()	{		$keyword = 'id:tmd45';				$this->template->set('title',       'Hatena-Haiku Keyword Timeline');		$this->template->set('description', 'はてなハイク Keyword Timeline からキーワード「' . $keyword . '」の結果を取得してブログっぽく表示させます。');		$this->template->set('keyword', $keyword);		$this->template->set('active_api',  2);				// 本文部分に ViewModel を利用		$this->template->article = ViewModel::forge('hhblog/article/keyword');		$this->template->article->set('keyword', $keyword);			}}/* End of file hhblog.php *//* Location: app/classes/controller/hhblog.php */キーワード・タイムラインで指定するキーワードは、コントローラーに直書きしています。$keyword = 'id:tmd45';この文字列を変更すれば好きなキーワードを指定することができます。指定したキーワードはビューモデルでも利用したいので、以下のように値を渡しています。$this->template->article->set('keyword', $keyword);template.php on Gistpublic.php on Gistkeyword.php on Gistテンプレートのソースコード（template.php）の中で、コントローラーで ViewModel を forge した $article 変数を利用して記事を埋め込みます。ファイル：~/hhblog/fuel/app/views/hhblog/template.php の一部<!-- CONTENT --><div class="span9">			<?php echo $article; ?>			</div><!-- /span9 --><!-- /CONTENT -->記事側のソースコード（public.php, keyword.php）では、API で取得した記事の配列を繰り返し表示させています。これがテンプレートの $article 部分に展開されます。ファイル：~/hhblog/fuel/app/views/hhblog/article/public.php の一部<?php	foreach ($articles as $a)	{?>		<section class="hhblog-section">			<div class="page-main">				<p><?php echo $a["html"]; ?></p>			</div>		</section><?php	} /* end foreach($articles) */	unset($articles);?>これらをサイトのトップページに表示させるために、ルーティングの設定も修正します。http://{user-id}.sakura.ne.jp/hhblog/public_timeline にアクセスするとパブリック・タイムラインが、http://{user-id}.sakura.ne.jp/hhblog/keyword_timeline にアクセスするとキーワード・タイムラインが表示されるようにしました(2)。ファイル：~/hhblog/fuel/app/config/routes.php<?phpreturn array(//	'_root_'  => 'welcome/index',              // コメント化	'_root_'  => 'hhblog/keyword_timeline',    // 追加(1)	'_404_'   => 'welcome/404',		'(:segment)' => 'hhblog/$1',               // 追加(2)		'hello(/:name)?' => array('welcome/hello', 'name' => 'hello'),);    ViewModel！ViewModel！！では、最後にお待ちかねのビューモデルです。まずはパブリック・タイムライン用のビューモデル。API 制限を考慮して、新しいデータは 30 分に 1 回だけ取得するようにして、それ以外はキャッシュからデータを取得します。API の実行は Model を呼び出すだけです。ファイル：~/hhblog/fuel/app/classes/view/hhblog/article/public.php<?php/** * Hatena-Haiku Blog Public Timeline ViewModel. *  * @package app * @extends ViewModel */class View_Hhblog_Article_Public extends ViewModel{	/**	 * Get Hatena-Haiku Public Timeline.	 * 	 * Data cache to make Access-control, 	 * and get data from cache in a time.	 */	public function view()	{		$cache_id = 'hhblog_public_timeline_json';				try		{			// キャッシュにあればキャッシュから取得			$json = Cache::get($cache_id);		}		catch (\CacheNotFoundException $e)		{			// タイムライン取得(json)			$json = Model_Api_Hatena_Haiku::getPublicTimeline();						// キャッシュに保存（0.5時間）			Cache::set($cache_id, $json, 3600 * 0.5);		}				// jsonデータの配列化		$articles = Format::forge($json, 'json')->to_array();				// Viewにセット		$this->set('articles', $articles, false);	}}/* End of file public.php *//* Location: app/classes/view/hhblog/article/public.php */次にキーワード・タイムライン用のビューモデルです。パブリック・タイムラインと同じです。Model の実行時に引数にキーワードを渡しています。ファイル：~/hhblog/fuel/app/classes/view/hhblog/article/keyword.php<?php/** * Hatena-Haiku Blog Keyword Timeline ViewModel. *  * @package app * @extends ViewModel */class View_Hhblog_Article_Keyword extends ViewModel{	/**	 * Get Hatena-Haiku keyword timeline.	 * 	 * Data cache to make Access-control, 	 * and get data from cache in a time.	 */	public function view()	{		$cache_id = 'hhblog_keyword_timeline_json';				try		{			// キャッシュにあればキャッシュから取得			$json = Cache::get($cache_id);		}		catch (\CacheNotFoundException $e)		{			// タイムライン取得(json)			$json = Model_Api_Hatena_Haiku::getKeywordTimeline($this->keyword);						// キャッシュに保存（0.5時間）			Cache::set($cache_id, $json, 3600 * 0.5);		}				// jsonデータの配列化		$articles = Format::forge($json, 'json')->to_array();				// Viewにセット		$this->set('articles', $articles, false);	}}/* End of file keyword.php *//* Location: app/classes/view/hhblog/article/keyword.php */ここまで内容が一緒だと、あまり分けた意味がなさそうですが、応用編では表示内容を整形するという ViewModel 本来の処理を組み込んでいくのでこんな感じかな、と思います。ちなみに応用編の記事はありません（笑）取得したタイムラインのデータを View にセットする際に false を指定しています。$this->set('articles', $articles, false);これは API の応答結果より HTML でマークアップ済みのハイク記事本文（key='html'）を、そのまま表示に利用しているためです。4. 公開環境で動かそう実装が完了して、テストが出来たら（苦笑）、用意してあったレンサバのフレームワーク上に変更分をアップロードします。http://{user-id}.sakura.ne.jp/hhblog や http://{user-id}.sakura.ne.jp/hhblog/public_timeline にアクセスして、はてなハイクブログを表示してみましょう！ちなみに自分で作成したものは以下で動いています。tmd45 のはてなハイクブログ見た目は Twitter Bootstrap と Bootswatch 様様です。タイムライン部分以外は、ブログっぽ～く見えるように適当に捏造しています。まとめいかがでしたでしょうか。コードを載せているのでだいぶ記事が長くなってしまいました。準備から実装まで通して記事にできたので自分の復習用には良かったと思うのですが、Advent Calendar としてどうなんでしょう（苦笑）明日は @ttikitt さんの『FuelPHPへのDoctrine2組み込み』です！('ω`)ｼ*1:自分の環境では、他のディレクトリのレベルと合わせて 755 にしました。]]></summary>
    </entry>
</feed>