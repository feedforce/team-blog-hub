{"pageProps":{"member":{"name":"yaonyan","role":"データサイエンティスト","bio":"かっこいいワンフレーズ自己紹介などない","avatarSrc":"https://s.gravatar.com/avatar/bea1d48ae8ab2b9480d5bbbe6bdd6a33?s=256","sources":["https://developer.feedforce.jp/rss/author/newton800"],"twitterUsername":"feed_yao","githubUsername":"yaoshunya"},"postItems":[{"title":"StreamlitとDeepLabv3を用いて物体切り抜きができるか検証したという話をした","content":"<p>こんにちは <a href=\"http://blog.hatena.ne.jp/newton800/\" class=\"hatena-id-icon\"><img src=\"https://cdn.profile-image.st-hatena.com/users/newton800/profile.png\" width=\"16\" height=\"16\" alt=\"\" class=\"hatena-id-icon\">id:newton800</a> です。</p>\n\n<p>先週末に社内勉強会 <a href=\"https://developer.feedforce.jp/archive/category/FFTT\">FFTT</a>で「StreamlitとDeepLabv3を用いて物体切り抜きができるか検証した」という話をしました。</p>\n\n<p>最近個人的に使用頻度が増加している<a href=\"https://streamlit.io/\">Streamlit</a>というフロントエンドの知識なしに実装ができるPythonのWebフレームワークの紹介をしてみたかったというのが背景です。ただし、Streamlitに触れている部分は本当に一部だけで、ほとんどはどんな感じで試行錯誤しながら試作を作っていくのかみたいな話になってしまいました。</p>\n\n<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQ6P2ZVvWxk_ZKgfgKVVQRSH1w1rZVfDojE0Khj2ZSp1Mrn9TKaz4n4-pDhHUr7s4Wg9xWbLpfJlv59/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>\n\n\n<p>年に1~2度ほど回ってくるFFTTの発表のために考えたネタなので、社内で何か活用したりする機会はないですが、自分的にはなかなか気に入っている自由研究です。</p>\n\n<p>次のFFTTではどんなネタを発表しようかな。。。</p>\n\n<p>P.S. Feedforce Developer Blogが <a href=\"http://blog.hatena.ne.jp/masutaka26/\" class=\"hatena-id-icon\"><img src=\"https://cdn.profile-image.st-hatena.com/users/masutaka26/profile.png\" width=\"16\" height=\"16\" alt=\"\" class=\"hatena-id-icon\">id:masutaka26</a>の記事ばかりになっているので、負けじと記事を書いていきたいです</p>\n","contentSnippet":"こんにちは id:newton800 です。先週末に社内勉強会 FFTTで「StreamlitとDeepLabv3を用いて物体切り抜きができるか検証した」という話をしました。最近個人的に使用頻度が増加しているStreamlitというフロントエンドの知識なしに実装ができるPythonのWebフレームワークの紹介をしてみたかったというのが背景です。ただし、Streamlitに触れている部分は本当に一部だけで、ほとんどはどんな感じで試行錯誤しながら試作を作っていくのかみたいな話になってしまいました。年に1~2度ほど回ってくるFFTTの発表のために考えたネタなので、社内で何か活用したりする機会はないですが、自分的にはなかなか気に入っている自由研究です。次のFFTTではどんなネタを発表しようかな。。。P.S. Feedforce Developer Blogが id:masutaka26の記事ばかりになっているので、負けじと記事を書いていきたいです","link":"https://developer.feedforce.jp/entry/2022/10/11/095230","isoDate":"2022-10-11T00:52:30.000Z","dateMiliSeconds":1665449550000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"yaonyan"},{"title":"画像上の文字を自動削除する仕組みを作ってみた","content":"<p>こんにちは　 データサイエンティストの八百俊哉です。</p>\n\n<p><strong>今回は画像上に存在する文字を自動的に削除し、背景を補完する仕組みを作成しました。</strong>ただ、弊社のプロダクトに実装される可能性が極めて低いので、自由研究の結果としてここに残そうと思います。</p>\n\n<p>弊社のサービスはインターネット広告と深く関わりがあり、インターネット広告に関する分析を実施することが多いです。今回はインターネット広告の画像に関する調査を実施したので、その結果を共有します。</p>\n\n<h1>背景</h1>\n\n<p>インターネット広告では、商品画像上にプロモーションのロゴや行動を促すフレーズが入っている場合、不承認とされ広告表示されなくなる可能性があります。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"プロモーションが入った商品画像の例\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220330/20220330091344.png\" width=\"496\" height=\"426\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>プロモーションが入った商品画像の例</figcaption></figure></p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fsupport.ecbooster.jp%2Fja%2Farticles%2F4666716-google-merchant-center%25E3%2581%25A7%25E3%2582%25AA%25E3%2583%25BC%25E3%2583%2590%25E3%2583%25BC%25E3%2583%25AC%25E3%2582%25A4%25E3%2581%25AB%25E3%2582%2588%25E3%2582%258B%25E7%2594%25BB%25E5%2583%258F%25E4%25B8%258D%25E6%2589%25BF%25E8%25AA%258D%25E3%2581%25AB%25E5%25AF%25BE%25E5%25BF%259C%25E3%2581%2597%25E3%2581%259F%25E3%2581%2584%25E3%2581%25A8%25E3%2581%258D\" title=\"Google Merchant Centerでオーバーレイによる画像不承認に対応したいとき\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://support.ecbooster.jp/ja/articles/4666716-google-merchant-center%E3%81%A7%E3%82%AA%E3%83%BC%E3%83%90%E3%83%BC%E3%83%AC%E3%82%A4%E3%81%AB%E3%82%88%E3%82%8B%E7%94%BB%E5%83%8F%E4%B8%8D%E6%89%BF%E8%AA%8D%E3%81%AB%E5%AF%BE%E5%BF%9C%E3%81%97%E3%81%9F%E3%81%84%E3%81%A8%E3%81%8D\">support.ecbooster.jp</a></cite></p>\n\n<p>商品数が少ない場合や再度商品の写真を用意できる場合は、商品画像の差し替えが可能ですが、そうではない場合はプロモーションが入っていない画像をいちから用意するのは非常に困難です。</p>\n\n<p>一部画像においては、<a href=\"https://support.google.com/merchants/answer/9242973\">Googleが公開している画像の自動改善</a>を用いることで、プロモーションを削除することができますが、例には背景が白のもののみになっています。一方で商品画像は何らかの背景（机の上・壁紙）が写真に写り込んでいることが多く、白い背景以外にも対応したプロモーション削除方法が必要です。</p>\n\n<p>そこで今回は背景ありのプロモーションが掲載されている商品画像から自動的にプロモーションを削除する機能が必要ではないかと考え検証を行いました。</p>\n\n<h1>全体の流れ</h1>\n\n<p>サンプルとして以下の画像を用意しました。（自作なのでクオリティは低いです）</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"サンプル画像\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220330/20220330092012.png\" width=\"551\" height=\"551\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>サンプル画像</figcaption></figure></p>\n\n<p>今回はこのサンプル画像にある送料無料という文字を消すことを目的にしたいと思います。</p>\n\n<p>以下に示すフローで文字を消し、背景を補完します。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"全体のフロー\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220330/20220330100726.png\" width=\"742\" height=\"306\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>全体のフロー</figcaption></figure></p>\n\n<p>最終的には以下のような文字除去ができました。\n<figure class=\"figure-image figure-image-fotolife\" title=\"実行することで文字除去に成功した例\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220509/20220509105601.png\" width=\"1200\" height=\"568\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>実行することで文字除去に成功した例</figcaption></figure></p>\n\n<p>では実際にどのように文字を除去したのかを紹介します。</p>\n\n<h1>画像上のどこに文字があるか判定する・文字の部分だけ黒い画像を用意</h1>\n\n<p>まずはじめに画像の上のどこに文字があるかを判定します。今回はOCRを用いて文字を見つけていきます。</p>\n\n<blockquote><p>OCR（Optical Character Recognition/Reader、オーシーアール、光学的文字認識）とは、手書きや印刷された文字を、イメージスキャナやデジタルカメラによって読みとり、コンピュータが利用できるデジタルの文字コードに変換する技術です。\n<cite><a href=\"https://mediadrive.jp/technology/ocr\">https://mediadrive.jp/technology/ocr</a></cite></p></blockquote>\n\n<p>まずpythonでOCRを使用するために必要なライブリラリをインストールします。（今回はColabを用いた実行を想定しています。）</p>\n\n<pre class=\"code\" data-lang=\"\" data-unlink>!pip install --upgrade opencv-contrib-python\n!apt install tesseract-ocr libtesseract-dev tesseract-ocr-jpn\n!pip install pyocr</pre>\n\n\n<p>以下がOCRを用いて、文字を検知する関数になります。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>tools = pyocr.get_available_tools()\ntool = tools[<span class=\"synConstant\">0</span>]\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">get_box</span>(img, tool, show=<span class=\"synIdentifier\">True</span>):\n    <span class=\"synConstant\">&quot;&quot;&quot;</span>\n<span class=\"synConstant\">    img:PIL.Image.Image</span>\n<span class=\"synConstant\">    &quot;&quot;&quot;</span>\n\n    results = tool.image_to_string(\n        img,\n        lang=<span class=\"synConstant\">'jpn'</span>,\n        builder=pyocr.builders.LineBoxBuilder(tesseract_layout=<span class=\"synConstant\">6</span>)\n    )\n\n    img_np = np.array(img)\n\n    w, h = img_np.shape[<span class=\"synConstant\">0</span>], img_np.shape[<span class=\"synConstant\">1</span>]\n\n    <span class=\"synStatement\">for</span> box <span class=\"synStatement\">in</span> results:\n        <span class=\"synStatement\">if</span> box.content == <span class=\"synConstant\">' '</span> <span class=\"synStatement\">or</span> box.content == <span class=\"synConstant\">''</span>:\n            <span class=\"synStatement\">continue</span>\n        <span class=\"synStatement\">if</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">1</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">0</span>] == w) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">1</span>] == h):\n            <span class=\"synStatement\">continue</span>\n        cv2.rectangle(img_np, box.position[<span class=\"synConstant\">0</span>], box.position[<span class=\"synConstant\">1</span>], (<span class=\"synConstant\">0</span>, <span class=\"synConstant\">255</span>, <span class=\"synConstant\">0</span>), <span class=\"synConstant\">1</span>)\n    <span class=\"synStatement\">if</span> show:\n        display(Image.fromarray(img_np))\n    <span class=\"synStatement\">return</span> img_np, results\n</pre>\n\n\n<p>しかし、ただ画像を上記の関数に渡すだけでは文字を認識できる精度が低いという課題がありました。そこで以下のようにすることで文字認識の精度向上を行いました。</p>\n\n<ol>\n<li>画像を拡大する</li>\n<li>拡大した画像を5×5に分割する</li>\n<li>分割後の画像をそれぞれOCRにかける</li>\n<li>分割前の拡大した画像をOCRにかける</li>\n<li>3,4で文字と判定され部分（OR）を文字がある場所とする</li>\n</ol>\n\n\n<p>図で示すと以下のようになっています。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"文字検知の精度向上のための取り組み\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220405/20220405122649.png\" width=\"932\" height=\"403\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>文字検知の精度向上のための取り組み</figcaption></figure></p>\n\n<p>実際には以下のようにして求めます。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synStatement\">def</span> <span class=\"synIdentifier\">split_images</span>(img, h_split, w_split):\n    <span class=\"synConstant\">&quot;&quot;&quot;</span>\n<span class=\"synConstant\">    画像を分割し、分割後の画像と元画像における座標を返す</span>\n<span class=\"synConstant\">    img:numpy</span>\n<span class=\"synConstant\">    &quot;&quot;&quot;</span>\n\n    h,w = img.shape[<span class=\"synConstant\">0</span>],img.shape[<span class=\"synConstant\">1</span>]\n    images = []\n\n    new_h = <span class=\"synIdentifier\">int</span>(h / h_split)\n    new_w = <span class=\"synIdentifier\">int</span>(w / w_split)\n\n    start_coordinates = []\n\n    <span class=\"synStatement\">for</span> _h <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">range</span>(h_split):\n        h_start = _h * new_h\n        h_end = h_start + new_h\n\n        <span class=\"synStatement\">for</span> _w <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">range</span>(w_split):\n            w_start = _w * new_w\n            w_end = w_start + new_w\n            images.append(img[h_start:h_end, w_start:w_end, :])\n\n            coordinate = {}\n            coordinate[<span class=\"synConstant\">'h_start'</span>] = h_start\n            coordinate[<span class=\"synConstant\">'h_end'</span>] = h_end\n            coordinate[<span class=\"synConstant\">'w_start'</span>] = w_start\n            coordinate[<span class=\"synConstant\">'w_end'</span>] = w_end\n\n            start_coordinates.append(coordinate)\n    \n    <span class=\"synStatement\">return</span> images, start_coordinates\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">get_splitImages2originalPositins</span>(images, img_resize, start_coordinates, show=<span class=\"synIdentifier\">True</span>):\n    <span class=\"synConstant\">&quot;&quot;&quot;</span>\n<span class=\"synConstant\">    分割後の画像それぞれのどこに文字があるかどうかを識別する</span>\n<span class=\"synConstant\">    また、それらの結果を元の座標系に戻してpositionsとして返す</span>\n<span class=\"synConstant\">    images:list</span>\n<span class=\"synConstant\">    img_resize:numpy</span>\n<span class=\"synConstant\">    start_coordinates:list</span>\n<span class=\"synConstant\">    &quot;&quot;&quot;</span>\n    positions = []\n    <span class=\"synStatement\">for</span> i, (img, coordinate)  <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">enumerate</span>(<span class=\"synIdentifier\">zip</span>(images, start_coordinates)):\n\n        bb,results = get_box(Image.fromarray(img), tool, show=<span class=\"synIdentifier\">False</span>)\n        w,h = img.shape[<span class=\"synConstant\">0</span>],img.shape[<span class=\"synConstant\">1</span>]\n\n        <span class=\"synStatement\">for</span> box <span class=\"synStatement\">in</span> results:\n            <span class=\"synStatement\">if</span> box.content == <span class=\"synConstant\">' '</span> <span class=\"synStatement\">or</span> box.content == <span class=\"synConstant\">''</span>:\n                <span class=\"synStatement\">continue</span>\n            <span class=\"synStatement\">if</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">1</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">0</span>] == w) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">1</span>] == h):\n                <span class=\"synStatement\">continue</span>\n            \n            position = [[p[<span class=\"synConstant\">0</span>],p[<span class=\"synConstant\">1</span>]] <span class=\"synStatement\">for</span> p <span class=\"synStatement\">in</span> box.position]\n            position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">0</span>] += coordinate[<span class=\"synConstant\">'w_start'</span>]\n            position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">1</span>] += coordinate[<span class=\"synConstant\">'h_start'</span>]\n            position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">0</span>] += coordinate[<span class=\"synConstant\">'w_start'</span>]\n            position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">1</span>] += coordinate[<span class=\"synConstant\">'h_start'</span>]\n            cv2.rectangle(img_resize, position[<span class=\"synConstant\">0</span>], position[<span class=\"synConstant\">1</span>], (<span class=\"synConstant\">0</span>, <span class=\"synConstant\">255</span>, <span class=\"synConstant\">0</span>), <span class=\"synConstant\">1</span>)\n            positions.append(position)\n    <span class=\"synStatement\">if</span> show:\n        display(Image.fromarray(img_resize))\n    <span class=\"synStatement\">return</span> positions\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">get_mask</span>(img, positions):\n    mask = np.zeros_like(img)\n    <span class=\"synStatement\">for</span> p <span class=\"synStatement\">in</span> positions:\n        cv2.rectangle(mask, p[<span class=\"synConstant\">0</span>], p[<span class=\"synConstant\">1</span>], (<span class=\"synConstant\">255</span>, <span class=\"synConstant\">255</span>, <span class=\"synConstant\">255</span>), thickness=-<span class=\"synConstant\">1</span>)\n    <span class=\"synStatement\">return</span> mask\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">results2positions</span>(results):\n    positions = []\n    <span class=\"synStatement\">for</span> box <span class=\"synStatement\">in</span> results:\n        <span class=\"synStatement\">if</span> box.content == <span class=\"synConstant\">' '</span> <span class=\"synStatement\">or</span> box.content == <span class=\"synConstant\">''</span>:\n            <span class=\"synStatement\">continue</span>\n        <span class=\"synStatement\">if</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">1</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">0</span>] == w) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">1</span>] == h):\n            <span class=\"synStatement\">continue</span>\n        positions.append([box.position[<span class=\"synConstant\">0</span>], box.position[<span class=\"synConstant\">1</span>]])\n    <span class=\"synStatement\">return</span> positions\n\n<span class=\"synComment\"># 画像の拡大</span>\nimg_resize = cv2.resize(img_np, (<span class=\"synIdentifier\">int</span>(img_np.shape[<span class=\"synConstant\">0</span>] * <span class=\"synConstant\">4</span>), <span class=\"synIdentifier\">int</span>(img_np.shape[<span class=\"synConstant\">1</span>] * <span class=\"synConstant\">4</span>)), interpolation=cv2.INTER_CUBIC)\n\n<span class=\"synComment\"># 画像を分割する</span>\nimages, start_coordinates = split_images(np.array(img_resize), <span class=\"synConstant\">5</span>, <span class=\"synConstant\">5</span>)\n<span class=\"synComment\"># 分割画像のそれぞれをOCRに入れ、どこに文字が有るか判定する</span>\n<span class=\"synComment\"># また、このときに返ってくるpositionsには、分割前の座標系でどこに文字があったのかが示されている</span>\npositions = get_splitImages2originalPositins(images, img_resize, start_coordinates, show=<span class=\"synIdentifier\">False</span>)\n<span class=\"synComment\"># 分割画像から特定された文字の位置がマスクになるようにする</span>\nmask_resize = get_mask(img_resize, positions)\n<span class=\"synComment\"># 元の画像サイズに戻す</span>\nmask_resize = cv2.resize(mask_resize, (img_np.shape[<span class=\"synConstant\">0</span>],img_np.shape[<span class=\"synConstant\">1</span>]))\n\n<span class=\"synComment\"># リサイズ画像を分割せずに、どこに文字が有るかを求める</span>\nimg_np, results = get_box(img1, tool, show=<span class=\"synIdentifier\">False</span>)\npositions = results2positions(results)\n<span class=\"synComment\"># リサイズ画像から特定された文字の位置がマスクになるようにする</span>\noriginal_mask = get_mask(img_np, positions)\n\n<span class=\"synComment\"># mask_resize（分割画像）とoriginal_mask（リサイズ画像）のORを求める</span>\nbitwise_or = cv2.bitwise_or(original_mask[:,:,<span class=\"synConstant\">0</span>], mask_resize[:,:,<span class=\"synConstant\">0</span>])\n</pre>\n\n\n<p>上記を実行することで、画像上に文字があるとされた部分の画素だけが255のマスク画像がbitwise_orに入っていることになります。</p>\n\n<h1>画像上の黒い部分を周りの情報から補完</h1>\n\n<p>次に先程作成したマスク画像と元画像を用いることで、文字の部分を周りの状況から補完していきます。</p>\n\n<p><strong>今回はOpneCVのInpaintingを用いて、文字の除去を行っていきます。</strong>\n<iframe src=\"https://hatenablog-parts.com/embed?url=http%3A%2F%2Flabs.eecs.tottori-u.ac.jp%2Fsd%2FMember%2Foyamada%2FOpenCV%2Fhtml%2Fpy_tutorials%2Fpy_photo%2Fpy_inpainting%2Fpy_inpainting.html\" title=\"画像のInpainting — OpenCV-Python Tutorials 1 documentation\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_photo/py_inpainting/py_inpainting.html\">labs.eecs.tottori-u.ac.jp</a></cite></p>\n\n<p>実際に以下のようにして複数のパラメータでInpaintingを実行し、結果を確認できるようにしました。(参考：<a href=\"https://data-analysis-stats.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/opencv%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AEinpainting%E3%81%AE%E8%A7%A3%E8%AA%AC/\">OpenCVで画像のInpaintingの解説</a>)</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synStatement\">def</span> <span class=\"synIdentifier\">get_inpainting</span>(img, mask):\n    img = np.array(img)\n    mask = np.array(mask)\n\n    <span class=\"synStatement\">if</span> mask.ndim == <span class=\"synConstant\">3</span>:\n        mask = mask[:,:,<span class=\"synConstant\">0</span>]\n    \n    dst11 = cv2.inpaint(img, mask , <span class=\"synConstant\">0</span>, cv2.INPAINT_TELEA)\n    dst12 = cv2.inpaint(img, mask , <span class=\"synConstant\">3</span>, cv2.INPAINT_TELEA)\n    dst13 = cv2.inpaint(img, mask , <span class=\"synConstant\">10</span>, cv2.INPAINT_TELEA)\n    dst21 = cv2.inpaint(img, mask , <span class=\"synConstant\">0</span>, cv2.INPAINT_NS)\n    dst22 = cv2.inpaint(img, mask , <span class=\"synConstant\">3</span>, cv2.INPAINT_NS)\n    dst23 = cv2.inpaint(img, mask , <span class=\"synConstant\">10</span>, cv2.INPAINT_NS)\n\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_TELEA, radius=0) : dst11&quot;</span>)\n    display(Image.fromarray(dst11))\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_TELEA, radius=3) : dst12&quot;</span>)\n    display(Image.fromarray(dst11))\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_TELEA, radius=10) : dst13&quot;</span>)\n    display(Image.fromarray(dst11))\n\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_NS, radius=0) : dst21&quot;</span>)\n    display(Image.fromarray(dst21))\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_NS, radius=3) : dst22&quot;</span>)\n    display(Image.fromarray(dst22))\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_NS, radius=10) : dst23&quot;</span>)\n    display(Image.fromarray(dst23))\n\n    <span class=\"synStatement\">return</span> dst11,dst12,dst13,dst21,dst22,dst23\n\ndst11,dst12,dst13,dst21,dst22,dst23 = get_inpainting(img_np, bitwise_or)\n</pre>\n\n\n<p>こちらを実行すると以下の出力が得られます。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"結果の出力例\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220509/20220509103312.png\" width=\"855\" height=\"1200\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>結果の出力例</figcaption></figure></p>\n\n<p>出力画像を確認するとしっかりと文字が消えており、文字があった部分は背景色と同じ色が補完されていることがわかります。</p>\n\n<h1>まとめ</h1>\n\n<p>今回は画像上に存在する文字を自動的に削除し、背景を補完する仕組みの紹介を行いました。</p>\n\n<p>はじめに画像上の文字を検知し、その検知結果をもとにマスク画像を作成します。そのマスク画像と元画像をInpaintingに入力することで、画像の補完を行いました。</p>\n\n<p><strong>特に文字を検知する部分で、文字検知の取りこぼしが多く見られたので画像を拡大・分割するなどの工夫を実施したところがポイントです。ただし、現状の文字検知の方法でもかなり取りこぼしが多い状態なので、実践で使用する場合にはもう少し工夫が必要になります。</strong></p>\n\n<p>大学の頃は画像系の卒論を書いたこともあり、とても楽しめながら作成することができました。\n最後までお付き合いいただき、ありがとうございます。</p>\n","contentSnippet":"こんにちは　 データサイエンティストの八百俊哉です。今回は画像上に存在する文字を自動的に削除し、背景を補完する仕組みを作成しました。ただ、弊社のプロダクトに実装される可能性が極めて低いので、自由研究の結果としてここに残そうと思います。弊社のサービスはインターネット広告と深く関わりがあり、インターネット広告に関する分析を実施することが多いです。今回はインターネット広告の画像に関する調査を実施したので、その結果を共有します。背景インターネット広告では、商品画像上にプロモーションのロゴや行動を促すフレーズが入っている場合、不承認とされ広告表示されなくなる可能性があります。プロモーションが入った商品画像の例support.ecbooster.jp商品数が少ない場合や再度商品の写真を用意できる場合は、商品画像の差し替えが可能ですが、そうではない場合はプロモーションが入っていない画像をいちから用意するのは非常に困難です。一部画像においては、Googleが公開している画像の自動改善を用いることで、プロモーションを削除することができますが、例には背景が白のもののみになっています。一方で商品画像は何らかの背景（机の上・壁紙）が写真に写り込んでいることが多く、白い背景以外にも対応したプロモーション削除方法が必要です。そこで今回は背景ありのプロモーションが掲載されている商品画像から自動的にプロモーションを削除する機能が必要ではないかと考え検証を行いました。全体の流れサンプルとして以下の画像を用意しました。（自作なのでクオリティは低いです）サンプル画像今回はこのサンプル画像にある送料無料という文字を消すことを目的にしたいと思います。以下に示すフローで文字を消し、背景を補完します。全体のフロー最終的には以下のような文字除去ができました。実行することで文字除去に成功した例では実際にどのように文字を除去したのかを紹介します。画像上のどこに文字があるか判定する・文字の部分だけ黒い画像を用意まずはじめに画像の上のどこに文字があるかを判定します。今回はOCRを用いて文字を見つけていきます。OCR（Optical Character Recognition/Reader、オーシーアール、光学的文字認識）とは、手書きや印刷された文字を、イメージスキャナやデジタルカメラによって読みとり、コンピュータが利用できるデジタルの文字コードに変換する技術です。https://mediadrive.jp/technology/ocrまずpythonでOCRを使用するために必要なライブリラリをインストールします。（今回はColabを用いた実行を想定しています。）!pip install --upgrade opencv-contrib-python!apt install tesseract-ocr libtesseract-dev tesseract-ocr-jpn!pip install pyocr以下がOCRを用いて、文字を検知する関数になります。tools = pyocr.get_available_tools()tool = tools[0]def get_box(img, tool, show=True):    \"\"\"    img:PIL.Image.Image    \"\"\"    results = tool.image_to_string(        img,        lang='jpn',        builder=pyocr.builders.LineBoxBuilder(tesseract_layout=6)    )    img_np = np.array(img)    w, h = img_np.shape[0], img_np.shape[1]    for box in results:        if box.content == ' ' or box.content == '':            continue        if (box.position[0][0] == 0) and (box.position[0][1] == 0) and (box.position[1][0] == w) and (box.position[1][1] == h):            continue        cv2.rectangle(img_np, box.position[0], box.position[1], (0, 255, 0), 1)    if show:        display(Image.fromarray(img_np))    return img_np, resultsしかし、ただ画像を上記の関数に渡すだけでは文字を認識できる精度が低いという課題がありました。そこで以下のようにすることで文字認識の精度向上を行いました。画像を拡大する拡大した画像を5×5に分割する分割後の画像をそれぞれOCRにかける分割前の拡大した画像をOCRにかける3,4で文字と判定され部分（OR）を文字がある場所とする図で示すと以下のようになっています。文字検知の精度向上のための取り組み実際には以下のようにして求めます。def split_images(img, h_split, w_split):    \"\"\"    画像を分割し、分割後の画像と元画像における座標を返す    img:numpy    \"\"\"    h,w = img.shape[0],img.shape[1]    images = []    new_h = int(h / h_split)    new_w = int(w / w_split)    start_coordinates = []    for _h in range(h_split):        h_start = _h * new_h        h_end = h_start + new_h        for _w in range(w_split):            w_start = _w * new_w            w_end = w_start + new_w            images.append(img[h_start:h_end, w_start:w_end, :])            coordinate = {}            coordinate['h_start'] = h_start            coordinate['h_end'] = h_end            coordinate['w_start'] = w_start            coordinate['w_end'] = w_end            start_coordinates.append(coordinate)        return images, start_coordinatesdef get_splitImages2originalPositins(images, img_resize, start_coordinates, show=True):    \"\"\"    分割後の画像それぞれのどこに文字があるかどうかを識別する    また、それらの結果を元の座標系に戻してpositionsとして返す    images:list    img_resize:numpy    start_coordinates:list    \"\"\"    positions = []    for i, (img, coordinate)  in enumerate(zip(images, start_coordinates)):        bb,results = get_box(Image.fromarray(img), tool, show=False)        w,h = img.shape[0],img.shape[1]        for box in results:            if box.content == ' ' or box.content == '':                continue            if (box.position[0][0] == 0) and (box.position[0][1] == 0) and (box.position[1][0] == w) and (box.position[1][1] == h):                continue                        position = [[p[0],p[1]] for p in box.position]            position[0][0] += coordinate['w_start']            position[0][1] += coordinate['h_start']            position[1][0] += coordinate['w_start']            position[1][1] += coordinate['h_start']            cv2.rectangle(img_resize, position[0], position[1], (0, 255, 0), 1)            positions.append(position)    if show:        display(Image.fromarray(img_resize))    return positionsdef get_mask(img, positions):    mask = np.zeros_like(img)    for p in positions:        cv2.rectangle(mask, p[0], p[1], (255, 255, 255), thickness=-1)    return maskdef results2positions(results):    positions = []    for box in results:        if box.content == ' ' or box.content == '':            continue        if (box.position[0][0] == 0) and (box.position[0][1] == 0) and (box.position[1][0] == w) and (box.position[1][1] == h):            continue        positions.append([box.position[0], box.position[1]])    return positions# 画像の拡大img_resize = cv2.resize(img_np, (int(img_np.shape[0] * 4), int(img_np.shape[1] * 4)), interpolation=cv2.INTER_CUBIC)# 画像を分割するimages, start_coordinates = split_images(np.array(img_resize), 5, 5)# 分割画像のそれぞれをOCRに入れ、どこに文字が有るか判定する# また、このときに返ってくるpositionsには、分割前の座標系でどこに文字があったのかが示されているpositions = get_splitImages2originalPositins(images, img_resize, start_coordinates, show=False)# 分割画像から特定された文字の位置がマスクになるようにするmask_resize = get_mask(img_resize, positions)# 元の画像サイズに戻すmask_resize = cv2.resize(mask_resize, (img_np.shape[0],img_np.shape[1]))# リサイズ画像を分割せずに、どこに文字が有るかを求めるimg_np, results = get_box(img1, tool, show=False)positions = results2positions(results)# リサイズ画像から特定された文字の位置がマスクになるようにするoriginal_mask = get_mask(img_np, positions)# mask_resize（分割画像）とoriginal_mask（リサイズ画像）のORを求めるbitwise_or = cv2.bitwise_or(original_mask[:,:,0], mask_resize[:,:,0])上記を実行することで、画像上に文字があるとされた部分の画素だけが255のマスク画像がbitwise_orに入っていることになります。画像上の黒い部分を周りの情報から補完次に先程作成したマスク画像と元画像を用いることで、文字の部分を周りの状況から補完していきます。今回はOpneCVのInpaintingを用いて、文字の除去を行っていきます。labs.eecs.tottori-u.ac.jp実際に以下のようにして複数のパラメータでInpaintingを実行し、結果を確認できるようにしました。(参考：OpenCVで画像のInpaintingの解説)def get_inpainting(img, mask):    img = np.array(img)    mask = np.array(mask)    if mask.ndim == 3:        mask = mask[:,:,0]        dst11 = cv2.inpaint(img, mask , 0, cv2.INPAINT_TELEA)    dst12 = cv2.inpaint(img, mask , 3, cv2.INPAINT_TELEA)    dst13 = cv2.inpaint(img, mask , 10, cv2.INPAINT_TELEA)    dst21 = cv2.inpaint(img, mask , 0, cv2.INPAINT_NS)    dst22 = cv2.inpaint(img, mask , 3, cv2.INPAINT_NS)    dst23 = cv2.inpaint(img, mask , 10, cv2.INPAINT_NS)    print(\"\\nOutput (INPAINT_TELEA, radius=0) : dst11\")    display(Image.fromarray(dst11))    print(\"\\nOutput (INPAINT_TELEA, radius=3) : dst12\")    display(Image.fromarray(dst11))    print(\"\\nOutput (INPAINT_TELEA, radius=10) : dst13\")    display(Image.fromarray(dst11))    print(\"\\nOutput (INPAINT_NS, radius=0) : dst21\")    display(Image.fromarray(dst21))    print(\"\\nOutput (INPAINT_NS, radius=3) : dst22\")    display(Image.fromarray(dst22))    print(\"\\nOutput (INPAINT_NS, radius=10) : dst23\")    display(Image.fromarray(dst23))    return dst11,dst12,dst13,dst21,dst22,dst23dst11,dst12,dst13,dst21,dst22,dst23 = get_inpainting(img_np, bitwise_or)こちらを実行すると以下の出力が得られます。結果の出力例出力画像を確認するとしっかりと文字が消えており、文字があった部分は背景色と同じ色が補完されていることがわかります。まとめ今回は画像上に存在する文字を自動的に削除し、背景を補完する仕組みの紹介を行いました。はじめに画像上の文字を検知し、その検知結果をもとにマスク画像を作成します。そのマスク画像と元画像をInpaintingに入力することで、画像の補完を行いました。特に文字を検知する部分で、文字検知の取りこぼしが多く見られたので画像を拡大・分割するなどの工夫を実施したところがポイントです。ただし、現状の文字検知の方法でもかなり取りこぼしが多い状態なので、実践で使用する場合にはもう少し工夫が必要になります。大学の頃は画像系の卒論を書いたこともあり、とても楽しめながら作成することができました。最後までお付き合いいただき、ありがとうございます。","link":"https://developer.feedforce.jp/entry/2022/05/09/114054","isoDate":"2022-05-09T02:40:54.000Z","dateMiliSeconds":1652064054000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"yaonyan"},{"title":"ECサイトのタイトルと説明文から取り扱いカテゴリの予測を行った","content":"<p>こんにちは　 データサイエンティストの八百俊哉です。最近は家でカクテルを作ることにはまっています。</p>\n\n<p><strong>今回はEC BoosterのクライントのECサイトのタイトルと説明文を用いることで、どのカテゴリの商品を扱っているのか予測する仕組みを作成しました</strong>ので、その方法を紹介します。</p>\n\n<p>弊社のサービス<a href=\"https://ecbooster.jp/\">EC Booster</a>は、Google ショッピング広告の自動運用による自社EC自動集客サービスです。主要ECシステムと連携することで、Google の検索結果画面に画像付きで自社商品を訴求することが可能となります。</p>\n\n<h1>分析背景</h1>\n\n<p>EC Boosterを利用いただくことで、Google ショッピング広告を自動運用することができます。\nGoogleショッピング広告では広告出稿する際に、<strong>Googleが自動的にそれぞれの商品にgoogle product categoryというカテゴリ情報を割り当てます。</strong></p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fsupport.google.com%2Fmerchants%2Fanswer%2F6324436%3Fhl%3Dja\" title=\"Google 商品カテゴリ [google_product_category] - Google Merchant Center ヘルプ\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://support.google.com/merchants/answer/6324436?hl=ja\">support.google.com</a></cite></p>\n\n<p>しかしながら、<strong>Googleが自動的に割り振った商品ごとのカテゴリ情報はEC Booster側からは確認することができません。</strong></p>\n\n<blockquote><p>Google's category of the item (see Google product taxonomy). When querying products, this field will contain the user provided value. There is currently no way to get back the auto assigned google product categories through the API.\n<cite><a href=\"https://developers.google.com/shopping-content/reference/rest/v2.1/products?hl=en#Product.FIELDS.google_product_category\">https://developers.google.com/shopping-content/reference/rest/v2.1/products?hl=en#Product.FIELDS.google_product_category</a></cite></p></blockquote>\n\n<p>商品ごとに割り振られたカテゴリがわからないと、それぞれのクライントがどのような商品を扱っているのかわからなくなります。</p>\n\n<p>その結果、<strong>クライアントの実績を取扱商品カテゴリごとに集計したり、扱っているカテゴリごとのサポートを行ったりすることができなくなってしまいます。</strong></p>\n\n<p>そこで今回は<strong>クライントごとに取扱商品カテゴリをEC Booster側で予測する</strong>ことで、上に挙げたような課題を解決するような仕組みを作成しました。</p>\n\n<h1>学習・予測に必要なデータの収集</h1>\n\n<p>EC Boosterは現在有料プラン約300のクライントとフリープラン約2000のクライントにご利用いただいております。（<a href=\"https://ssl4.eir-parts.net/doc/7068/tdnet/2065834/00.pdf\">2022年5月第2四半期 決算説明資料より</a>）</p>\n\n<p><strong>学習データ生成のために、有料プランユーザーを中心に約470のクライントに対して手動でカテゴリを割り振るという作業を実施しました。</strong>また、このデータの作成はそれぞれのクライントのホームページを目視で確認し、適切なカテゴリを割り振りました。</p>\n\n<p>次に学習・予測に必要な入力データの収集を行いました。<strong>今回はそれぞれのクライントのタイトルと説明文を用いて学習をします。</strong>\n<strong>そのため以下に示すようにそれぞれのホームページからtitleとdescriptionのスクレイピングを行いました。\n</strong>\n<figure class=\"figure-image figure-image-fotolife\" title=\"タイトル(title)、説明文(description)の例\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220215/20220215113752.png\" alt=\"f:id:newton800:20220215113752p:plain\" width=\"1200\" height=\"575\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>タイトル(title)、説明文(description)の例</figcaption></figure></p>\n\n<p>それぞれのtitleとdescriptionが収集できると以下のようなデータになります。</p>\n\n<table>\n<thead>\n<tr>\n<th>  shop_id  </th>\n<th>  手入力カテゴリ  </th>\n<th> title</th>\n<th> description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>  A  </td>\n<td>  カテゴリA  </td>\n<td> レトロな服を買うならA  </td>\n<td> レトロな服を買うならAで決まり!!お問い合わせは...  </td>\n</tr>\n<tr>\n<td>  B  </td>\n<td>  カテゴリB  </td>\n<td> 高級食材のB </td>\n<td> 高級食材を買うならBが安い!!  </td>\n</tr>\n<tr>\n<td>  C  </td>\n<td>  カテゴリC  </td>\n<td> 車のパーツはC </td>\n<td> パーツの取付も行います!!  </td>\n</tr>\n</tbody>\n</table>\n\n\n<p>すべてのtitleとdescriptionを収集すると、手動でカテゴリを定めたデータ数と等しいデータ数になるので約470の学習データが集まったことになります。</p>\n\n<h1>学習</h1>\n\n<p>今回はBERTを用いて学習・予測を行いました。</p>\n\n<p><a href=\"https://arxiv.org/abs/1810.04805\">BERT（Bidirectional Encoder Representations from Transformers）</a>とは2018年にGoogleから発表された自然言語モデルです。</p>\n\n<p>また、私自身BERTに精通しているわけではないので、今回は勉強も兼ねて「<a href=\"https://www.amazon.co.jp/dp/B098J9M4PP/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1\">BERTによる自然言語処理入門 ―Transformersを使った実践プログラミング</a>」を参考に学習を実施しました。</p>\n\n<p>第6章にBERTによる文章分類というものがあるので、そちらを一部書き換えて使用しました。</p>\n\n<h1>工夫したポイント</h1>\n\n<p>モデルやpre-trained modelは本で紹介されている通りのものを使用しましたが、<strong>18カテゴリへの分類問題で正解率が62%前後しか確保できていないため、この精度では運用できない</strong>と判断し、以下のような工夫を行いました。</p>\n\n<h2>工夫したポイント①　2種類の予測モデルを用いる</h2>\n\n<p>精度改善のために元々1つの予測モデルで学習・予測行っていたものを、2つの予測モデルで行うように変更しました。それぞれの予測モデルは以下のような役割を担っています。</p>\n\n<table>\n<thead>\n<tr>\n<th>  予測モデル  </th>\n<th>  役割  </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>  予測モデル1  </td>\n<td>  データ量が多いカテゴリを予測するモデル。簡単な予測問題を間違えない。また、データ数が少ないカテゴリはまとめて「その他」と予測する。  </td>\n</tr>\n<tr>\n<td>  予測モデル2  </td>\n<td>  予測モデル1で「その他」と予測されたデータを、それぞれのカテゴリに割り振る予測モデル。データ数が少なく難しいカテゴリに対して予測を行うので正解率は低い。  </td>\n</tr>\n</tbody>\n</table>\n\n\n<p>手動で割り振られた約470データのカテゴリは以下のようになっており、<strong>学習に使用できるデータ量がカテゴリごとにばらつきが大きいことがわかります。</strong> <strong>またデータ量が少ないカテゴリは、多くの場合低い精度を示していました。</strong></p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220216/20220216105400.png\" alt=\"f:id:newton800:20220216105400p:plain\" width=\"601\" height=\"372\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p><br></p>\n\n<p>予測モデル1では、データ量が少ないカテゴリは「その他」とまとめてしまい学習を行いました。予測モデル2では「その他」をより細分化するための学習を行いました。</p>\n\n<p><br></p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220216/20220216111039.png\" alt=\"f:id:newton800:20220216111039p:plain\" width=\"951\" height=\"393\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p><br>\nしかしながら、<strong>この方法だけではデータ量が少ないカテゴリを学習している予測モデル2はこれまで通り低い精度でした</strong>ので、以下に示す工夫ポイント2を導入することにしました。</p>\n\n<h2>工夫したポイント②　 自信のない予測は、後で人手で確認するようなフローにする</h2>\n\n<p>これまでは出力されたカテゴリの全てを信頼するようにしていましたが、<strong>一定の信頼値以下の予測に関しては後で人手で確認するというフローを導入</strong>しました。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220216/20220216111955.png\" alt=\"f:id:newton800:20220216111955p:plain\" width=\"951\" height=\"441\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>そうすることで、<strong>予測結果の信頼値が高いものは予測結果がそのまま採用され、信頼値が低いものは人が後で確認する</strong>という流れになりました。また人が後で確認してカテゴリを定めたデータは、学習モデルの精度向上のための再学習に使用することができます。</p>\n\n<h2>工夫したポイントを導入した結果</h2>\n\n<p>元々正解率が62%だったものが上に示したような<strong>工夫ポイントを導入することで正解率は91%まで向上しました。</strong>また、そのうち2割ほどが人手での再確認を要するものとなりました。</p>\n\n<h1>さいごに</h1>\n\n<p>工夫したポイント②の「自信のない予測は、後で人手で確認するようなフローにする」はちょっとせこいのではないか？と思われる方もいるかもしれません。しかし、残りの全てのクライントに手動で割り振ることと比較すると業務量としてはかなり削減できることになっています。（8割軽減）</p>\n\n<p><strong>「機械にできる簡単なタスクは機械に任せてしまって、人間は本当に考えないといけないような業務に集中できる」ような分析やサービス作りに携わっていきたいなぁと改めて感じた業務でした。</strong></p>\n\n<p>最後までありがとうございました。</p>\n\n<p>P.S. 工夫ポイント1はlossの設計をうまくやれば不要だったりするのかなぁなんて思ったり...</p>\n","contentSnippet":"こんにちは　 データサイエンティストの八百俊哉です。最近は家でカクテルを作ることにはまっています。今回はEC BoosterのクライントのECサイトのタイトルと説明文を用いることで、どのカテゴリの商品を扱っているのか予測する仕組みを作成しましたので、その方法を紹介します。弊社のサービスEC Boosterは、Google ショッピング広告の自動運用による自社EC自動集客サービスです。主要ECシステムと連携することで、Google の検索結果画面に画像付きで自社商品を訴求することが可能となります。分析背景EC Boosterを利用いただくことで、Google ショッピング広告を自動運用することができます。Googleショッピング広告では広告出稿する際に、Googleが自動的にそれぞれの商品にgoogle product categoryというカテゴリ情報を割り当てます。support.google.comしかしながら、Googleが自動的に割り振った商品ごとのカテゴリ情報はEC Booster側からは確認することができません。Google's category of the item (see Google product taxonomy). When querying products, this field will contain the user provided value. There is currently no way to get back the auto assigned google product categories through the API.https://developers.google.com/shopping-content/reference/rest/v2.1/products?hl=en#Product.FIELDS.google_product_category商品ごとに割り振られたカテゴリがわからないと、それぞれのクライントがどのような商品を扱っているのかわからなくなります。その結果、クライアントの実績を取扱商品カテゴリごとに集計したり、扱っているカテゴリごとのサポートを行ったりすることができなくなってしまいます。そこで今回はクライントごとに取扱商品カテゴリをEC Booster側で予測することで、上に挙げたような課題を解決するような仕組みを作成しました。学習・予測に必要なデータの収集EC Boosterは現在有料プラン約300のクライントとフリープラン約2000のクライントにご利用いただいております。（2022年5月第2四半期 決算説明資料より）学習データ生成のために、有料プランユーザーを中心に約470のクライントに対して手動でカテゴリを割り振るという作業を実施しました。また、このデータの作成はそれぞれのクライントのホームページを目視で確認し、適切なカテゴリを割り振りました。次に学習・予測に必要な入力データの収集を行いました。今回はそれぞれのクライントのタイトルと説明文を用いて学習をします。そのため以下に示すようにそれぞれのホームページからtitleとdescriptionのスクレイピングを行いました。タイトル(title)、説明文(description)の例それぞれのtitleとdescriptionが収集できると以下のようなデータになります。  shop_id    手入力カテゴリ   title description  A    カテゴリA   レトロな服を買うならA   レトロな服を買うならAで決まり!!お問い合わせは...    B    カテゴリB   高級食材のB  高級食材を買うならBが安い!!    C    カテゴリC   車のパーツはC  パーツの取付も行います!!  すべてのtitleとdescriptionを収集すると、手動でカテゴリを定めたデータ数と等しいデータ数になるので約470の学習データが集まったことになります。学習今回はBERTを用いて学習・予測を行いました。BERT（Bidirectional Encoder Representations from Transformers）とは2018年にGoogleから発表された自然言語モデルです。また、私自身BERTに精通しているわけではないので、今回は勉強も兼ねて「BERTによる自然言語処理入門 ―Transformersを使った実践プログラミング」を参考に学習を実施しました。第6章にBERTによる文章分類というものがあるので、そちらを一部書き換えて使用しました。工夫したポイントモデルやpre-trained modelは本で紹介されている通りのものを使用しましたが、18カテゴリへの分類問題で正解率が62%前後しか確保できていないため、この精度では運用できないと判断し、以下のような工夫を行いました。工夫したポイント①　2種類の予測モデルを用いる精度改善のために元々1つの予測モデルで学習・予測行っていたものを、2つの予測モデルで行うように変更しました。それぞれの予測モデルは以下のような役割を担っています。  予測モデル    役割    予測モデル1    データ量が多いカテゴリを予測するモデル。簡単な予測問題を間違えない。また、データ数が少ないカテゴリはまとめて「その他」と予測する。    予測モデル2    予測モデル1で「その他」と予測されたデータを、それぞれのカテゴリに割り振る予測モデル。データ数が少なく難しいカテゴリに対して予測を行うので正解率は低い。  手動で割り振られた約470データのカテゴリは以下のようになっており、学習に使用できるデータ量がカテゴリごとにばらつきが大きいことがわかります。 またデータ量が少ないカテゴリは、多くの場合低い精度を示していました。予測モデル1では、データ量が少ないカテゴリは「その他」とまとめてしまい学習を行いました。予測モデル2では「その他」をより細分化するための学習を行いました。しかしながら、この方法だけではデータ量が少ないカテゴリを学習している予測モデル2はこれまで通り低い精度でしたので、以下に示す工夫ポイント2を導入することにしました。工夫したポイント②　 自信のない予測は、後で人手で確認するようなフローにするこれまでは出力されたカテゴリの全てを信頼するようにしていましたが、一定の信頼値以下の予測に関しては後で人手で確認するというフローを導入しました。そうすることで、予測結果の信頼値が高いものは予測結果がそのまま採用され、信頼値が低いものは人が後で確認するという流れになりました。また人が後で確認してカテゴリを定めたデータは、学習モデルの精度向上のための再学習に使用することができます。工夫したポイントを導入した結果元々正解率が62%だったものが上に示したような工夫ポイントを導入することで正解率は91%まで向上しました。また、そのうち2割ほどが人手での再確認を要するものとなりました。さいごに工夫したポイント②の「自信のない予測は、後で人手で確認するようなフローにする」はちょっとせこいのではないか？と思われる方もいるかもしれません。しかし、残りの全てのクライントに手動で割り振ることと比較すると業務量としてはかなり削減できることになっています。（8割軽減）「機械にできる簡単なタスクは機械に任せてしまって、人間は本当に考えないといけないような業務に集中できる」ような分析やサービス作りに携わっていきたいなぁと改めて感じた業務でした。最後までありがとうございました。P.S. 工夫ポイント1はlossの設計をうまくやれば不要だったりするのかなぁなんて思ったり...","link":"https://developer.feedforce.jp/entry/2022/02/21/104757","isoDate":"2022-02-21T01:47:57.000Z","dateMiliSeconds":1645408077000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"yaonyan"},{"title":"操作ログを用いた行動パターン変化の検知","content":"<p>こんにちは　\nデータサイエンティストの<a href=\"https://twitter.com/feed_yao\">八百俊哉</a>です。\n最近は花粉が飛んでいることを感じています。目がかゆい。</p>\n\n<p>今回はdfplus.ioの操作ログを用いてクライアントの行動パターン変化を検知する仕組みを作成し、社内で有効活用された事例がありますので、その手法と成果を紹介します。</p>\n\n<p>弊社のサービス<a href=\"https://dfplus.io/\">dfplus.io</a>はマーケティング・広告運用チームのためのデータフィード管理ツールになります。様々な商品・商材データをマーケティングでフル活用できるようになるサービスです。</p>\n\n<h1>分析背景</h1>\n\n<p>dfplus.ioでの解約理由の一つに<strong>「クライアントの担当者が変更する際に引き継ぎがうまくできず、dfplus.ioの使い方がわからなくなってしまう」</strong>というケースがありました。</p>\n\n<p>dfplus.ioカスタマーサクセスチーム（※以下CSチーム）では、<strong>クライアント側から事前に担当者の変更のご連絡を頂いた場合は新しいご担当者様にdfplus.ioの操作や活用に関するご案内をしております。しかし、クライアント側からご連絡がない場合は担当者の変更に気付くことができずそのご案内ができないという課題</strong>がありました。</p>\n\n<p>その為なんらかの方法でクライアントの担当者の変更を検知することができると、新しい担当者にCSチームからdfplus.ioの操作や活用に関するご案内をすることができます。</p>\n\n<p>そこで今回は<strong>操作ログを用いて行動パターンの変化を検知するという分析を実施し、クライアントの担当者の変更に気がつけるようにしました。</strong></p>\n\n<h1>操作ログを用いて行動パターンを可視化</h1>\n\n<p>dfplus.ioには全部で90個のAPIが存在します。どのアカウントがいつどのAPIを使用したのかのデータがすべて蓄積されている状態です。以下が実際に今回の分析で使用したデータになります。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"実際に使用した元データ\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220208/20220208141429.png\" alt=\"f:id:newton800:20220208141429p:plain\" width=\"643\" height=\"173\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>実際に使用した元データ</figcaption></figure></p>\n\n<p>今回はこちらのデータを用いて各クライアントの行動パターンを作成しました。以下が実際の操作ログを用いて作成できた行動パターンの可視化例です。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220209/20220209092518.png\" alt=\"f:id:newton800:20220209092518p:plain\" width=\"778\" height=\"531\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>それぞれの<strong>APIに0~89の通し番号を割り振り、x軸</strong>としました。また、<strong>y軸には日数を使用</strong>して上のように可視化しました。</p>\n\n<p>1日のうち1度でもAPIを操作すれば、(x(API番号), y(操作日)) = 1となるようにしており、画素値は0か1しか取らないようになっています。(画像で表現するときは1を255としている。)</p>\n\n<p><strong>それらのデータを月ごとに切り分け、前月とその前の月の行動パターンを比較することで、行動パターンの変化を確認しました。</strong></p>\n\n<p>上の画像の例だと2021年12月と2022年01月を比較することで、変化の検知を行うということになります。</p>\n\n<h1>行動パターン変化の定義</h1>\n\n<p>ここから2つの月の行動パターンを比較していきます。</p>\n\n<p>はじめは画像同士をそのまま引き算することで変化量を求めようと思いました。しかし、仮にdfplus.ioを使用する業務が1日遅れになってしまったクライアントがいた場合、同じ作業をしているにも関わらず、行動パターン変化量が大きくなってしまいます。</p>\n\n<p>そこで行動パターン変化量(<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20d\" alt=\" d\"/>)を以下のように定義しました。</p>\n\n<div align=\"center\">\n<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20d%20%3D%20%20%5Cdfrac%7B%5Csum%20%5E%7B89%7D_%7Bx%3D0%7D%5Cleft%28%20%5Csum%20%5E%7B30%7D_%7By%3D0%7Dp_%7Bx%2Cy%7D%5E%7Bbetore%7D-%5Csum%20%5E%7B30%7D_%7By%3D0%7Dp_%7Bx%2Cy%7D%5E%7Bafter%7D%5Cright%29%20%5E%7B2%7D%7D%7B%5Cleft%28%20%5Csum%20%5E%7B89%7D_%7Bx%3D0%7D%5Csum%20%5E%7B30%7D_%7By%3D0%7Dp_%7Bx%2Cy%7D%5E%7Bbetore%7D%2B%5Csum%20%5E%7B89%7D_%7Bx%3D0%7D%5Csum%20%5E%7B30%7D_%7By%3D0%7Dp_%7Bx%2Cy%7D%5E%7Bafter%7D%5Cright%29%20%7D%5C%5C%20\" alt=\" d =  \\dfrac{\\sum ^{89}_{x=0}\\left( \\sum ^{30}_{y=0}p_{x,y}^{betore}-\\sum ^{30}_{y=0}p_{x,y}^{after}\\right) ^{2}}{\\left( \\sum ^{89}_{x=0}\\sum ^{30}_{y=0}p_{x,y}^{betore}+\\sum ^{89}_{x=0}\\sum ^{30}_{y=0}p_{x,y}^{after}\\right) }\\\\ \"/>\n</div>\n\n\n<p></p></p>\n\n<div align=\"center\">\n<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%0Ap_%7Bx%2Cy%7D%20%3D%20%5Cleft%5C%7B%0A%5Cbegin%7Barray%7D%7Bll%7D%0A1%20%26%20%E6%93%8D%E4%BD%9C%E3%81%8C%E3%81%82%E3%81%A3%E3%81%9F%E5%A0%B4%E5%90%88%5C%5C%0A0%20%26%20%E6%93%8D%E4%BD%9C%E3%81%8C%E3%81%AA%E3%81%8B%E3%81%A3%E3%81%9F%E5%A0%B4%E5%90%88%0A%5Cend%7Barray%7D%0A%5Cright.%0A\" alt=\" \np_{x,y} = \\left\\{\n\\begin{array}{ll}\n1 &amp; &#x64CD;&#x4F5C;&#x304C;&#x3042;&#x3063;&#x305F;&#x5834;&#x5408;\\\\\n0 &amp; &#x64CD;&#x4F5C;&#x304C;&#x306A;&#x304B;&#x3063;&#x305F;&#x5834;&#x5408;\n\\end{array}\n\\right.\n\"/>\n</div>\n\n\n<p></p></p>\n\n<p>もともとは分子の部分だけを用いて行動パターン変化量と定義していましたが、そうすると必然的に操作頻度が多いユーザーの変化量が多くなってしまい、月に数度しか使用しないクライアントの行動パターン変化量がとても小さくなってしまうという課題がありました。</p>\n\n<p>そこで上のように分母に全体の操作頻度を導入することで、操作頻度による評価のばらつきを削減しました。</p>\n\n<p>行動パターン変化量(<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20d\" alt=\" d\"/>)がいくつ以上になれば、行動パターンが変化したと言えるかについては、CSチームと<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20d\" alt=\" d\"/>と行動パターンの可視化画像を確認の上、決定しております。今後もその閾値については調整を行っていく予定になります。</p>\n\n<h1>実際に検知できた例</h1>\n\n<p><strong>これらの仕組みを使用することで毎月10件前後の変化検知があります。\nその検知結果をCSチームで確認し、ご案内が必要だと判断した場合にCSチームからクライアントに連絡を実施するという流れになっています。</strong></p>\n\n<p>以下は実際に担当者の変更の検知に成功した事例です。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220208/20220208160800.png\" alt=\"f:id:newton800:20220208160800p:plain\" width=\"721\" height=\"361\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>可視化された行動パターンからも確認できるように、<strong>2021年11月までは頻繁に使用されていましたが、2021月12月の前半以降は使用回数が激減しています。</strong></p>\n\n<p><strong>実際にこちらのクライアントは担当者が退職されており、担当者の変更検知の成功例となりました。</strong></p>\n\n<p>以下は、実際にCSチームから頂いた連絡です。\n<span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220208/20220208161732.png\" alt=\"f:id:newton800:20220208161732p:plain\" width=\"883\" height=\"251\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h1>さいごに</h1>\n\n<p>今回は操作ログを用いた行動パターン変化の検知について紹介しました。\nこちらの分析案件は、ヒアリングから現在まで1年ほどかけて現状の形になりました。入社2年目の私にとってはかなり長い間実施した業務になります。</p>\n\n<p>今回は触れられていませんが、現在に至るまで紆余曲折ありました。最後まで諦めずに現在の形にできて非常に嬉しく感じます。</p>\n\n<p>現在運用から4ヶ月ほど経過しており、実際に担当者の変更が検知できた事例が2件となっています。引き続き運用を続けていくことで、より多くの成果が残せると考えています。</p>\n\n<p>最後までお付き合いいただき、ありがとうございました。</p>\n","contentSnippet":"こんにちは　データサイエンティストの八百俊哉です。最近は花粉が飛んでいることを感じています。目がかゆい。今回はdfplus.ioの操作ログを用いてクライアントの行動パターン変化を検知する仕組みを作成し、社内で有効活用された事例がありますので、その手法と成果を紹介します。弊社のサービスdfplus.ioはマーケティング・広告運用チームのためのデータフィード管理ツールになります。様々な商品・商材データをマーケティングでフル活用できるようになるサービスです。分析背景dfplus.ioでの解約理由の一つに「クライアントの担当者が変更する際に引き継ぎがうまくできず、dfplus.ioの使い方がわからなくなってしまう」というケースがありました。dfplus.ioカスタマーサクセスチーム（※以下CSチーム）では、クライアント側から事前に担当者の変更のご連絡を頂いた場合は新しいご担当者様にdfplus.ioの操作や活用に関するご案内をしております。しかし、クライアント側からご連絡がない場合は担当者の変更に気付くことができずそのご案内ができないという課題がありました。その為なんらかの方法でクライアントの担当者の変更を検知することができると、新しい担当者にCSチームからdfplus.ioの操作や活用に関するご案内をすることができます。そこで今回は操作ログを用いて行動パターンの変化を検知するという分析を実施し、クライアントの担当者の変更に気がつけるようにしました。操作ログを用いて行動パターンを可視化dfplus.ioには全部で90個のAPIが存在します。どのアカウントがいつどのAPIを使用したのかのデータがすべて蓄積されている状態です。以下が実際に今回の分析で使用したデータになります。実際に使用した元データ今回はこちらのデータを用いて各クライアントの行動パターンを作成しました。以下が実際の操作ログを用いて作成できた行動パターンの可視化例です。それぞれのAPIに0~89の通し番号を割り振り、x軸としました。また、y軸には日数を使用して上のように可視化しました。1日のうち1度でもAPIを操作すれば、(x(API番号), y(操作日)) = 1となるようにしており、画素値は0か1しか取らないようになっています。(画像で表現するときは1を255としている。)それらのデータを月ごとに切り分け、前月とその前の月の行動パターンを比較することで、行動パターンの変化を確認しました。上の画像の例だと2021年12月と2022年01月を比較することで、変化の検知を行うということになります。行動パターン変化の定義ここから2つの月の行動パターンを比較していきます。はじめは画像同士をそのまま引き算することで変化量を求めようと思いました。しかし、仮にdfplus.ioを使用する業務が1日遅れになってしまったクライアントがいた場合、同じ作業をしているにも関わらず、行動パターン変化量が大きくなってしまいます。そこで行動パターン変化量()を以下のように定義しました。もともとは分子の部分だけを用いて行動パターン変化量と定義していましたが、そうすると必然的に操作頻度が多いユーザーの変化量が多くなってしまい、月に数度しか使用しないクライアントの行動パターン変化量がとても小さくなってしまうという課題がありました。そこで上のように分母に全体の操作頻度を導入することで、操作頻度による評価のばらつきを削減しました。行動パターン変化量()がいくつ以上になれば、行動パターンが変化したと言えるかについては、CSチームとと行動パターンの可視化画像を確認の上、決定しております。今後もその閾値については調整を行っていく予定になります。実際に検知できた例これらの仕組みを使用することで毎月10件前後の変化検知があります。その検知結果をCSチームで確認し、ご案内が必要だと判断した場合にCSチームからクライアントに連絡を実施するという流れになっています。以下は実際に担当者の変更の検知に成功した事例です。可視化された行動パターンからも確認できるように、2021年11月までは頻繁に使用されていましたが、2021月12月の前半以降は使用回数が激減しています。実際にこちらのクライアントは担当者が退職されており、担当者の変更検知の成功例となりました。以下は、実際にCSチームから頂いた連絡です。さいごに今回は操作ログを用いた行動パターン変化の検知について紹介しました。こちらの分析案件は、ヒアリングから現在まで1年ほどかけて現状の形になりました。入社2年目の私にとってはかなり長い間実施した業務になります。今回は触れられていませんが、現在に至るまで紆余曲折ありました。最後まで諦めずに現在の形にできて非常に嬉しく感じます。現在運用から4ヶ月ほど経過しており、実際に担当者の変更が検知できた事例が2件となっています。引き続き運用を続けていくことで、より多くの成果が残せると考えています。最後までお付き合いいただき、ありがとうございました。","link":"https://developer.feedforce.jp/entry/2022/02/09/110824","isoDate":"2022-02-09T02:08:24.000Z","dateMiliSeconds":1644372504000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"yaonyan"},{"title":"Causal Impactを用いた入札単価調整の効果検証","content":"<p>こんにちは　機械学習エンジニアの<a href=\"https://twitter.com/feed_yao\">八百俊哉</a>です。最近はバレーボールをやることにハマっており、激しめに運動しています。今日も筋肉痛です。</p>\n\n<p>今回はGoogle広告の入札単価調整が広告成果にどのような影響を与えるのかCausal Impactを用いて検証を実施しましたので、その結果を共有したいと思います。</p>\n\n<h1>分析背景</h1>\n\n<p>弊社のサービス<a href=\"https://ecbooster.jp/\">EC Booster</a>は、Google ショッピング広告の自動運用による自社EC自動集客サービスです。主要ECシステムと連携することで、Google の検索結果画面に画像付きで自社商品を訴求することが可能となります。</p>\n\n<p>Google 広告には<a href=\"https://support.google.com/google-ads/answer/2732132?hl=ja\">入札単価調整</a>という機能があり、それをショッピング広告でも使用することによって効率的に広告配信を行うことができるとされています。\nそこで<strong>今回はEC Boosterで入札単価調整を実施することで、どれほど広告の実績がよくなるのかを検証しました。</strong></p>\n\n<h1>Causal Impactを用いた効果検証</h1>\n\n<p>今回使用した効果検証の方法である<a href=\"https://google.github.io/CausalImpact/CausalImpact.html\">Causal Impact</a>をご紹介します。</p>\n\n<p><strong>Causal ImpactとはGoogleが作成したベイズ構造時系列モデルを使用した因果推論のためのパッケージになります。</strong></p>\n\n<p>仕組みを簡単に表現すると、あるイベントが介入した日を境に「過去の実績から推定される反事実」と「実際に観測された事実」を比較することによって、イベントの効果の大きさを測ると言うものです。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211215/20211215120354.png\" alt=\"f:id:newton800:20211215120354p:plain\" width=\"761\" height=\"261\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>今回は実際に検証したことを紹介する記事になりますので、これ以上詳しくCausal Impactの仕組みについては言及しないです。\nより詳しくCausal Impactについて知りたい方は以下の書籍が参考になるので是非読んでみてください。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fwww.amazon.co.jp%2F%25E3%2583%2587%25E3%2583%25BC%25E3%2582%25BF%25E8%25A7%25A3%25E6%259E%2590%25E3%2581%25AE%25E3%2581%259F%25E3%2582%2581%25E3%2581%25AE%25E7%25B5%25B1%25E8%25A8%2588%25E3%2583%25A2%25E3%2583%2587%25E3%2583%25AA%25E3%2583%25B3%25E3%2582%25B0%25E5%2585%25A5%25E9%2596%2580%25E2%2580%2595%25E2%2580%2595%25E4%25B8%2580%25E8%2588%25AC%25E5%258C%2596%25E7%25B7%259A%25E5%25BD%25A2%25E3%2583%25A2%25E3%2583%2587%25E3%2583%25AB%25E3%2583%25BB%25E9%259A%258E%25E5%25B1%25A4%25E3%2583%2599%25E3%2582%25A4%25E3%2582%25BA%25E3%2583%25A2%25E3%2583%2587%25E3%2583%25AB%25E3%2583%25BBMCMC-%25E7%25A2%25BA%25E7%258E%2587%25E3%2581%25A8%25E6%2583%2585%25E5%25A0%25B1%25E3%2581%25AE%25E7%25A7%2591%25E5%25AD%25A6-%25E4%25B9%2585%25E4%25BF%259D-%25E6%258B%2593%25E5%25BC%25A5%2Fdp%2F400006973X\" title=\"データ解析のための統計モデリング入門――一般化線形モデル・階層ベイズモデル・MCMC (確率と情報の科学) | 久保 拓弥 |本 | 通販 | Amazon\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://www.amazon.co.jp/%E3%83%87%E3%83%BC%E3%82%BF%E8%A7%A3%E6%9E%90%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E7%B5%B1%E8%A8%88%E3%83%A2%E3%83%87%E3%83%AA%E3%83%B3%E3%82%B0%E5%85%A5%E9%96%80%E2%80%95%E2%80%95%E4%B8%80%E8%88%AC%E5%8C%96%E7%B7%9A%E5%BD%A2%E3%83%A2%E3%83%87%E3%83%AB%E3%83%BB%E9%9A%8E%E5%B1%A4%E3%83%99%E3%82%A4%E3%82%BA%E3%83%A2%E3%83%87%E3%83%AB%E3%83%BBMCMC-%E7%A2%BA%E7%8E%87%E3%81%A8%E6%83%85%E5%A0%B1%E3%81%AE%E7%A7%91%E5%AD%A6-%E4%B9%85%E4%BF%9D-%E6%8B%93%E5%BC%A5/dp/400006973X\">www.amazon.co.jp</a></cite></p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fwww.amazon.co.jp%2F%25E6%2599%2582%25E7%25B3%25BB%25E5%2588%2597%25E5%2588%2586%25E6%259E%2590%25E3%2581%25A8%25E7%258A%25B6%25E6%2585%258B%25E7%25A9%25BA%25E9%2596%2593%25E3%2583%25A2%25E3%2583%2587%25E3%2583%25AB%25E3%2581%25AE%25E5%259F%25BA%25E7%25A4%258E-R%25E3%2581%25A8Stan%25E3%2581%25A7%25E5%25AD%25A6%25E3%2581%25B6%25E7%2590%2586%25E8%25AB%2596%25E3%2581%25A8%25E5%25AE%259F%25E8%25A3%2585-%25E9%25A6%25AC%25E5%25A0%25B4-%25E7%259C%259F%25E5%2593%2589%2Fdp%2F4903814874\" title=\"時系列分析と状態空間モデルの基礎: RとStanで学ぶ理論と実装 | 真哉, 馬場 |本 | 通販 | Amazon\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://www.amazon.co.jp/%E6%99%82%E7%B3%BB%E5%88%97%E5%88%86%E6%9E%90%E3%81%A8%E7%8A%B6%E6%85%8B%E7%A9%BA%E9%96%93%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%9F%BA%E7%A4%8E-R%E3%81%A8Stan%E3%81%A7%E5%AD%A6%E3%81%B6%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%A3%85-%E9%A6%AC%E5%A0%B4-%E7%9C%9F%E5%93%89/dp/4903814874\">www.amazon.co.jp</a></cite></p>\n\n<h1>入札単価調整はコンバージョン率に良い効果をもたらしている</h1>\n\n<p>入札単価調整を実施することで、広告効果の高いユーザーに対して積極的に入札を行うようになり、逆に広告効果の低いユーザーに対しては入札を控えるようになります。</p>\n\n<p>そのようにすることで、コンバージョン率の向上が期待されていました。コンバージョン率が向上するとコンバージョン数が上がり、最終的には費用対効果であるROASの向上までが考えられます。</p>\n\n<p>実際に分析前に立てられた仮説は以下です。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"分析前に立てた仮説\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211221/20211221153325.png\" alt=\"f:id:newton800:20211221153325p:plain\" width=\"691\" height=\"171\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>分析前に立てた仮説</figcaption></figure></p>\n\n<p>それでは実際に入札単価調整を実施することで、広告成果がどう変化したのかを紹介します。</p>\n\n<p>入札単価調整を行う前の実績と入札単価調整を行った後の実績を用いてCausal Impactを実施しました。</p>\n\n<p>その結果以下のようにそれぞれの広告成果に入札単価調整が影響を与えているということがわかりました。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"分析を実施することで分かった結果の一部抜粋\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211221/20211221153346.png\" alt=\"f:id:newton800:20211221153346p:plain\" width=\"691\" height=\"171\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>分析を実施することで分かった結果の一部抜粋</figcaption></figure></p>\n\n<p>結果から<strong>コンバージョン率が向上することによってコンバージョン数が向上していることが確認されました。</strong>（他の要素も向上していますが、今回はコンバージョン率に焦点を当てています）</p>\n\n<p>実際にCVRに関してのCausal Impactの実行結果は以下のようになっていました。（一部公開できない部分は黒くしてます）</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211215/20211215123528.png\" alt=\"f:id:newton800:20211215123528p:plain\" width=\"1180\" height=\"590\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>上段に図示された実線が実際に観測されたコンバージョン率で青い点線が過去の傾向を元にした予測コンバージョン率になります。入札単価調を開始した縦の点線の位置から、予測コンバージョン率（入札単価調整を仮にやっていない場合）よりも実際に観測されたコンバージョン率（入札単価調整を実施した場合）の方が大きくなっていることがわかります。</p>\n\n<p>中段に図示されたものが入札単価調整の効果となります。入札単価調整を実施してから、コンバージョン率が増加したといえそうです。</p>\n\n<p>下段に図示されたものは増加したコンバージョン率の累計値になります。</p>\n\n<p><strong>このようなことから入札単価調整は、コンバージョン率向上に貢献していることがわかりました。</strong></p>\n\n<h1>そして現在入札単価調整は</h1>\n\n<p>このような分析もあり、EC Boosterでは現在入札単価調整は全てのユーザーに対して適用されるように設定されています。</p>\n\n<p>このようにEC Boosterの裏側では効果検証を繰り返し、より広告成果が良くなるように最適化を実施しています。</p>\n\n<h1>さいごに</h1>\n\n<p>最後までお付き合いいただきありがとうございます。\n弊社のデータ分析チームに新卒入社してはや2年弱ほど経過し、少しずつ成果が出せるようになってきました。</p>\n\n<p>今後はより積極的にデータ分析に関する情報発信を行っていきたいと考えています。よろしくお願いします。</p>\n\n<p>P.S.祖父母に自分の仕事伝えるのが難しいです。</p>\n","contentSnippet":"こんにちは　機械学習エンジニアの八百俊哉です。最近はバレーボールをやることにハマっており、激しめに運動しています。今日も筋肉痛です。今回はGoogle広告の入札単価調整が広告成果にどのような影響を与えるのかCausal Impactを用いて検証を実施しましたので、その結果を共有したいと思います。分析背景弊社のサービスEC Boosterは、Google ショッピング広告の自動運用による自社EC自動集客サービスです。主要ECシステムと連携することで、Google の検索結果画面に画像付きで自社商品を訴求することが可能となります。Google 広告には入札単価調整という機能があり、それをショッピング広告でも使用することによって効率的に広告配信を行うことができるとされています。そこで今回はEC Boosterで入札単価調整を実施することで、どれほど広告の実績がよくなるのかを検証しました。Causal Impactを用いた効果検証今回使用した効果検証の方法であるCausal Impactをご紹介します。Causal ImpactとはGoogleが作成したベイズ構造時系列モデルを使用した因果推論のためのパッケージになります。仕組みを簡単に表現すると、あるイベントが介入した日を境に「過去の実績から推定される反事実」と「実際に観測された事実」を比較することによって、イベントの効果の大きさを測ると言うものです。今回は実際に検証したことを紹介する記事になりますので、これ以上詳しくCausal Impactの仕組みについては言及しないです。より詳しくCausal Impactについて知りたい方は以下の書籍が参考になるので是非読んでみてください。www.amazon.co.jpwww.amazon.co.jp入札単価調整はコンバージョン率に良い効果をもたらしている入札単価調整を実施することで、広告効果の高いユーザーに対して積極的に入札を行うようになり、逆に広告効果の低いユーザーに対しては入札を控えるようになります。そのようにすることで、コンバージョン率の向上が期待されていました。コンバージョン率が向上するとコンバージョン数が上がり、最終的には費用対効果であるROASの向上までが考えられます。実際に分析前に立てられた仮説は以下です。分析前に立てた仮説それでは実際に入札単価調整を実施することで、広告成果がどう変化したのかを紹介します。入札単価調整を行う前の実績と入札単価調整を行った後の実績を用いてCausal Impactを実施しました。その結果以下のようにそれぞれの広告成果に入札単価調整が影響を与えているということがわかりました。分析を実施することで分かった結果の一部抜粋結果からコンバージョン率が向上することによってコンバージョン数が向上していることが確認されました。（他の要素も向上していますが、今回はコンバージョン率に焦点を当てています）実際にCVRに関してのCausal Impactの実行結果は以下のようになっていました。（一部公開できない部分は黒くしてます）上段に図示された実線が実際に観測されたコンバージョン率で青い点線が過去の傾向を元にした予測コンバージョン率になります。入札単価調を開始した縦の点線の位置から、予測コンバージョン率（入札単価調整を仮にやっていない場合）よりも実際に観測されたコンバージョン率（入札単価調整を実施した場合）の方が大きくなっていることがわかります。中段に図示されたものが入札単価調整の効果となります。入札単価調整を実施してから、コンバージョン率が増加したといえそうです。下段に図示されたものは増加したコンバージョン率の累計値になります。このようなことから入札単価調整は、コンバージョン率向上に貢献していることがわかりました。そして現在入札単価調整はこのような分析もあり、EC Boosterでは現在入札単価調整は全てのユーザーに対して適用されるように設定されています。このようにEC Boosterの裏側では効果検証を繰り返し、より広告成果が良くなるように最適化を実施しています。さいごに最後までお付き合いいただきありがとうございます。弊社のデータ分析チームに新卒入社してはや2年弱ほど経過し、少しずつ成果が出せるようになってきました。今後はより積極的にデータ分析に関する情報発信を行っていきたいと考えています。よろしくお願いします。P.S.祖父母に自分の仕事伝えるのが難しいです。","link":"https://developer.feedforce.jp/entry/2021/12/21/154206","isoDate":"2021-12-21T06:42:06.000Z","dateMiliSeconds":1640068926000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211215/20211215120354.png","authorName":"yaonyan"},{"title":"広告の複数媒体に対するCPA最小化・ROAS最大化となる予算配分を計算しよう","content":"<p>こんにちは　機械学習エンジニアの<a href=\"https://twitter.com/feed_yao\">八百俊哉</a>です。</p>\n\n<p>今回は複数媒体へ広告を出稿する際に、多くの方が悩まれるであろう「各媒体への予算配分」に関して有効な配分手法を紹介します。</p>\n\n<p><strong>今回の記事で登場する広告用語</strong></p>\n\n<ul>\n<li>媒体・・・広告の配信先や配信手法</li>\n<li>ROAS・・・広告経由で発生した売り上げを広告費用で割った値(広告の費用対効果)</li>\n<li>CPA・・・1件のコンバージョン(目標)を獲得するのにかかった広告コスト</li>\n</ul>\n\n\n<h1>広告運用者が抱える課題とは？</h1>\n\n<p>1つの媒体のみで運用している場合は別ですが、<strong>複数の媒体で広告配信を行っている場合は、どの媒体に対していくら予算を割り振れば良いのかわからない場合があると思います。</strong></p>\n\n<p>過去の実績を元に成果が良い媒体に対して、多く予算を割り振れば良いことは理解しているものの、<strong>「どれくらい」「どの媒体から」予算を割り振れば良いのか</strong>は経験則や簡単な分析で決めている方も多いのではないでしょうか？</p>\n\n<p>今回はこれらの課題を解決するために、<strong>数学的に根拠のある予算配分方法</strong>について紹介しようと思います。</p>\n\n<p>まず今回の手法を紹介するにあたり、例題がある方が話が進めやすいので以下の広告運用者さんを例に考えます。</p>\n\n<pre class=\"code\" data-lang=\"\" data-unlink>広告運用者○○さん\n\n現在A,B,Cの３媒体で広告配信を行っています。\n\n全体のROASを高めるために予算配分を見直したいと考えています。\n\n３媒体での合計予算は3万円です。</pre>\n\n\n<p>では、実際にどのようにして最適な予算を求めるのか見ていきましょう。</p>\n\n<h1>過去の実績から各媒体の実績をシミュレーションします</h1>\n\n<p>最初に過去の実績から各媒体での予算とROASの傾向を、式で表現します。</p>\n\n<p>ここで<strong>ROASを最大化するということは、限られた予算の中で売り上げを最大化すると言い換えることができる</strong>ので、今回は <img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%28%E4%BA%88%E7%AE%97%2C%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%29\" alt=\" (&#x4E88;&#x7B97;,&#x58F2;&#x308A;&#x4E0A;&#x3052;)\"/> を2次回帰で近似します。</p>\n\n<p>今回の例だと媒体A,B,Cに対してそれぞれ近似式が用意できるので以下のように表現できます。(各媒体の予算を<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/>とします)</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%5Cdisplaystyle%0AA%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_1%29%20%3D%20a_A%20x_1%5E2%20%2B%20b_A%20x_1%0A\" alt=\" \\displaystyle\nA&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_1) = a_A x_1^2 + b_A x_1\n\"/>\n</div>\n\n\n\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%5Cdisplaystyle%0AB%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_2%29%20%3D%20a_B%20x_2%5E2%20%2B%20b_B%20x_2%0A\" alt=\" \\displaystyle\nB&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_2) = a_B x_2^2 + b_B x_2\n\"/>\n</div>\n\n\n\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%5Cdisplaystyle%0AC%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_3%29%20%3D%20a_C%20x_3%5E2%20%2B%20b_C%20x_3%0A\" alt=\" \\displaystyle\nC&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_3) = a_C x_3^2 + b_C x_3\n\"/>\n</div>\n\n\n<p><br>\n予算が0円の時は、原点を通る(売り上げが0円)になるように切片は使用していないです。</p>\n\n<h1>ラグランジュの未定乗数法を用いて最適な予算配分を見つける</h1>\n\n<h2>ラグランジュの未定乗数法とは？</h2>\n\n<p>ラグランジュの未定乗数法とは、</p>\n\n<blockquote><p>束縛条件のもとで最適化を行うための数学的な方法である。いくつかの変数に対して、いくつかの関数の値を固定するという束縛条件のもとで、別のある1つの関数の極値を求める</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fja.wikipedia.org%2Fwiki%2F%25E3%2583%25A9%25E3%2582%25B0%25E3%2583%25A9%25E3%2583%25B3%25E3%2582%25B8%25E3%2583%25A5%25E3%2581%25AE%25E6%259C%25AA%25E5%25AE%259A%25E4%25B9%2597%25E6%2595%25B0%25E6%25B3%2595\" title=\"ラグランジュの未定乗数法 - Wikipedia\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://ja.wikipedia.org/wiki/%E3%83%A9%E3%82%B0%E3%83%A9%E3%83%B3%E3%82%B8%E3%83%A5%E3%81%AE%E6%9C%AA%E5%AE%9A%E4%B9%97%E6%95%B0%E6%B3%95\">ja.wikipedia.org</a></cite></p></blockquote>\n\n<p>少し小難しく聞こえますが、今回の例題に当てはめて考えてみます。</p>\n\n<pre class=\"code\" data-lang=\"\" data-unlink>いくつかの変数に対して(各媒体の予算)\n\nいくつかの関数の値を固定する束縛条件(3媒体の総予算は3万円)\n\n別のある関数の極値を求める(3媒体の売り上げが最大となるポイントを求める)</pre>\n\n\n<p>ラグランジュの未定乗数法とは、上のような条件を満たす予算<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/>を見つけてくれる手法です。</p>\n\n<p>ラグランジュの未定乗数法では、媒体A,B,Cのそれぞれの<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%28%E4%BA%88%E7%AE%97%2C%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%29\" alt=\" (&#x4E88;&#x7B97;,&#x58F2;&#x308A;&#x4E0A;&#x3052;)\"/> に対して<strong>近似式が二階微分可能である必要がある</strong>ため、今回の例では2次回帰で近似を行いました。</p>\n\n<p>また今回は、<strong>3媒体の総予算(束縛条件)が広告によって全て使用される</strong>という仮説のもとで計算しています。予算を全て使わない場合は、計算が複雑になってしまうので今回は紹介しません。</p>\n\n<h2>実際にどのように計算するのか検証します</h2>\n\n<p>まず初めに束縛条件<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/> を定義します。今回の束縛条件は、それぞれの予算<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/>を足し合わせたものが30000円になるということですので、以下のように書けます。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%0Ax_1%20%2B%20x_2%20%2B%20x_3%20%3D%2030000%0A%5Ctag%7B1%7D%0A\" alt=\" \nx_1 + x_2 + x_3 = 30000\n\\tag{1}\n\"/></div>\n\n\n<p><br>\nここで式(1)を変換し、<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20g%28x_1%2Cx_2%2Cx_3%29\" alt=\" g(x_1,x_2,x_3)\"/>とおきます。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%0Ag%28x_1%2Cx_2%2Cx_3%29%20%3D%20x_1%20%2B%20x_2%20%2B%20x_3%20-%2030000%20%3D%200%0A\" alt=\"\ng(x_1,x_2,x_3) = x_1 + x_2 + x_3 - 30000 = 0\n\"/></div>\n\n\n<p><br>\nまた、今回最大にしたい3媒体の総売り上げを<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20f%28x_1%2Cx_2%2Cx_3%29\" alt=\" f(x_1,x_2,x_3)\"/>と置きます。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%0A%5Cbegin%7Baligned%7D%0A%20f%28x_1%2Cx_2%2Cx_3%29%20%26%3D%20A%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_1%29%20%2B%20B%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_2%29%20%2B%20C%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_3%29%20%5C%5C%0A%26%3D%20a_A%20x_1%5E2%20%2B%20b_A%20x_1%20%2B%20a_B%20x_2%5E2%20%2B%20b_B%20x_2%20%2B%20a_C%20x_3%5E2%20%2B%20b_C%20x_3%0A%5Cend%7Baligned%7D%0A\" alt=\"\n\\begin{aligned}\n f(x_1,x_2,x_3) &amp;= A&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_1) + B&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_2) + C&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_3) \\\\\n&amp;= a_A x_1^2 + b_A x_1 + a_B x_2^2 + b_B x_2 + a_C x_3^2 + b_C x_3\n\\end{aligned}\n\"/></div>\n\n\n<p><br>\nここで未定乗数<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%5Clambda%20\" alt=\" \\lambda \"/>と<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20f%28x_1%2Cx_2%2Cx_3%29%2Cg%28x_1%2Cx_2%2Cx_3%29\" alt=\" f(x_1,x_2,x_3),g(x_1,x_2,x_3)\"/>を用いてラグランジュ関数<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20L\" alt=\" L\"/>を作ります。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%0A%5Cbegin%7Baligned%7D%0AL%28x_1%2Cx_2%2Cx_3%2C%5Clambda%29%20%26%3D%20a_A%20x_1%5E2%20%2B%20b_A%20x_1%20%2B%20a_B%20x_2%5E2%20%2B%20b_B%20x_2%20%2B%20a_C%20x_3%5E2%20%2B%20b_C%20x_3%20-%20%5Clambda%20%28x_1%20%2B%20x_2%20%2B%20x_3%20-%2030000%29%0A%5Cend%7Baligned%7D%0A\" alt=\"\n\\begin{aligned}\nL(x_1,x_2,x_3,\\lambda) &amp;= a_A x_1^2 + b_A x_1 + a_B x_2^2 + b_B x_2 + a_C x_3^2 + b_C x_3 - \\lambda (x_1 + x_2 + x_3 - 30000)\n\\end{aligned}\n\"/></div>\n\n\n<p><br>\nそれぞれの変数で偏微分すると以下のようになります。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_1%7D%20%26%3D%202%20a_A%20x_1%20%2B%20b_A%20-%20%5Clambda%20%20%3D%200%5C%5C%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_2%7D%20%26%3D%202%20a_B%20x_2%20%2B%20b_B%20-%20%5Clambda%20%3D%200%5C%5C%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_3%7D%20%26%3D%202%20a_C%20x_3%20%2B%20b_C%20-%20%5Clambda%20%3D%200%5C%5C%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Clambda%7D%20%26%3D%20-%20x_1%20-%20x_2%20-%20x_3%20%2B%2030000%20%3D%200%5C%5C%0A%5Cend%7Baligned%7D%0A\" alt=\"\n\\begin{aligned}\n\\frac{\\partial L}{\\partial x_1} &amp;= 2 a_A x_1 + b_A - \\lambda  = 0\\\\\n\\frac{\\partial L}{\\partial x_2} &amp;= 2 a_B x_2 + b_B - \\lambda = 0\\\\\n\\frac{\\partial L}{\\partial x_3} &amp;= 2 a_C x_3 + b_C - \\lambda = 0\\\\\n\\frac{\\partial L}{\\partial \\lambda} &amp;= - x_1 - x_2 - x_3 + 30000 = 0\\\\\n\\end{aligned}\n\"/></div>\n\n\n<p><br>\nこれら4変数の4元連立方程式を説くと、予算30000円で総売り上げが最大になる予算配分<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/>が求まります。</p>\n\n<p>今回は、ROASを最大化するための方法を紹介しましたがCPAを最小化する場合は2次回帰式を求める際に<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%28cost%2Ccv%29\" alt=\" (cost,cv)\"/>としてcvを最大化するようにラグランジュを適応することで求めることができます。</p>\n\n<p>また今回の例では3媒体までの予算配分を計算しましたが、<strong>媒体数を増やしても計算速度が極端に遅くなることがない</strong>ところが今回の手法の良いところです。</p>\n\n<h1>この手法の課題点</h1>\n\n<p>ここまで「ネット広告の複数媒体に対するCPA最小化・ROAS最大化となる予算配分」を紹介しましたが、この手法には2つほど課題があります。</p>\n\n<p>まず一つ目が、最適予算<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%28x_1%2Cx_2%2Cx_3%29\" alt=\" (x_1,x_2,x_3)\"/>にマイナスの結果が得られる可能性があるということです。売り上げを最大化しようとするあまり、もともとROASが低い媒体に対しては予算を割り振らずにマイナスの予算を割り振り、そのほかのROASが高い媒体により多くの予算を割り振ろうとしてしまうことが確認できています。</p>\n\n<p>次に、媒体の周期性や外部要因を一切考慮していないということです。広告は少なからず外部要因によって成果が左右されますが、この手法では過去の実績のみを用いて最適予算を割り振るので外部要因は一切考慮されていないということに注意が必要です。</p>\n\n<h1>まとめ</h1>\n\n<p>いかがだったでしょうか。\n今回は、ラグランジュの未定乗数法を用いて複数媒体への予算配分方法を紹介しました。流石に手作業では計算できないので私はpythonで上記の流れを実装しています。</p>\n\n<p>ラグランジュの未定乗数法は、理系の方は大学の数学の講義で習っていたかもしれないです。私も大学の時に習いましたが、当時は何に使うのか一切わかりませんでした。社会人になって学生の時に学んだことが活用できると、学んだ甲斐があったと感じることができて良いです。</p>\n\n<p>最後まで読んでいただきありがとうございます。</p>\n","contentSnippet":"こんにちは　機械学習エンジニアの八百俊哉です。今回は複数媒体へ広告を出稿する際に、多くの方が悩まれるであろう「各媒体への予算配分」に関して有効な配分手法を紹介します。今回の記事で登場する広告用語媒体・・・広告の配信先や配信手法ROAS・・・広告経由で発生した売り上げを広告費用で割った値(広告の費用対効果)CPA・・・1件のコンバージョン(目標)を獲得するのにかかった広告コスト広告運用者が抱える課題とは？1つの媒体のみで運用している場合は別ですが、複数の媒体で広告配信を行っている場合は、どの媒体に対していくら予算を割り振れば良いのかわからない場合があると思います。過去の実績を元に成果が良い媒体に対して、多く予算を割り振れば良いことは理解しているものの、「どれくらい」「どの媒体から」予算を割り振れば良いのかは経験則や簡単な分析で決めている方も多いのではないでしょうか？今回はこれらの課題を解決するために、数学的に根拠のある予算配分方法について紹介しようと思います。まず今回の手法を紹介するにあたり、例題がある方が話が進めやすいので以下の広告運用者さんを例に考えます。広告運用者○○さん現在A,B,Cの３媒体で広告配信を行っています。全体のROASを高めるために予算配分を見直したいと考えています。３媒体での合計予算は3万円です。では、実際にどのようにして最適な予算を求めるのか見ていきましょう。過去の実績から各媒体の実績をシミュレーションします最初に過去の実績から各媒体での予算とROASの傾向を、式で表現します。ここでROASを最大化するということは、限られた予算の中で売り上げを最大化すると言い換えることができるので、今回は  を2次回帰で近似します。今回の例だと媒体A,B,Cに対してそれぞれ近似式が用意できるので以下のように表現できます。(各媒体の予算をとします)ラグランジュの未定乗数法を用いて最適な予算配分を見つけるラグランジュの未定乗数法とは？ラグランジュの未定乗数法とは、束縛条件のもとで最適化を行うための数学的な方法である。いくつかの変数に対して、いくつかの関数の値を固定するという束縛条件のもとで、別のある1つの関数の極値を求めるja.wikipedia.org少し小難しく聞こえますが、今回の例題に当てはめて考えてみます。いくつかの変数に対して(各媒体の予算)いくつかの関数の値を固定する束縛条件(3媒体の総予算は3万円)別のある関数の極値を求める(3媒体の売り上げが最大となるポイントを求める)ラグランジュの未定乗数法とは、上のような条件を満たす予算を見つけてくれる手法です。ラグランジュの未定乗数法では、媒体A,B,Cのそれぞれの に対して近似式が二階微分可能である必要があるため、今回の例では2次回帰で近似を行いました。また今回は、3媒体の総予算(束縛条件)が広告によって全て使用されるという仮説のもとで計算しています。予算を全て使わない場合は、計算が複雑になってしまうので今回は紹介しません。実際にどのように計算するのか検証しますまず初めに束縛条件 を定義します。今回の束縛条件は、それぞれの予算を足し合わせたものが30000円になるということですので、以下のように書けます。ここで式(1)を変換し、とおきます。また、今回最大にしたい3媒体の総売り上げをと置きます。ここで未定乗数とを用いてラグランジュ関数を作ります。それぞれの変数で偏微分すると以下のようになります。これら4変数の4元連立方程式を説くと、予算30000円で総売り上げが最大になる予算配分が求まります。今回は、ROASを最大化するための方法を紹介しましたがCPAを最小化する場合は2次回帰式を求める際にとしてcvを最大化するようにラグランジュを適応することで求めることができます。また今回の例では3媒体までの予算配分を計算しましたが、媒体数を増やしても計算速度が極端に遅くなることがないところが今回の手法の良いところです。この手法の課題点ここまで「ネット広告の複数媒体に対するCPA最小化・ROAS最大化となる予算配分」を紹介しましたが、この手法には2つほど課題があります。まず一つ目が、最適予算にマイナスの結果が得られる可能性があるということです。売り上げを最大化しようとするあまり、もともとROASが低い媒体に対しては予算を割り振らずにマイナスの予算を割り振り、そのほかのROASが高い媒体により多くの予算を割り振ろうとしてしまうことが確認できています。次に、媒体の周期性や外部要因を一切考慮していないということです。広告は少なからず外部要因によって成果が左右されますが、この手法では過去の実績のみを用いて最適予算を割り振るので外部要因は一切考慮されていないということに注意が必要です。まとめいかがだったでしょうか。今回は、ラグランジュの未定乗数法を用いて複数媒体への予算配分方法を紹介しました。流石に手作業では計算できないので私はpythonで上記の流れを実装しています。ラグランジュの未定乗数法は、理系の方は大学の数学の講義で習っていたかもしれないです。私も大学の時に習いましたが、当時は何に使うのか一切わかりませんでした。社会人になって学生の時に学んだことが活用できると、学んだ甲斐があったと感じることができて良いです。最後まで読んでいただきありがとうございます。","link":"https://developer.feedforce.jp/entry/2021/05/13/093842","isoDate":"2021-05-17T00:38:42.000Z","dateMiliSeconds":1621211922000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"yaonyan"},{"title":" 夜間光データから土地価格を予測 コンペの参加記録","content":"<p>こんにちは\n株式会社フィードフォース2020年入社の機械学習エンジニア\n<a href=\"https://twitter.com/feed_yao\">八百　俊哉</a>と申します。</p>\n\n<p>今回は、solafuneで開催された「<a href=\"https://solafune.com/#/competitions/f03f39cc-597b-4819-b1a5-41479d4b73d6\">夜間光データから土地価格を予測</a>」という機械学習コンペに参加したので工夫した点や反省点などを紹介します。</p>\n\n<p>コンペ参加の目標設定としては、「賞金獲得！！（4位以内）」を設定していましたが、36位/201人中と目標達成できませんでした。残念な結果に終わってしまいましたが、多くのことを学ぶことができました。</p>\n\n<h1>参加経緯</h1>\n\n<p>私は、2020年10月から2021年2月ごろまで顧客の課題解決のために機械学習を応用する方法を学ぶためにAI Questというイベントに参加していました。そのイベントをきっかけに私は精度の高いモデルや良い特徴量を作成することに興味を持ちました。</p>\n\n<p>そこでより多くのコンペに参加することで精度を上げるためのノウハウを身に付けたいと思ったことが今回のコンペに参加したきっかけです。</p>\n\n<p>また、今回参加したコンペは与えられている特徴量が4つしかないので、初心者が参加しやすいコンペだったということも魅力的なポイントでした。</p>\n\n<h1>課題と与えられているデータ</h1>\n\n<p>課題としては、「夜間光データを元に土地価格を予測するアルゴリズムを開発する」というものです。\n使用可能なデータとしては、以下のものが与えられました。</p>\n\n<ul>\n<li>地域ごとのデータ・・・地域固有のID</li>\n<li>年代・・・1992~2013年まで</li>\n<li>土地の平均価格（目的変数）・・・1992~2013年まで</li>\n<li>夜間光量の平均値・・・0~63までのレンジでその地域の平均光量</li>\n<li>夜間光量の合計値・・・その地域の合計光量</li>\n</ul>\n\n\n<h1>全体構成</h1>\n\n<p>今回最終submitとして選択したモデルの全体構成は以下です。\n<span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210409/20210409155716.png\" alt=\"f:id:newton800:20210409155716p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h1>前処理に関して</h1>\n\n<h2>集約的特徴量について</h2>\n\n<p>集約的特徴量の作成にあたっては<a href=\"https://twitter.com/mst_8823\">masato8823 (@mst_8823) | Twitter</a>さんがBaseLineとして公開されていた以下のものを使用しました。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fzenn.dev%2Fmst8823%2Farticles%2Fcd40cb971f702e\" title=\"[solafune] 夜間光データから土地価格を予測 BaseLine\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://zenn.dev/mst8823/articles/cd40cb971f702e\">zenn.dev</a></cite></p>\n\n<p>作成した特徴量としては以下です。</p>\n\n<table>\n<thead>\n<tr>\n<th>    </th>\n<th>    </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>  面積  </td>\n<td>  夜間光量の合計値/夜間光量の平均値を行い面積を算出した </td>\n</tr>\n<tr>\n<td>   PlaceID,Yearごとの統計情報 </td>\n<td>  PlaceID,Yearをキーとして平均光量、合計光量、面積のmin,max,median,mean,std,max-min,q75-q25を算出した  </td>\n</tr>\n<tr>\n<td> PlaceID をキーにしたグループ内差分</td>\n<td>  平均光量、合計光量の年ごとの差分を算出した</td>\n</tr>\n<tr>\n<td>PlaceID をキーにしたグループ内シフト </td>\n<td> 平均光量、合計光量の年ごとの値をシフトした</td>\n</tr>\n<tr>\n<td>ピボットテーブルを用いた特徴量</td>\n<td>index=PlaceID,columns=Yearとして平均光量、合計光量、面積のピボットテーブルを作成し、PCAで次元削減したものを算出した</td>\n</tr>\n<tr>\n<td>PlaceIDをキーにしたグループ内相関係数</td>\n<td>PlaceIDごとにデータを集約しYearと平均光量、合計光量、面積との相関係数を算出した</td>\n</tr>\n<tr>\n<td>平均光量が63であった回数</td>\n<td>平均光量の最大値が63であることから平均光量が63である数を追加した</td>\n</tr>\n</tbody>\n</table>\n\n\n<h2>Area特徴量について</h2>\n\n<p>先ほど<i>集約的特徴量について</i>で面積の求め方について書きました。面積=合計光量/平均光量で算出しています。ここで求められる<b>土地の面積は、年が変化しようと変化しないと思われますが、実際のデータを確認すると年が変化すると面積も変化していました。</b></p>\n\n<p>そこで<b>合計光量/平均光量より算出された面積をPlaceIDをキーとして平均を取ったものを新たな面積としました。</b>\n新たな面積が求まると <b>新たな合計光量 =  平均光量×新たな面積,新たな平均面積 = 合計光量/新たな面積</b> が求まります。</p>\n\n<p>これらより求まる新たな合計光量、新たな平均光量、新たな面積を元々の合計光量、平均光量、面積と置き換えて集約的特徴量の作成を行いました。</p>\n\n<h2>gplearnについて</h2>\n\n<p>上で紹介した集約的特徴量とArea特徴量のそれぞれに対して<a href=\"https://gplearn.readthedocs.io/en/stable/\">gplearn</a>というライブラリを用いて新たな特徴量を作成しました。このライブラリは遺伝的アルゴリズムにより目的変数をよく表している変数を作成してくれるものです。</p>\n\n<p>このライブラリを用いて新しい特徴量を10個,25個,50個作成し、元々の集約的特徴量、Area特徴量と組み合わせてそれぞれに対して予測を行いました。</p>\n\n<p>gplearnでの特徴量作成については以下のサイトが参考になります。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fqiita.com%2FHatomugi%2Fitems%2F3bb16ed9c6bdc15f1e00\" title=\"遺伝的アルゴリズムを使って特徴量エンジニアリングしてみた - Qiita\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://qiita.com/Hatomugi/items/3bb16ed9c6bdc15f1e00\">qiita.com</a></cite></p>\n\n<h1>モデル構築に関して</h1>\n\n<p>モデルの構築としてはgroup k fold(fold=5)でStackingのモデルを採用しました。</p>\n\n<p>1層目はrandom forest,lgb,multi regression,catboost,xgboostに加えてAutoMLの<a href=\"https://auto.gluon.ai/stable/index.html\">Autogluon</a>を採用しました。</p>\n\n<p>Autogluonは以下のようにデータを渡すだけで、11個のモデルを検証し最後に出力結果を重量平均で作成してくれます。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>\npredictor = TabularPredictor(\n                                label=<span class=\"synConstant\">'label'</span>,\n                                problem_type=<span class=\"synConstant\">'regression'</span>, \n                                eval_metric=<span class=\"synConstant\">'root_mean_squared_error'</span>, <span class=\"synComment\"># 評価指標</span>\n)\n\nX_train[<span class=\"synConstant\">'label'</span>] = y_train\nX_test[<span class=\"synConstant\">'label'</span>] = y_test\n\npredictor.fit(\n            train_data=X_train,\n            tuning_data=X_test, <span class=\"synComment\"># これを渡さない場合はランダムスプリット</span>\n            time_limit=<span class=\"synIdentifier\">None</span>, <span class=\"synComment\"># おおよその時間制限を設けられる</span>\n)\n</pre>\n\n\n<p>そして2層目は1層目でも採用しているAutogluonで出力を作成しました。</p>\n\n<h1>感想・反省点</h1>\n\n<p>Public Scoreの時点では6位と賞金獲得の可能性が十分にありましたが、Private Scoreでは36位と大幅にshake downしてしまいました。今回目標達成できなかった理由としては以下の2つが考えられます。</p>\n\n<p><b>1 CVの値とPublic ScoreからPrivate Scoreについて考えられなかった\n</b></p>\n\n<p>1つ目の要因としては、Public Scoreが下がることのみを考えてモデルの改善・特徴量の作成を行っていたということです。その時CVの値とPublic Scoreをどこかに記録しておけばよかったのですが、どこにも保存せずPublic Scoreが下がることが最も良いことであると捉えていました。実際は、CVが下がったモデル・特徴量においてPublic Scoreも同じように下がることが望ましく、その記録を取っておくべきでした。</p>\n\n<p>実際これまで提出していたファイルの中にPrivate Scoreが0.48774というものがあり、このファイルを最終提出としておけば3位に入ることができていました。しっかりとPrivate Scoreに効いているであろう提出ファイルが選べるようにCVとPublic Scoreに着目できるようにならないといけないと感じました。</p>\n\n<p><b>2 gplearnを行う位置が悪かった\n</b></p>\n\n<p>2つめは、group k foldを行う前にgplearnを行ったことによって、validationの目的変数が確認できる状態でgplearnが特徴量作成を行ってたことです。これは本来見ることができないデータを確認しながらデータ生成を行っていることになるので過学習を引き起こす可能性がありました。\n　\n　<span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210413/20210413095008.png\" alt=\"f:id:newton800:20210413095008p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>あるべき姿としては、group k foldでtrainをtrain,validationに分割した後にtrainのみのデータを用いてgplearnをfitさせるべきだったと思います。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210413/20210413112402.png\" alt=\"f:id:newton800:20210413112402p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h1>次回コンペでは</h1>\n\n<p>今回のコンペを通じて集約的特徴量の作成方法、Stackingの実装方法、gplearnの実行位置、CVとPublic Scoreの関係性の重要度について学ぶことができました。\nテーブルコンペ において有効な手法を多く学ぶことができたので、次回参加するコンペでは賞金獲得を目標に頑張ります！！</p>\n","contentSnippet":"こんにちは株式会社フィードフォース2020年入社の機械学習エンジニア八百　俊哉と申します。今回は、solafuneで開催された「夜間光データから土地価格を予測」という機械学習コンペに参加したので工夫した点や反省点などを紹介します。コンペ参加の目標設定としては、「賞金獲得！！（4位以内）」を設定していましたが、36位/201人中と目標達成できませんでした。残念な結果に終わってしまいましたが、多くのことを学ぶことができました。参加経緯私は、2020年10月から2021年2月ごろまで顧客の課題解決のために機械学習を応用する方法を学ぶためにAI Questというイベントに参加していました。そのイベントをきっかけに私は精度の高いモデルや良い特徴量を作成することに興味を持ちました。そこでより多くのコンペに参加することで精度を上げるためのノウハウを身に付けたいと思ったことが今回のコンペに参加したきっかけです。また、今回参加したコンペは与えられている特徴量が4つしかないので、初心者が参加しやすいコンペだったということも魅力的なポイントでした。課題と与えられているデータ課題としては、「夜間光データを元に土地価格を予測するアルゴリズムを開発する」というものです。使用可能なデータとしては、以下のものが与えられました。地域ごとのデータ・・・地域固有のID年代・・・1992~2013年まで土地の平均価格（目的変数）・・・1992~2013年まで夜間光量の平均値・・・0~63までのレンジでその地域の平均光量夜間光量の合計値・・・その地域の合計光量全体構成今回最終submitとして選択したモデルの全体構成は以下です。前処理に関して集約的特徴量について集約的特徴量の作成にあたってはmasato8823 (@mst_8823) | TwitterさんがBaseLineとして公開されていた以下のものを使用しました。zenn.dev作成した特徴量としては以下です。          面積    夜間光量の合計値/夜間光量の平均値を行い面積を算出した    PlaceID,Yearごとの統計情報   PlaceID,Yearをキーとして平均光量、合計光量、面積のmin,max,median,mean,std,max-min,q75-q25を算出した   PlaceID をキーにしたグループ内差分  平均光量、合計光量の年ごとの差分を算出したPlaceID をキーにしたグループ内シフト  平均光量、合計光量の年ごとの値をシフトしたピボットテーブルを用いた特徴量index=PlaceID,columns=Yearとして平均光量、合計光量、面積のピボットテーブルを作成し、PCAで次元削減したものを算出したPlaceIDをキーにしたグループ内相関係数PlaceIDごとにデータを集約しYearと平均光量、合計光量、面積との相関係数を算出した平均光量が63であった回数平均光量の最大値が63であることから平均光量が63である数を追加したArea特徴量について先ほど集約的特徴量についてで面積の求め方について書きました。面積=合計光量/平均光量で算出しています。ここで求められる土地の面積は、年が変化しようと変化しないと思われますが、実際のデータを確認すると年が変化すると面積も変化していました。そこで合計光量/平均光量より算出された面積をPlaceIDをキーとして平均を取ったものを新たな面積としました。新たな面積が求まると 新たな合計光量 =  平均光量×新たな面積,新たな平均面積 = 合計光量/新たな面積 が求まります。これらより求まる新たな合計光量、新たな平均光量、新たな面積を元々の合計光量、平均光量、面積と置き換えて集約的特徴量の作成を行いました。gplearnについて上で紹介した集約的特徴量とArea特徴量のそれぞれに対してgplearnというライブラリを用いて新たな特徴量を作成しました。このライブラリは遺伝的アルゴリズムにより目的変数をよく表している変数を作成してくれるものです。このライブラリを用いて新しい特徴量を10個,25個,50個作成し、元々の集約的特徴量、Area特徴量と組み合わせてそれぞれに対して予測を行いました。gplearnでの特徴量作成については以下のサイトが参考になります。qiita.comモデル構築に関してモデルの構築としてはgroup k fold(fold=5)でStackingのモデルを採用しました。1層目はrandom forest,lgb,multi regression,catboost,xgboostに加えてAutoMLのAutogluonを採用しました。Autogluonは以下のようにデータを渡すだけで、11個のモデルを検証し最後に出力結果を重量平均で作成してくれます。'label',                                problem_type='regression',                                 eval_metric='root_mean_squared_error', # 評価指標)X_train['label'] = y_trainX_test['label'] = y_testpredictor.fit(            train_data=X_train,            tuning_data=X_test, # これを渡さない場合はランダムスプリット            time_limit=None, # おおよその時間制限を設けられる)そして2層目は1層目でも採用しているAutogluonで出力を作成しました。感想・反省点Public Scoreの時点では6位と賞金獲得の可能性が十分にありましたが、Private Scoreでは36位と大幅にshake downしてしまいました。今回目標達成できなかった理由としては以下の2つが考えられます。1 CVの値とPublic ScoreからPrivate Scoreについて考えられなかった1つ目の要因としては、Public Scoreが下がることのみを考えてモデルの改善・特徴量の作成を行っていたということです。その時CVの値とPublic Scoreをどこかに記録しておけばよかったのですが、どこにも保存せずPublic Scoreが下がることが最も良いことであると捉えていました。実際は、CVが下がったモデル・特徴量においてPublic Scoreも同じように下がることが望ましく、その記録を取っておくべきでした。実際これまで提出していたファイルの中にPrivate Scoreが0.48774というものがあり、このファイルを最終提出としておけば3位に入ることができていました。しっかりとPrivate Scoreに効いているであろう提出ファイルが選べるようにCVとPublic Scoreに着目できるようにならないといけないと感じました。2 gplearnを行う位置が悪かった2つめは、group k foldを行う前にgplearnを行ったことによって、validationの目的変数が確認できる状態でgplearnが特徴量作成を行ってたことです。これは本来見ることができないデータを確認しながらデータ生成を行っていることになるので過学習を引き起こす可能性がありました。　　あるべき姿としては、group k foldでtrainをtrain,validationに分割した後にtrainのみのデータを用いてgplearnをfitさせるべきだったと思います。次回コンペでは今回のコンペを通じて集約的特徴量の作成方法、Stackingの実装方法、gplearnの実行位置、CVとPublic Scoreの関係性の重要度について学ぶことができました。テーブルコンペ において有効な手法を多く学ぶことができたので、次回参加するコンペでは賞金獲得を目標に頑張ります！！","link":"https://developer.feedforce.jp/entry/2021/04/13/174808","isoDate":"2021-04-13T08:48:08.000Z","dateMiliSeconds":1618303688000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210409/20210409155716.png","authorName":"yaonyan"},{"title":"Self-Attentionを用いてGoogle 無料リスティングの「拡張リスティングの不承認」に挑んだ話","content":"<p>こんにちは\n株式会社フィードフォース2020年入社の機械学習エンジニア\n<a href=\"https://twitter.com/feed_yao\">&#x516B;&#x767E;&#x4FCA;&#x54C9;@Feedforce (@feed_yao) | Twitter</a>と申します。</p>\n\n<p>最近はロードバイク にはまっており、ロードバイク購入後一ヶ月で一日100km走行に成功しました。</p>\n\n<p>今回、<b>Google無料リスティングで不承認アカウントが発生する要因を調査する分析</b>を行いました。</p>\n\n<ul class=\"table-of-contents\">\n    <li><a href=\"#Google-無料リスティングとは\">Google 無料リスティングとは？</a></li>\n    <li><a href=\"#なぜ今回分析が必要とされたのか\">なぜ今回分析が必要とされたのか？</a></li>\n    <li><a href=\"#結果と考察\">結果と考察</a></li>\n    <li><a href=\"#Self-Attentionを採用した理由\">Self-Attentionを採用した理由</a></li>\n    <li><a href=\"#実装手順\">実装手順</a><ul>\n            <li><a href=\"#使用データのフォーマット\">使用データのフォーマット</a></li>\n            <li><a href=\"#必要ライブラリのインストールインポート\">必要ライブラリのインストール・インポート</a></li>\n            <li><a href=\"#データの前処理\">データの前処理</a></li>\n            <li><a href=\"#学習\">学習</a></li>\n            <li><a href=\"#評価出力\">評価・出力</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#まとめ\">まとめ</a></li>\n</ul>\n\n<h1 id=\"Google-無料リスティングとは\">Google 無料リスティングとは？</h1>\n\n<p>2020年10月にGoogleから公開された<b>Googleショッピングタブに無料で商品掲載ができる「無料リスティング」のことです。</b></p>\n\n<p>Google 検索にサイトがインデックス登録されても料金が発生しないのと同様に、EC事業者は無料で利用可能になりました。\nGoogle 無料リスティングについての詳細は以下のサイトが参考になります。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Flab.ecbooster.jp%2Fabout-google-free-listings%2F\" title=\"Googleに無料で自社商品が掲載できる「無料リスティング」とは？\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://lab.ecbooster.jp/about-google-free-listings/\">lab.ecbooster.jp</a></cite></p>\n\n<h1 id=\"なぜ今回分析が必要とされたのか\">なぜ今回分析が必要とされたのか？</h1>\n\n<p>無料リスティングでは自社製品を無料でGoogleに掲載できます。\nしかしながら、課題として<b>一部商品掲載が不承認となるケースが見受けられました。</b></p>\n\n<p>不承認となってしまうと自社商品の掲載ができていない状況が発生しています。不承認となる理由としては、「Googleが定める基準に対して、登録している商品データの属性数が足りない、内容が仕様に沿っていない場合、商品データの品質が低いため不承認になり、Googleの検索結果に表示させることができません。」とされています。</p>\n\n<p>これらを<b>定量的に分析することで不承認となる理由を見つけ出す試み</b>が始まりました。</p>\n\n<p>そのため今回の分析の目的は、<b>商品の属性情報（title,description）から承認・不承認の要因を見つけ出し、不承認の商品を承認へと改善するための施策を考案する</b>ことです。</p>\n\n<h1 id=\"結果と考察\">結果と考察</h1>\n\n<p>今回の目的である「商品の属性情報（title,description）から承認・不承認の要因を見つけ出し、不承認の商品を承認へと改善するための施策を考案する」は、<b>達成できませんでした。</b></p>\n\n<p>目的が達成できなかった理由として考えられる要因は、<b>承認・不承認は商品のtitle,descriptionだけでは判断されていない</b>ということです。商品ごとのtitle,descriptionのみで承認・不承認が判断されているのではなく、商品データ全体またはアカウント全体のデータを総合的に見て、判断されている可能性が高いということがわかりました。</p>\n\n<p>承認・不承認予測のAccuracyとしては5割〜６割ほどで、承認・不承認を予測するという点でも低い精度となってしまいました。</p>\n\n<h1 id=\"Self-Attentionを採用した理由\">Self-Attentionを採用した理由</h1>\n\n<p>今回はSelf-Attentionという手法を用いてこの課題解決を試みました。</p>\n\n<p>Self-Attentionとは、<b>文章全体で重要とされるキーワードが予測結果と一緒に確認できるようになる</b>手法です。</p>\n\n<p>Self-Attentionの仕組みについては詳しく書かれている方が多くいますので、ここでは割愛します。</p>\n\n<p>最初は、word2vecを用いて文章特徴量を作成し、承認・不承認を予測して終了という一連の流れを想定していました。</p>\n\n<p>しかし、<b>今回の目的は</b>承認・不承認を予測したいわけではなく、<b>どの単語が承認・不承認と関わっているのかを確認し、不承認となっているアカウントを承認にすること</b>です。\nもし仮にword2vecを用いた手法を採用すると予測結果の要因や理由が明確にならないので、不承認のアカウントを承認に改善する施策を考えることはできません。</p>\n\n<p>そのため今回は、<b>Self-Attentionを用いて分類モデルを構築することで、承認・不承認の要因が文章内のどこにあるのかを分析する</b>ために、この手法を選択しました。</p>\n\n<h1 id=\"実装手順\">実装手順</h1>\n\n<p>本来の目的は達成できませんでしたが、Self-Attentionでの分類モデルの実装はできましたので、実装方法を記載します。\n今回はkerasを用いてSelf-Attention + LSTMで予測を行いました。\n検証環境はGoogle Colaboratoryを想定しています。</p>\n\n<h2 id=\"使用データのフォーマット\">使用データのフォーマット</h2>\n\n<p>今回使用できるデータとしては以下のようなデータになっています。</p>\n\n<p>各アカウント・各商品ごとに商品IDが割り振られており、それぞれの商品にtitle,descriptionが割り振られています。</p>\n\n<p><b>承認・不承認のラベルは、アカウントごとに付加されています。\n</b></p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210224/20210224172435.png\" alt=\"f:id:newton800:20210224172435p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h2 id=\"必要ライブラリのインストールインポート\">必要ライブラリのインストール・インポート</h2>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>!pip install text_vectorian\n!pip install mojimoji\n!apt install aptitude\n!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils <span class=\"synIdentifier\">file</span> -y\n!pip install mecab-python3==<span class=\"synConstant\">0.7</span>\n</pre>\n\n\n\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synPreProc\">import</span> pandas <span class=\"synStatement\">as</span> pd\n<span class=\"synPreProc\">import</span> numpy <span class=\"synStatement\">as</span> np\n<span class=\"synPreProc\">import</span> keras\n<span class=\"synPreProc\">import</span> os\n<span class=\"synPreProc\">import</span> warnings\nwarnings.simplefilter(<span class=\"synConstant\">'ignore'</span>)\n<span class=\"synPreProc\">import</span> subprocess\n<span class=\"synPreProc\">import</span> mojimoji\n<span class=\"synPreProc\">import</span> re\n<span class=\"synPreProc\">import</span> MeCab\n<span class=\"synPreProc\">import</span> matplotlib.pyplot <span class=\"synStatement\">as</span> plt\n\n<span class=\"synPreProc\">from</span> keras.layers <span class=\"synPreProc\">import</span> Dense, Dropout, LSTM, Embedding, BatchNormalization\n<span class=\"synPreProc\">from</span> keras.layers.wrappers <span class=\"synPreProc\">import</span> Bidirectional\n<span class=\"synPreProc\">from</span> keras.callbacks <span class=\"synPreProc\">import</span> EarlyStopping, ModelCheckpoint\n<span class=\"synPreProc\">from</span> keras <span class=\"synPreProc\">import</span> Input, Model, utils\n<span class=\"synPreProc\">from</span> keras.preprocessing.sequence <span class=\"synPreProc\">import</span> pad_sequences\n<span class=\"synPreProc\">from</span> keras.callbacks <span class=\"synPreProc\">import</span> EarlyStopping\n\n<span class=\"synPreProc\">from</span> text_vectorian <span class=\"synPreProc\">import</span> SentencePieceVectorian\n<span class=\"synPreProc\">from</span> keras_self_attention <span class=\"synPreProc\">import</span> SeqSelfAttention\n<span class=\"synPreProc\">from</span> sklearn.model_selection <span class=\"synPreProc\">import</span> train_test_split\n<span class=\"synPreProc\">from</span> sklearn.metrics <span class=\"synPreProc\">import</span> classification_report\n</pre>\n\n\n<h2 id=\"データの前処理\">データの前処理</h2>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synComment\"># データの読み込み</span>\napp = pd.read_csv(<span class=\"synConstant\">'data/app.csv'</span>) <span class=\"synComment\"># 承認データ</span>\ndisapp = pd.read_csv(<span class=\"synConstant\">'data/disapp.csv'</span>) <span class=\"synComment\"># 不承認データ</span>\n\napp[<span class=\"synConstant\">'target'</span>] = <span class=\"synConstant\">'app'</span> <span class=\"synComment\"># targetにlabelを代入する</span>\ndisapp[<span class=\"synConstant\">'target'</span>] = <span class=\"synConstant\">'disapp'</span>\n\n<span class=\"synComment\"># 今回は、titleとdescriptionを用いて予測するので、それら二つの変数を一つにまとめる</span>\napp[<span class=\"synConstant\">'sentence'</span>] = app[<span class=\"synConstant\">'title'</span>] + app[<span class=\"synConstant\">'description'</span>] \ndisapp[<span class=\"synConstant\">'sentence'</span>] = disapp[<span class=\"synConstant\">'title'</span>] + disapp[<span class=\"synConstant\">'description'</span>]\n\n<span class=\"synComment\"># これまで別々に処理していたappとdisappをまとめてdfとする</span>\ndf = app.append(disapp)\n</pre>\n\n\n<p>今回のデータは特殊で、承認・不承認は商品ごとについているラベルではなくアカウントと紐づいたラベルとなっています。それらを各商品と承認・不承認が紐づいているとして各商品ごとに予測することを行ってます。</p>\n\n<p>ここで注意が必要なのは、データの分割方法です。<b>アカウントを無視してデータを分割してしまうとリークを起こす可能性があります</b>（リークとは、本来予測では使用できないデータが学習時に入ってしまっていることです）。</p>\n\n<p><b>そのため同じアカウントのデータが訓練データ、検証データ、テストデータに渡って存在しないようにしなければなりません</b>。</p>\n\n<p>例えば、アカウントAの商品データは全て訓練データとする,アカウントBの商品データは全てテストデータにするといったようなことを意味しています。</p>\n\n<p>アカウントごとにデータを分割するには、各アカウントごとの商品数がある程度同じである方がlabelが不均衡にならないと考え、データ数を揃える処理を施しました。\n（これらはGroupKFoldを使用すれば解決できると考えられますが、分析実施時はGroupKFoldを知らなかった）</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synComment\"># アカウントごとに商品数が異なるので50以上商品数がある場合は50までの商品を使用する</span>\n<span class=\"synComment\"># アカウントごとに商品数を揃えることで、labelが不均衡になることを緩和している</span>\n<span class=\"synComment\"># アカウントごとにlabelがふられるが、商品ごとに予測結果を出す時のみ実施</span>\ncutted_df = pd.DataFrame([])\n<span class=\"synStatement\">for</span> acc <span class=\"synStatement\">in</span> df.account_name.unique():\n  data = df[df.account_name == acc]\n  <span class=\"synStatement\">if</span> data.shape[<span class=\"synConstant\">0</span>] &gt; <span class=\"synConstant\">50</span>: \n    data = data[:<span class=\"synConstant\">50</span>]\n  cutted_df = pd.concat([cutted_df,data],<span class=\"synConstant\">0</span>)  \n\ndf = cutted_df.sample(frac=<span class=\"synConstant\">1</span>,random_state=<span class=\"synConstant\">1</span>).reset_index(drop=<span class=\"synIdentifier\">True</span>)\n</pre>\n\n\n<p>次は、データの前処理についてです。</p>\n\n<p>自然言語処理の前処理で有効と言われている半角->全角、数字は全て0にする、スペース文字の消去を行いました。\nまた、これまでlabelが'app'または'disapp'だったのでそれらを入力できる形式に変換しています。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">PreprocessData</span>(df,dirname):\n  <span class=\"synComment\"># データの前処理関数</span>\n  <span class=\"synComment\"># 辞書型を返す</span>\n\n  mecab = MeCab.Tagger(<span class=\"synConstant\">'-Ochasen'</span>)\n\n  <span class=\"synComment\"># textデータの前処理</span>\n  df = TextPreprocess(df)\n\n  label2index = {k: i <span class=\"synStatement\">for</span> i, k <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">enumerate</span>(df.target.unique())}\n  index2label = {i: k <span class=\"synStatement\">for</span> i, k <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">enumerate</span>(df.target.unique())}\n\n  class_count = <span class=\"synIdentifier\">len</span>(label2index)\n  labels = utils.to_categorical([label2index[label] <span class=\"synStatement\">for</span> label <span class=\"synStatement\">in</span> df.target], num_classes=class_count)\n\n  features,sentences,vectorian,account = MakeFeatures(df)\n\n  <span class=\"synStatement\">return</span> {\n      <span class=\"synConstant\">'class_count'</span>: class_count,\n      <span class=\"synConstant\">'label2index'</span>: label2index,\n      <span class=\"synConstant\">'index2label'</span>: index2label,\n      <span class=\"synConstant\">'labels'</span>: labels,\n      <span class=\"synConstant\">'features'</span>: features,\n      <span class=\"synConstant\">'sentences'</span>:sentences,\n      <span class=\"synConstant\">'input_len'</span>: vectorian.max_tokens_len,\n      <span class=\"synConstant\">'vectorian'</span>:vectorian,\n      <span class=\"synConstant\">'account'</span>:account\n  }\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">TextPreprocess</span>(df):\n  <span class=\"synStatement\">for</span> i <span class=\"synStatement\">in</span> df.index:\n    sen = df.loc[i,<span class=\"synConstant\">'sentence'</span>]\n    sen = mojimoji.han_to_zen(sen)\n    sen = re.sub(<span class=\"synConstant\">r'\\d+'</span>,<span class=\"synConstant\">'0'</span>,sen)\n    df.loc[i,<span class=\"synConstant\">'sentence'</span>] = sen.replace(<span class=\"synConstant\">'</span><span class=\"synSpecial\">\\u3000</span><span class=\"synConstant\">'</span>,<span class=\"synConstant\">''</span>)\n  <span class=\"synStatement\">return</span> df\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">MakeFeatures</span>(df):\n  vectorian = SentencePieceVectorian()\n\n  features = []\n  sentences = []\n  accounts = []\n  <span class=\"synStatement\">for</span> feature,account <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">zip</span>(df[<span class=\"synConstant\">'sentence'</span>],df[<span class=\"synConstant\">'account_name'</span>]):\n    f = vectorian.fit(feature).indices\n    features.append(f)\n    sentences.append(feature)\n    accounts.append(account)\n\n  features = pad_sequences(features, maxlen=vectorian.max_tokens_len)\n\n  <span class=\"synStatement\">return</span> features,sentences,vectorian,accounts\n</pre>\n\n\n<p>では、ここまでの前処理を流します。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>data = PreprocessData(df,dirname) <span class=\"synComment\"># dirnameは、出力結果などを入れたいpath入れてください</span>\n</pre>\n\n\n<p>次はtrain_test_splitを行いますが、先ほども記述した通り通常の手法ではリークするので、以下のようにしました。\n（上述の通りGroupKFoldの実施で回避できる）</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synStatement\">def</span> <span class=\"synIdentifier\">CollectData</span>(data,account):\n  features = []\n  sentences = []\n  labels = []\n  \n  <span class=\"synStatement\">for</span> ac <span class=\"synStatement\">in</span> account:\n    where_ = np.where(np.array(data[<span class=\"synConstant\">'account'</span>]) == ac)\n    features.extend(np.array(data[<span class=\"synConstant\">'features'</span>])[where_])\n    sentences.extend(np.array(data[<span class=\"synConstant\">'sentences'</span>])[where_])\n    labels.extend(np.array(data[<span class=\"synConstant\">'labels'</span>])[where_])\n  <span class=\"synStatement\">return</span> np.array(features),np.array(sentences),np.array(labels)\n\ntrain_account,test_account = train_test_split(<span class=\"synIdentifier\">list</span>(<span class=\"synIdentifier\">set</span>(data[<span class=\"synConstant\">'account'</span>])),test_size=<span class=\"synConstant\">0.2</span>,random_state=<span class=\"synConstant\">1</span>)\ntrain_account,val_account = train_test_split(train_account,test_size=<span class=\"synConstant\">0.25</span>,random_state=<span class=\"synConstant\">1</span>)\n\ntrain_features,train_sen,train_labels = CollectData(data,train_account)\nval_features,val_sen,val_labels = CollectData(data,val_account)\ntest_features,test_sen,test_labels = CollectData(data,test_account)\n</pre>\n\n\n<p>通常のデータセットであれば、以下のようにすることでデータの分割が行えます。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>(train_features,val_features,\n train_labels,val_labels,\n train_sen,val_sen) = train_test_split(data[<span class=\"synConstant\">'features'</span>], data[<span class=\"synConstant\">'labels'</span>], data[<span class=\"synConstant\">'sentences'</span>], test_size=<span class=\"synConstant\">0.2</span>, random_state=<span class=\"synConstant\">1</span>)\n\n(train_features,test_features,\n train_labels,test_labels,\n train_sen,test_sen) = train_test_split(train_features, train_labels, train_sen, test_size=<span class=\"synConstant\">0.25</span>, random_state=<span class=\"synConstant\">1</span>)\n</pre>\n\n\n<p>ここまででデータの整形が完了です。</p>\n\n<h2 id=\"学習\">学習</h2>\n\n<p>次は、モデルの定義を行います。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synStatement\">def</span> <span class=\"synIdentifier\">_create_model</span>(input_shape, hidden, class_count,vectorian):\n    input_tensor = Input(input_shape)\n    common_input = vectorian.get_keras_layer(trainable=<span class=\"synIdentifier\">True</span>)(input_tensor)\n    x1 = SeqSelfAttention(name=<span class=\"synConstant\">'attention'</span>)(common_input)\n    x1 = Bidirectional(LSTM(hidden))(x1)\n    x1 = Dropout(<span class=\"synConstant\">0.5</span>)(x1)\n    x1 = Dense(<span class=\"synConstant\">32</span>)(x1)\n    x1 = Dropout(<span class=\"synConstant\">0.5</span>)(x1)\n    x1 = Dense(<span class=\"synConstant\">16</span>)(x1)\n    x1 = Dropout(<span class=\"synConstant\">0.5</span>)(x1)\n    output_tensor = Dense(class_count, activation=<span class=\"synConstant\">'softmax'</span>, name=<span class=\"synConstant\">'class'</span>)(x1)\n\n    model = Model(input_tensor, output_tensor)\n    model.compile(loss=<span class=\"synConstant\">'categorical_crossentropy'</span>, optimizer=<span class=\"synConstant\">'nadam'</span>, metrics=[<span class=\"synConstant\">'acc'</span>])\n\n    <span class=\"synStatement\">return</span> model\n\nhidden = <span class=\"synConstant\">356</span>\nmodel = _create_model(train_features[<span class=\"synConstant\">0</span>].shape, hidden, data[<span class=\"synConstant\">'class_count'</span>],data[<span class=\"synConstant\">'vectorian'</span>])\nmodel.summary()\n</pre>\n\n\n<p>作成したモデルにデータを流して学習を進めます。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>model_filename=<span class=\"synConstant\">'{0}/model.h5'</span>.format(dirname)\n\nhistory = model.fit(train_features, train_labels,\n                    epochs=<span class=\"synConstant\">50</span>,\n                    batch_size=<span class=\"synConstant\">32</span>,\n                    validation_data=(val_features, val_labels),\n                    shuffle=<span class=\"synIdentifier\">False</span>,\n                    callbacks = [\n                        EarlyStopping(patience=<span class=\"synConstant\">5</span>, monitor=<span class=\"synConstant\">'val_acc'</span>, mode=<span class=\"synConstant\">'max'</span>),\n                        ModelCheckpoint(filepath=model_filename, monitor=<span class=\"synConstant\">'val_acc'</span>, mode=<span class=\"synConstant\">'max'</span>, save_best_only=<span class=\"synIdentifier\">True</span>)\n                    ])\n</pre>\n\n\n<h2 id=\"評価出力\">評価・出力</h2>\n\n<p>ModelCheckpointで保存したmodelを読み取り、さらにSelf-Attentionの結果を得られるようにします。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synPreProc\">from</span> keras.models <span class=\"synPreProc\">import</span> load_model\nmodel = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())\nmodel = Model(inputs=model.input, outputs=[model.output, model.get_layer(<span class=\"synConstant\">'attention'</span>).output])\n</pre>\n\n\n<p>modelにtest dataを入れて結果を取得します。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>out = model.predict(test_features)\n\ny = out[<span class=\"synConstant\">0</span>] <span class=\"synComment\"># 予測labelのsoftmaxが入っている</span>\nweight = out[<span class=\"synConstant\">1</span>] <span class=\"synComment\"># Self-Attentionのweighが入っている</span>\n\npred = np.argmax(y,<span class=\"synConstant\">1</span>) <span class=\"synComment\"># 予測値</span>\n<span class=\"synIdentifier\">max</span> = np.max(y,<span class=\"synConstant\">1</span>) <span class=\"synComment\"># 信頼値</span>\n\ndf_y = pd.DataFrame(np.array([np.argmax(test_labels,<span class=\"synConstant\">1</span>),pred,<span class=\"synIdentifier\">max</span>*<span class=\"synConstant\">100</span>]).T,columns=[<span class=\"synConstant\">'true'</span>,<span class=\"synConstant\">'pred'</span>,<span class=\"synConstant\">'trust'</span>]) <span class=\"synComment\"># 結果をまとめておくと精度確認に使える</span>\n</pre>\n\n\n<p>精度の確認を行います。</p>\n\n<p>ただ、testを入力し得られた結果を出力するだけでは精度が得られなかったので、信頼値が高いものだけを選別し、出力するようにしました。\n信頼値を90~55の間で出力し、<b>最もAccuracyが高い時の信頼値以上のものを出力</b>としました。</p>\n\n<p> 一方で信頼値を上げすぎるとわずかな出力しか得られないので、元のtestデータ数の1/3はデータ数が出力として確保できるような条件を加えました。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>report = classification_report(pred, np.argmax(test_labels,<span class=\"synConstant\">1</span>),output_dict=<span class=\"synIdentifier\">True</span>,target_names=[data[<span class=\"synConstant\">'index2label'</span>][i] <span class=\"synStatement\">for</span> i <span class=\"synStatement\">in</span> [<span class=\"synConstant\">0</span>,<span class=\"synConstant\">1</span>]])\nFirstReport_df = pd.DataFrame(report).T\n\n<span class=\"synIdentifier\">print</span>(FirstReport_df)\nFirstReport_df.to_csv(dirname+<span class=\"synConstant\">'NotCutReport.csv'</span>)\n\n<span class=\"synComment\"># 信頼値が高い予測だけを出力とすることで確からしいものだけをみる</span>\nAppSupport = FirstReport_df.loc[<span class=\"synConstant\">'app'</span>,<span class=\"synConstant\">'support'</span>] <span class=\"synComment\"># 予測した数を取得</span>\nDisappSupport = FirstReport_df.loc[<span class=\"synConstant\">'disapp'</span>,<span class=\"synConstant\">'support'</span>]\n\n<span class=\"synStatement\">for</span> UpperLimit <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">range</span>(<span class=\"synConstant\">90</span>,<span class=\"synConstant\">55</span>,-<span class=\"synConstant\">1</span>):\n  max_acc = <span class=\"synConstant\">0</span>\n  <span class=\"synStatement\">for</span> i <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">range</span>(<span class=\"synConstant\">50</span>,UpperLimit,<span class=\"synConstant\">1</span>):\n    df_y_cut = df_y[df_y.trust &gt; i]\n    report = classification_report(df_y_cut.pred, df_y_cut.true ,output_dict=<span class=\"synIdentifier\">True</span>)\n    report_df = pd.DataFrame(report).T\n    acc = report_df.loc[<span class=\"synConstant\">'accuracy'</span>,<span class=\"synConstant\">'precision'</span>]\n    <span class=\"synStatement\">if</span> max_acc &lt; acc:\n      max_acc = acc\n      max_i = i\n  df_y_cut = df_y[df_y.trust &gt; max_i]\n  report = classification_report(df_y_cut.pred, df_y_cut.true ,output_dict=<span class=\"synIdentifier\">True</span>,target_names=[data[<span class=\"synConstant\">'index2label'</span>][i] <span class=\"synStatement\">for</span> i <span class=\"synStatement\">in</span> [<span class=\"synConstant\">0</span>,<span class=\"synConstant\">1</span>]])\n  report_df = pd.DataFrame(report).T\n  <span class=\"synStatement\">if</span> (report_df.loc[<span class=\"synConstant\">'app'</span>,<span class=\"synConstant\">'support'</span>] &gt; HighSupport/<span class=\"synConstant\">3</span>) <span class=\"synStatement\">and</span> (report_df.loc[<span class=\"synConstant\">'disapp'</span>,<span class=\"synConstant\">'support'</span>] &gt; LowSupport/<span class=\"synConstant\">3</span>):\n    <span class=\"synComment\"># 元の予測値の1/3のデータ数が確保できていればクリア</span>\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">'UpperLimit:'</span> + <span class=\"synIdentifier\">str</span>(UpperLimit))\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">'max_i:'</span> + <span class=\"synIdentifier\">str</span>(max_i))\n    <span class=\"synIdentifier\">print</span>(report_df)\n    report_df.to_csv(dirname+<span class=\"synConstant\">'Report.csv'</span>)\n    <span class=\"synStatement\">break</span>\n</pre>\n\n\n<p>最後にSelf-AttentionのWeightをcsvで出力します。</p>\n\n<p>得られた出力結果は、予測値-承認と真値-承認、予測値-承認と真値-不承認、予測値-不承認と真値-承認、予測値-不承認と真値-不承認のように予測値と真値の結果に応じて4つに分けてcsvで出力するようになっています。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>app_app = pd.DataFrame([])\napp_disapp = pd.DataFrame([])\ndisapp_app = pd.DataFrame([])\ndisapp_disapp = pd.DataFrame([])\n\n<span class=\"synStatement\">for</span> i <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">range</span>(<span class=\"synIdentifier\">len</span>(test_features)):\n  input_text = test_sen[i]\n  tokens = data[<span class=\"synConstant\">'vectorian'</span>].tokenizer._tokenizer.encode_as_pieces(input_text)\n\n  conf = out[<span class=\"synConstant\">0</span>][i] * <span class=\"synConstant\">100</span>\n  wei = out[<span class=\"synConstant\">1</span>][i]\n\n  <span class=\"synStatement\">if</span> np.max(conf) &lt;= max_i:\n    <span class=\"synStatement\">continue</span>\n\n  pred = [data[<span class=\"synConstant\">'index2label'</span>][np.argmax(conf)]]\n  labels = [data[<span class=\"synConstant\">'index2label'</span>][np.argmax(test_labels[i])]]\n\n  weights = [w.max() <span class=\"synStatement\">for</span> w <span class=\"synStatement\">in</span> wei[-<span class=\"synIdentifier\">len</span>(tokens):]]\n\n  df = pd.DataFrame([tokens, weights], index=[<span class=\"synConstant\">'token'</span>, <span class=\"synConstant\">'weight'</span>]).T\n\n  mean = np.asarray(weights).mean()\n  <span class=\"synStatement\">for</span> j <span class=\"synStatement\">in</span> df.index:\n    <span class=\"synStatement\">if</span> df.loc[j,<span class=\"synConstant\">'weight'</span>] - mean &lt;= <span class=\"synConstant\">0</span>:\n      df.loc[j,<span class=\"synConstant\">'weight'</span>] = <span class=\"synConstant\">0</span>\n    <span class=\"synStatement\">else</span>:\n      df.loc[j,<span class=\"synConstant\">'weight'</span>] = df.loc[j,<span class=\"synConstant\">'weight'</span>] - mean\n  \n  pred += df.token.values.tolist()\n  labels += df.weight.values.tolist()\n\n  final = pd.DataFrame(np.array([pred,labels]).T,columns=[<span class=\"synConstant\">'pred'</span>,input_text])\n\n  <span class=\"synStatement\">if</span> (pred[<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">'app'</span>) &amp; (labels[<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">'app'</span>):\n    app_app = pd.concat([app_app,final],<span class=\"synConstant\">1</span>)\n  <span class=\"synStatement\">elif</span> (pred[<span class=\"synConstant\">0</span>]  == <span class=\"synConstant\">'app'</span>) &amp; (labels[<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">'disapp'</span>):\n    app_disapp = pd.concat([app_disapp,final],<span class=\"synConstant\">1</span>)\n  <span class=\"synStatement\">elif</span> (pred[<span class=\"synConstant\">0</span>]  == <span class=\"synConstant\">'disapp'</span>) &amp; (labels[<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">'app'</span>):\n    disapp_app = pd.concat([disapp_app,final],<span class=\"synConstant\">1</span>)\n  <span class=\"synStatement\">elif</span> (pred[<span class=\"synConstant\">0</span>]  == <span class=\"synConstant\">'disapp'</span>) &amp; (labels[<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">'diaspp'</span>):\n    disapp_disapp = pd.concat([disapp_disapp,final],<span class=\"synConstant\">1</span>)\n\napp_app.to_csv(dirname+<span class=\"synConstant\">'app_app.csv'</span>,index=<span class=\"synIdentifier\">False</span>)\napp_disapp.to_csv(dirname+<span class=\"synConstant\">'app_disapp.csv'</span>,index=<span class=\"synIdentifier\">False</span>)\ndisapp_app.to_csv(dirname+<span class=\"synConstant\">'disapp_app.csv'</span>,index=<span class=\"synIdentifier\">False</span>)\ndisapp_disapp.to_csv(dirname+<span class=\"synConstant\">'disapp_disapp.csv'</span>,index=<span class=\"synIdentifier\">False</span>)\n</pre>\n\n\n<h1 id=\"まとめ\">まとめ</h1>\n\n<p>Self-Attentionを用いて無料リスティングの不承認理由を解き明かそうと分析しました。\nしかし、<b>title,descriptionのみからは承認と不承認を分類することができず、不承認理由の解明には貢献できせんでした。</b></p>\n\n<p>Self-Attentionとデータセットの相性が悪いという可能性も考えられるので、tfidf+lgbも試みましたが、こちらもうまくいきませんでした。やはりこちらの結果からもtitle,descriptionのみからは承認と不承認を分類することができないということが考えられます。</p>\n","contentSnippet":"こんにちは株式会社フィードフォース2020年入社の機械学習エンジニア八百俊哉@Feedforce (@feed_yao) | Twitterと申します。最近はロードバイク にはまっており、ロードバイク購入後一ヶ月で一日100km走行に成功しました。今回、Google無料リスティングで不承認アカウントが発生する要因を調査する分析を行いました。Google 無料リスティングとは？なぜ今回分析が必要とされたのか？結果と考察Self-Attentionを採用した理由実装手順使用データのフォーマット必要ライブラリのインストール・インポートデータの前処理学習評価・出力まとめGoogle 無料リスティングとは？2020年10月にGoogleから公開されたGoogleショッピングタブに無料で商品掲載ができる「無料リスティング」のことです。Google 検索にサイトがインデックス登録されても料金が発生しないのと同様に、EC事業者は無料で利用可能になりました。Google 無料リスティングについての詳細は以下のサイトが参考になります。lab.ecbooster.jpなぜ今回分析が必要とされたのか？無料リスティングでは自社製品を無料でGoogleに掲載できます。しかしながら、課題として一部商品掲載が不承認となるケースが見受けられました。不承認となってしまうと自社商品の掲載ができていない状況が発生しています。不承認となる理由としては、「Googleが定める基準に対して、登録している商品データの属性数が足りない、内容が仕様に沿っていない場合、商品データの品質が低いため不承認になり、Googleの検索結果に表示させることができません。」とされています。これらを定量的に分析することで不承認となる理由を見つけ出す試みが始まりました。そのため今回の分析の目的は、商品の属性情報（title,description）から承認・不承認の要因を見つけ出し、不承認の商品を承認へと改善するための施策を考案することです。結果と考察今回の目的である「商品の属性情報（title,description）から承認・不承認の要因を見つけ出し、不承認の商品を承認へと改善するための施策を考案する」は、達成できませんでした。目的が達成できなかった理由として考えられる要因は、承認・不承認は商品のtitle,descriptionだけでは判断されていないということです。商品ごとのtitle,descriptionのみで承認・不承認が判断されているのではなく、商品データ全体またはアカウント全体のデータを総合的に見て、判断されている可能性が高いということがわかりました。承認・不承認予測のAccuracyとしては5割〜６割ほどで、承認・不承認を予測するという点でも低い精度となってしまいました。Self-Attentionを採用した理由今回はSelf-Attentionという手法を用いてこの課題解決を試みました。Self-Attentionとは、文章全体で重要とされるキーワードが予測結果と一緒に確認できるようになる手法です。Self-Attentionの仕組みについては詳しく書かれている方が多くいますので、ここでは割愛します。最初は、word2vecを用いて文章特徴量を作成し、承認・不承認を予測して終了という一連の流れを想定していました。しかし、今回の目的は承認・不承認を予測したいわけではなく、どの単語が承認・不承認と関わっているのかを確認し、不承認となっているアカウントを承認にすることです。もし仮にword2vecを用いた手法を採用すると予測結果の要因や理由が明確にならないので、不承認のアカウントを承認に改善する施策を考えることはできません。そのため今回は、Self-Attentionを用いて分類モデルを構築することで、承認・不承認の要因が文章内のどこにあるのかを分析するために、この手法を選択しました。実装手順本来の目的は達成できませんでしたが、Self-Attentionでの分類モデルの実装はできましたので、実装方法を記載します。今回はkerasを用いてSelf-Attention + LSTMで予測を行いました。検証環境はGoogle Colaboratoryを想定しています。使用データのフォーマット今回使用できるデータとしては以下のようなデータになっています。各アカウント・各商品ごとに商品IDが割り振られており、それぞれの商品にtitle,descriptionが割り振られています。承認・不承認のラベルは、アカウントごとに付加されています。必要ライブラリのインストール・インポート!pip install text_vectorian!pip install mojimoji!apt install aptitude!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y!pip install mecab-python3==0.7import pandas as pdimport numpy as npimport kerasimport osimport warningswarnings.simplefilter('ignore')import subprocessimport mojimojiimport reimport MeCabimport matplotlib.pyplot as pltfrom keras.layers import Dense, Dropout, LSTM, Embedding, BatchNormalizationfrom keras.layers.wrappers import Bidirectionalfrom keras.callbacks import EarlyStopping, ModelCheckpointfrom keras import Input, Model, utilsfrom keras.preprocessing.sequence import pad_sequencesfrom keras.callbacks import EarlyStoppingfrom text_vectorian import SentencePieceVectorianfrom keras_self_attention import SeqSelfAttentionfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_reportデータの前処理# データの読み込みapp = pd.read_csv('data/app.csv') # 承認データdisapp = pd.read_csv('data/disapp.csv') # 不承認データapp['target'] = 'app' # targetにlabelを代入するdisapp['target'] = 'disapp'# 今回は、titleとdescriptionを用いて予測するので、それら二つの変数を一つにまとめるapp['sentence'] = app['title'] + app['description'] disapp['sentence'] = disapp['title'] + disapp['description']# これまで別々に処理していたappとdisappをまとめてdfとするdf = app.append(disapp)今回のデータは特殊で、承認・不承認は商品ごとについているラベルではなくアカウントと紐づいたラベルとなっています。それらを各商品と承認・不承認が紐づいているとして各商品ごとに予測することを行ってます。ここで注意が必要なのは、データの分割方法です。アカウントを無視してデータを分割してしまうとリークを起こす可能性があります（リークとは、本来予測では使用できないデータが学習時に入ってしまっていることです）。そのため同じアカウントのデータが訓練データ、検証データ、テストデータに渡って存在しないようにしなければなりません。例えば、アカウントAの商品データは全て訓練データとする,アカウントBの商品データは全てテストデータにするといったようなことを意味しています。アカウントごとにデータを分割するには、各アカウントごとの商品数がある程度同じである方がlabelが不均衡にならないと考え、データ数を揃える処理を施しました。（これらはGroupKFoldを使用すれば解決できると考えられますが、分析実施時はGroupKFoldを知らなかった）# アカウントごとに商品数が異なるので50以上商品数がある場合は50までの商品を使用する# アカウントごとに商品数を揃えることで、labelが不均衡になることを緩和している# アカウントごとにlabelがふられるが、商品ごとに予測結果を出す時のみ実施cutted_df = pd.DataFrame([])for acc in df.account_name.unique():  data = df[df.account_name == acc]  if data.shape[0] > 50:     data = data[:50]  cutted_df = pd.concat([cutted_df,data],0)  df = cutted_df.sample(frac=1,random_state=1).reset_index(drop=True)次は、データの前処理についてです。自然言語処理の前処理で有効と言われている半角->全角、数字は全て0にする、スペース文字の消去を行いました。また、これまでlabelが'app'または'disapp'だったのでそれらを入力できる形式に変換しています。def PreprocessData(df,dirname):  # データの前処理関数  # 辞書型を返す  mecab = MeCab.Tagger('-Ochasen')  # textデータの前処理  df = TextPreprocess(df)  label2index = {k: i for i, k in enumerate(df.target.unique())}  index2label = {i: k for i, k in enumerate(df.target.unique())}  class_count = len(label2index)  labels = utils.to_categorical([label2index[label] for label in df.target], num_classes=class_count)  features,sentences,vectorian,account = MakeFeatures(df)  return {      'class_count': class_count,      'label2index': label2index,      'index2label': index2label,      'labels': labels,      'features': features,      'sentences':sentences,      'input_len': vectorian.max_tokens_len,      'vectorian':vectorian,      'account':account  }def TextPreprocess(df):  for i in df.index:    sen = df.loc[i,'sentence']    sen = mojimoji.han_to_zen(sen)    sen = re.sub(r'\\d+','0',sen)    df.loc[i,'sentence'] = sen.replace('\\u3000','')  return dfdef MakeFeatures(df):  vectorian = SentencePieceVectorian()  features = []  sentences = []  accounts = []  for feature,account in zip(df['sentence'],df['account_name']):    f = vectorian.fit(feature).indices    features.append(f)    sentences.append(feature)    accounts.append(account)  features = pad_sequences(features, maxlen=vectorian.max_tokens_len)  return features,sentences,vectorian,accountsでは、ここまでの前処理を流します。data = PreprocessData(df,dirname) # dirnameは、出力結果などを入れたいpath入れてください次はtrain_test_splitを行いますが、先ほども記述した通り通常の手法ではリークするので、以下のようにしました。（上述の通りGroupKFoldの実施で回避できる）def CollectData(data,account):  features = []  sentences = []  labels = []    for ac in account:    where_ = np.where(np.array(data['account']) == ac)    features.extend(np.array(data['features'])[where_])    sentences.extend(np.array(data['sentences'])[where_])    labels.extend(np.array(data['labels'])[where_])  return np.array(features),np.array(sentences),np.array(labels)train_account,test_account = train_test_split(list(set(data['account'])),test_size=0.2,random_state=1)train_account,val_account = train_test_split(train_account,test_size=0.25,random_state=1)train_features,train_sen,train_labels = CollectData(data,train_account)val_features,val_sen,val_labels = CollectData(data,val_account)test_features,test_sen,test_labels = CollectData(data,test_account)通常のデータセットであれば、以下のようにすることでデータの分割が行えます。(train_features,val_features, train_labels,val_labels, train_sen,val_sen) = train_test_split(data['features'], data['labels'], data['sentences'], test_size=0.2, random_state=1)(train_features,test_features, train_labels,test_labels, train_sen,test_sen) = train_test_split(train_features, train_labels, train_sen, test_size=0.25, random_state=1)ここまででデータの整形が完了です。学習次は、モデルの定義を行います。def _create_model(input_shape, hidden, class_count,vectorian):    input_tensor = Input(input_shape)    common_input = vectorian.get_keras_layer(trainable=True)(input_tensor)    x1 = SeqSelfAttention(name='attention')(common_input)    x1 = Bidirectional(LSTM(hidden))(x1)    x1 = Dropout(0.5)(x1)    x1 = Dense(32)(x1)    x1 = Dropout(0.5)(x1)    x1 = Dense(16)(x1)    x1 = Dropout(0.5)(x1)    output_tensor = Dense(class_count, activation='softmax', name='class')(x1)    model = Model(input_tensor, output_tensor)    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])    return modelhidden = 356model = _create_model(train_features[0].shape, hidden, data['class_count'],data['vectorian'])model.summary()作成したモデルにデータを流して学習を進めます。model_filename='{0}/model.h5'.format(dirname)history = model.fit(train_features, train_labels,                    epochs=50,                    batch_size=32,                    validation_data=(val_features, val_labels),                    shuffle=False,                    callbacks = [                        EarlyStopping(patience=5, monitor='val_acc', mode='max'),                        ModelCheckpoint(filepath=model_filename, monitor='val_acc', mode='max', save_best_only=True)                    ])評価・出力ModelCheckpointで保存したmodelを読み取り、さらにSelf-Attentionの結果を得られるようにします。from keras.models import load_modelmodel = load_model(model_filename, custom_objects=SeqSelfAttention.get_custom_objects())model = Model(inputs=model.input, outputs=[model.output, model.get_layer('attention').output])modelにtest dataを入れて結果を取得します。out = model.predict(test_features)y = out[0] # 予測labelのsoftmaxが入っているweight = out[1] # Self-Attentionのweighが入っているpred = np.argmax(y,1) # 予測値max = np.max(y,1) # 信頼値df_y = pd.DataFrame(np.array([np.argmax(test_labels,1),pred,max*100]).T,columns=['true','pred','trust']) # 結果をまとめておくと精度確認に使える精度の確認を行います。ただ、testを入力し得られた結果を出力するだけでは精度が得られなかったので、信頼値が高いものだけを選別し、出力するようにしました。信頼値を90~55の間で出力し、最もAccuracyが高い時の信頼値以上のものを出力としました。 一方で信頼値を上げすぎるとわずかな出力しか得られないので、元のtestデータ数の1/3はデータ数が出力として確保できるような条件を加えました。report = classification_report(pred, np.argmax(test_labels,1),output_dict=True,target_names=[data['index2label'][i] for i in [0,1]])FirstReport_df = pd.DataFrame(report).Tprint(FirstReport_df)FirstReport_df.to_csv(dirname+'NotCutReport.csv')# 信頼値が高い予測だけを出力とすることで確からしいものだけをみるAppSupport = FirstReport_df.loc['app','support'] # 予測した数を取得DisappSupport = FirstReport_df.loc['disapp','support']for UpperLimit in range(90,55,-1):  max_acc = 0  for i in range(50,UpperLimit,1):    df_y_cut = df_y[df_y.trust > i]    report = classification_report(df_y_cut.pred, df_y_cut.true ,output_dict=True)    report_df = pd.DataFrame(report).T    acc = report_df.loc['accuracy','precision']    if max_acc < acc:      max_acc = acc      max_i = i  df_y_cut = df_y[df_y.trust > max_i]  report = classification_report(df_y_cut.pred, df_y_cut.true ,output_dict=True,target_names=[data['index2label'][i] for i in [0,1]])  report_df = pd.DataFrame(report).T  if (report_df.loc['app','support'] > HighSupport/3) and (report_df.loc['disapp','support'] > LowSupport/3):    # 元の予測値の1/3のデータ数が確保できていればクリア    print('UpperLimit:' + str(UpperLimit))    print('max_i:' + str(max_i))    print(report_df)    report_df.to_csv(dirname+'Report.csv')    break最後にSelf-AttentionのWeightをcsvで出力します。得られた出力結果は、予測値-承認と真値-承認、予測値-承認と真値-不承認、予測値-不承認と真値-承認、予測値-不承認と真値-不承認のように予測値と真値の結果に応じて4つに分けてcsvで出力するようになっています。app_app = pd.DataFrame([])app_disapp = pd.DataFrame([])disapp_app = pd.DataFrame([])disapp_disapp = pd.DataFrame([])for i in range(len(test_features)):  input_text = test_sen[i]  tokens = data['vectorian'].tokenizer._tokenizer.encode_as_pieces(input_text)  conf = out[0][i] * 100  wei = out[1][i]  if np.max(conf) <= max_i:    continue  pred = [data['index2label'][np.argmax(conf)]]  labels = [data['index2label'][np.argmax(test_labels[i])]]  weights = [w.max() for w in wei[-len(tokens):]]  df = pd.DataFrame([tokens, weights], index=['token', 'weight']).T  mean = np.asarray(weights).mean()  for j in df.index:    if df.loc[j,'weight'] - mean <= 0:      df.loc[j,'weight'] = 0    else:      df.loc[j,'weight'] = df.loc[j,'weight'] - mean    pred += df.token.values.tolist()  labels += df.weight.values.tolist()  final = pd.DataFrame(np.array([pred,labels]).T,columns=['pred',input_text])  if (pred[0] == 'app') & (labels[0] == 'app'):    app_app = pd.concat([app_app,final],1)  elif (pred[0]  == 'app') & (labels[0] == 'disapp'):    app_disapp = pd.concat([app_disapp,final],1)  elif (pred[0]  == 'disapp') & (labels[0] == 'app'):    disapp_app = pd.concat([disapp_app,final],1)  elif (pred[0]  == 'disapp') & (labels[0] == 'diaspp'):    disapp_disapp = pd.concat([disapp_disapp,final],1)app_app.to_csv(dirname+'app_app.csv',index=False)app_disapp.to_csv(dirname+'app_disapp.csv',index=False)disapp_app.to_csv(dirname+'disapp_app.csv',index=False)disapp_disapp.to_csv(dirname+'disapp_disapp.csv',index=False)まとめSelf-Attentionを用いて無料リスティングの不承認理由を解き明かそうと分析しました。しかし、title,descriptionのみからは承認と不承認を分類することができず、不承認理由の解明には貢献できせんでした。Self-Attentionとデータセットの相性が悪いという可能性も考えられるので、tfidf+lgbも試みましたが、こちらもうまくいきませんでした。やはりこちらの結果からもtitle,descriptionのみからは承認と不承認を分類することができないということが考えられます。","link":"https://developer.feedforce.jp/entry/2021/03/11/101244","isoDate":"2021-03-11T01:12:44.000Z","dateMiliSeconds":1615425164000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210224/20210224172435.png","authorName":"yaonyan"},{"title":"新卒機械学習エンジニアがAI Quest2020に参加しました","content":"<p>この記事は <a href=\"https://adventar.org/calendars/5560\">Feedforce Advent Calendar 2020</a> の 4 日目の記事です。</p>\n\n<p>昨日は<a href=\"https://masutaka.net/chalow/\">ますたかさん</a>の<a href=\"https://masutaka.net/chalow/2020-12-03-1.html\">優先度より優先順位のほうが偉い</a>でした。優先度をつける時って高高高高高高ってなってしまうのわかります...。</p>\n\n<p>では、本題。</p>\n\n<p>今回AI Questという長期のプログラムに参加しました。そこで実際に行ったことや、学んだことを記録として残したいと思います。\nAI Questに参加してみたいと思っている方は、是非参考にしてください。\n（事実を列挙しただけなので読み物としては、面白味が少ないです...）</p>\n\n<p>先に書いておくと</p>\n\n<h1>長文失礼しました</h1>\n\n<h1>自己紹介</h1>\n\n<p>私は、2020年に株式会社フィードフォースに新卒で入社した八百です。データ分析チームで機械学習エンジニアとして、業務に携わっています。\n大学では、YOLOv2を用いて360度カメラで撮影された画像の物体検出を研究していました。最近、金魚を飼い始めました。\n<iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Ftwitter.com%2Ffeed_yao\" title=\"八百俊哉@Feedforce (@feed_yao) | Twitter\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://twitter.com/feed_yao\">twitter.com</a></cite></p>\n\n<h1>参加したきっかけ</h1>\n\n<p>弊社のデータ分析チームには機械学習経験者が私しかおらず、その私も学部で機械学習の卒論を書いた程度の知識しかありませんでした。そのような状態で社内のプロダクトにいきなり機械学習を導入するということがとてもハードルが高く、実際どのような工程を踏んで進めたら良いのかわかりませんでした。</p>\n\n<p>そんな時に上司の加藤さんがAI Questというプログラムを紹介してくださいました。</p>\n\n<blockquote><p>AI Questは、参加者同士が学び合い、高め合いながら\nAI活用を通した企業の課題解決方法を身につけるプログラムです。</p></blockquote>\n\n<p>面白そうだし、現在抱えている「どのようにして機械学習をプロダクトに導入したら良いのか」という疑問が解消できる良い機会だと思い、応募しました。</p>\n\n<h1>アセスメント課題を解く</h1>\n\n<p>参加するにあたり、まずは自身のプロフィール（志望動機など）を提出し、その後アセスメント課題というものを解きました。このアセスメント課題は、任意課題となっていましたが、やった方が合格率が高いのではないかと思い、挑戦しました。（実際のところどうなのかはわかりません。）</p>\n\n<p>内容としては、「民泊サービスにおける物件データを利用した宿泊価格予測モデルの作成」でした。基本的にはkaggleのようなコンペ形式でファイルを提出するとスコアが返ってくるというようなものです。また、それらのスコアは参加者全員に公開されており、参加者全体のうち自分がに何位なのかということを確認することができます。</p>\n\n<p>私はこれまでNNしか扱ったことがなかったので、NNを用いて宿泊価格を回帰しました。しかし、思ったようにスコアは向上しませんでした。その時は精度が上がらない原因は、モデルにあるだろうと勝手に決めてしまって、モデルの最適化ばかりに時間を使っていました。（ネットワーク構造を変更してみたり、optimizarを調整してみたり）</p>\n\n<p>それでもスコアは向上せず、アセスメント課題の締め切りの日数がだんだんと迫ってきていました。焦りを感じた私は、大学の頃にお世話になっていた先生に連絡を取り、相談してみました。</p>\n\n<p>すると、「コンペは、入力データチューニングすることによってスコアが上がる」ということを教えていただきました。</p>\n\n<p>大学の卒論では画像処理を行っていたので、入力データをチューニングする？？？って感じでしたが、kaggle notebookや様々な方のブログを通して、Data engineeringというものを知りました。</p>\n\n<p>そこからは様々なデータ加工を<a href=\"https://www.kaggle.com/learn/overview\">kaggle cources</a>という無料の学習教材でData engineeringを学習し、実践してみました。</p>\n\n<p>その結果、無事AI Questに参加できるということが決定しました!!</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154012.png\" alt=\"f:id:newton800:20201201154012p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>メールが届いた時は社内のいろんな人に自慢したい気持ちでしたが、弊社はほとんどの方がリモートワークなので席が近い複数の人に自慢したと思います。</p>\n\n<p>また、業務時間を用いてAI Questに挑戦してもよいということだったので、ここで落選してしまうと、、、って感じだったので良かったです。</p>\n\n<p>そこからは、AI QuestのSlackに招待されたり、SIGNATEアカウントを作成したりちょっとした雑務をやる感じでした。その中でも重要なのが、どのコースを受講するかを選択するところです。</p>\n\n<p>AI Questは大きく2つのタームに分かれていて、それぞれのタームごとに個人が参加したいコースを4つのうちから1つ選ぶというものです。</p>\n\n<p>ちなみに第一タームの４コースは以下のようになっていました。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154054.png\" alt=\"f:id:newton800:20201201154054p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>弊社は広告配信するためのフィードを作成しているプロダクトがあるので、自動的に最適な価格に変更できたらより良い広告配信に携われるのではないかと考えてPBL_02の小売価格最適化を選択しました。</p>\n\n<h1>AI Questが始まった！！</h1>\n\n<p>初回は、開会式と今後のについての説明をZoomで行いました。そこでアイスブレイクとして参加者同士で、自己紹介を行う時間が用意されていました。</p>\n\n<p>今後の流れとしては、ビジネス課題、AI課題、最終課題の3つのフェーズで区切られて、それぞれのフェーズごとに期限つきの課題が課せられるというものです。</p>\n\n<h1>ビジネス課題を解く</h1>\n\n<p>第一課題では主に要求定義、要件定義を行いました。私は新卒ということもあり業務上、そのようなことをやった経験がなく、ビジネス課題を通して新しい知識を身に付けたいと思っていました。</p>\n\n<p>課題の詳細としては以下のようなものです。</p>\n\n<ol>\n<li>要求定義\n\n<ol>\n<li>必要な情報の洗い出し、情報の取得応報の検討</li>\n</ol>\n</li>\n<li>要件定義\n\n<ol>\n<li>AI化業務の具体化、PoCにおける要件定義項目の検討</li>\n<li>2.1で検討したPoCにおける要件定義項目ごとの要件検討</li>\n</ol>\n</li>\n</ol>\n\n\n<p>といった感じです。</p>\n\n<p>まずPoCという言葉を初めて知りました。</p>\n\n<blockquote><p>PoCとは、Proof of Conceptの略で、「概念実証」という意味です。新しい概念や理論、原理、アイディアの実証を目的とした、試作開発の前段階における検証やデモンストレーションを指します。</p></blockquote>\n\n<p>なるほど。開発する前の試行錯誤のことをPoCというのか...といった知識レベルでの参加でした。</p>\n\n<p>ここは、自身で考えたものを提出することで、模範解答がダウンロードできるので、模範解答と自身で作成したものを比較することで、何が足りていなかったのかを確認する事ができました。</p>\n\n<p>この時私が学んだこととしては、まず機械学習を導入するにあたりどれくらいの精度または利益が生まれることを想定しているのかを事前に相談しておく必要があるということです。\nクライアントが機械学習への期待が高すぎると、実際に完成しても「それだけしか効果ないんですか？」みたいな感じになりかねないです。そのためにまず初めに、効果として具体的な目標数値をすり合わせておく必要があります。\nまた、ヒアリングで聞いておいたら良いことや、モデル作成のために行わなければいけない工程を一通り体験できたのは、良い経験になったと思います。</p>\n\n<h1>AI課題を解く</h1>\n\n<p>私はPBL_02の小売価格最適化を選択していたので、粗利が最大となるように約700個の商品の価格を決めるという課題でした。</p>\n\n<p>また、提出回数は24回と定められており、この24回というのは6ヶ月×4週間ということを意味しています。そのため、6ヶ月間かけて週ごとのデータを用いてPoCを実施しているということですね。</p>\n\n<p>そして最終的な粗利が最大となった人がコンペ形式で入賞するというものになっています。</p>\n\n<h2>データ収集編</h2>\n\n<p>今回の課題で面白いところは、最初のデータは各商品につき一つずつしか存在していないということです。なんというデータ管理なんでしょうか...</p>\n\n<p>最適な価格を予想するとなると、各商品ごとに様々な価格で販売して、どれくらい売れたかのデータがすでにあることを想定していましたが、まさかの一個でした。</p>\n\n<p>言葉ではデータは伝えにくいので図示すると以下のような感じになっています。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"私がイメージしていたデータ構造\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154140.png\" alt=\"f:id:newton800:20201201154140p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>私がイメージしていたデータ構造</figcaption></figure></p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"実際に配布されたデータ\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154215.png\" alt=\"f:id:newton800:20201201154215p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>実際に配布されたデータ</figcaption></figure></p>\n\n<p>あれ？これ最適価格を予想できなく無いか...？？</p>\n\n<p>三日くらい、これどうやったら最適価格を作成できるんだと悩んでいました。</p>\n\n<p>しかし、ここで一人で悩むだけで終わらないのがAI Questです。同じ課題を解いている人がたくさんいるので、他の人に聞けば良いのです。</p>\n\n<p>AI Questでは毎週土曜日にサロンと言って参加者同士でコミュニケーションをとり、現在悩んでいることや解決の糸口の共有などを行う機会が設けられていました。こちらは自由参加で予定が空いている人が参加するという感じになっています。</p>\n\n<p>このサロンでどのようにして最適価格を予想しているのか、他の人に聞いてみると、以下の解答が得られました。</p>\n\n<p>「今回の課題は、予測価格を提出するといくら売れたかのフィードバックが得られます。そのため、初めは適当な数字で価格を設定して提出すると、データが増える。」</p>\n\n<p>ということでした。</p>\n\n<p>なるほど。最初はデータは一つしかないが、提出回数の24回の内の何回かはデータ集めのために適当な価格で提出する必要がありました。</p>\n\n<p>データ集めのイメージとしては下の画像の感じです。これらを複数回繰り返すことで機械学習に必要なデータを収集していきました。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"データ収集のイメージ\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154251.png\" alt=\"f:id:newton800:20201201154251p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>データ収集のイメージ</figcaption></figure></p>\n\n<p>また、適当な値といっても、ランダムな値を入力すると24回の提出を無駄にすることになるので、各商品の（原価×X）円というものを3回ほど行い最初のデータ収集を行いました。</p>\n\n<h2>売り上げ個数予測モデル作成編</h2>\n\n<p>次に売り上げ個数を予測するモデルを作成しました。粗利最大化するのに売り上げ個数を予測する理由は、粗利 = 販売個数×(価格 - 原価)なので、価格を入力として販売個数が出力されるモデルを作成すれば、粗利が最大となる各商品の値段がわかるからです。</p>\n\n<p>ここでは各商品ごとの属性データ（レビューや商品のサイズ、商品説明文の長さなど）と価格を入力することで売り上げ個数を予測するモデル(xgboost)を作成しました。</p>\n\n<p>この時、最適と思われる価格とその価格を提出した際に得られるフィードバックデータでデータ数がどんどん増えるので、予測精度は向上していきました。また、それに伴って粗利も向上していました。</p>\n\n<p>しかし、途中から精度、粗利が横這いで停滞してしまった時がありました。その時は、モデルを見直したり、他の予測モデルと組み合わせてみたりと様々なことをして粗利向上を目指しましたが、効果は得られませんでした。</p>\n\n<p>複数回粗利改善が失敗した時に、さすがに提出データがおかしいのではないか？と思い始め、改めてこれまで提出したデータとそれによって得られていたフィードバックを確認してみました。すると!!!</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154320.png\" alt=\"f:id:newton800:20201201154320p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>同じ価格で提出しているではありませんか！！！\nすでに既知の価格で提出することは、その価格での販売個数はすでにわかっているものになるので、無意味な提出となってしまいます。</p>\n\n<p>このようなことを複数回にわたって行ってしまっていたので、精度は上がらず、粗利も停滞してしまいました。\nもう少し早めにデータ全体を確認しておけば、防げたミスだったので、データの確認を怠ってはいけないということを学びました。</p>\n\n<h2>ルールベース編</h2>\n\n<p>ここからはルールベースを用いて最適価格を求めていきました。この段階で改めてこれまでフィードバックで得られたデータを元に各商品ごとに価格と粗利をプロットして状況を確認しました。\nその結果大きく2種類の粗利曲線に分類できることがわかりました。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"画像a:粗利最大点が定まってきている\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154407.png\" alt=\"f:id:newton800:20201201154407p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>画像a:粗利最大点が定まってきている</figcaption></figure></p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"画像b:粗利最大点が見つかっていない\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154449.png\" alt=\"f:id:newton800:20201201154449p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>画像b:粗利最大点が見つかっていない</figcaption></figure></p>\n\n<p>上の画像（画像a）では大体粗利最大の位置が定まってきています。下の画像（画像b）では、まだ粗利が最大点は見つかっていないようです。</p>\n\n<p>これら2パターンに場合分けを行い、それぞれにルールを用意することで次の提案価格を探索していきました。</p>\n\n<h3>画像aに対する次の提案価格を探索するルール</h3>\n\n<p>画像aのようになっている商品ではある程度、粗利の最大点がわかってきているので、現在粗利が最大となっている箇所の付近から探索しました。その方法としては、現在粗利が最大となっている価格点から両隣の価格点で間が広い価格との真ん中を新たな価格とするというものです。\n言葉で説明しても伝わりづらいので、図示すると以下の感じです。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"ルールベースで粗利が最大となる価格を求めるロジック\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154534.png\" alt=\"f:id:newton800:20201201154534p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>ルールベースで粗利が最大となる価格を求めるロジック</figcaption></figure></p>\n\n<p>価格点の差が大きい箇所の方が、探索が不十分であるという仮定のもとで成り立っています。</p>\n\n<h3>画像bに対する次の提案価格を探索するルール</h3>\n\n<p>画像bの場合は、まだ最大点が見えていないので、価格と粗利の関係を二次回帰で曲線を引き、最大点となる価格を探索していきました。</p>\n\n<p>ここで24回の提出が終えたという感じです。\n結果は....\n<span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154621.png\" alt=\"f:id:newton800:20201201154621p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>17位でした...\n70人参加していたので25%には入ったという感じでしょうか...</p>\n\n<p> 早めに同じ価格で提出して無意味なフィードバックを得ていることに気がついておけば...と後悔が残る結果になってしまいました。</p>\n\n<p>この課題でAI課題は終了です。\nAI課題を振り返ってやって良かったと思うことは、毎週サロンに参加したことです。サロンでは、自分一人では思いつかないようなアイデアを誰かが持っていたり、それらを共有し合うことで次のステップが見えてきたりします。見学しているだけでも楽しいので、とりあえず参加して見るのも良いかもしれないです。\nまた、土曜日の昼過ぎのゴールデンタイムに開催されていたので、用事がある時などは、中抜けしたりもしていました。。。</p>\n\n<p>そのほかにも、実務では挑戦していなかった新しい手法にも挑戦することができました。これまでアンサンブルで予測モデルは作成したことがなかったんですが、作成できるようになったり、Optunaでパラメータ調整に挑戦することができたのは良かったと思います。（Kaggleでも学べますが...）</p>\n\n<p>逆によくなかったと思うこともあります。それは、データの確認とゴールまでの経路を考えるということです。\nデータの確認に関しては、分析着手する際はかなり入念にやっていたのですが、フィードバックデータが増えてから、データの確認を行わずに進めてしまっていました。そんなこともあり、同じ価格で提出してしまうという凡ミスをしてしまいました。データは定期的に確認しないといけないということを学びました。</p>\n\n<p>もう一つは、ゴールまでの経路を考えるということです。PoCの実施なので右往左往することもありますが、私の場合は無計画すぎたと思います。特に今回は24回しか検証できないので、どこまでデータを集めて、どこまで機械学習を使い、どこからルールベースでやるのかといった大まかなロードマップは作成しておいた方が良いと思いました。これは、少なからず経験も必要だと思うので、あと何回でこの精度まで持っていくという目標を持ちながら、日々の分析業務を行っていきたいです。</p>\n\n<h1>最終課題を解く</h1>\n\n<p>最終課題では、これまでPoC結果を元に、実企業での意思決定の場を想定し、本番実装の意思決定をしてもらえるようなプレゼンテーション資料を作成するというものです。</p>\n\n<p>そしてプレゼン資料を作成し、提出すると次は他の参加者が作成したプレゼン資料を評価するというものでした。他の参加者の資料を見るまでは、自分のプレゼン資料はそれなりにわかりやすいと思っていたのですが、他の参加者のスライドを拝見して自身のスライドの未熟さに気がつきました。</p>\n\n<p>今回参加されている方々は、社会人歴が私よりも長い方が多かったので、実業務をやっている上で身についていることも多いと思います。また、他の参加者の方が作成したプレゼンをレビューすることで現在の私と何が違ったのかを何が足りなかったのかがわかったので良い経験になりました。</p>\n\n<p>具体的に私自身のプレゼンが何が良くて何が足りていなかったのか振り返って見たいと思います。</p>\n\n<p><strong>よかったところ</strong></p>\n\n<p>事実は簡潔にわかりやすくかけていた\n今回のプレゼンは、経営者向けのプレゼンなので機械学習の仕組みを解説することよりも全体的にどう動いているのかや、導入することでどれほどの粗利改善効果が見込めるのかといった、これまでの分析・予測から結果がはっきりしているものについては、丁寧にわかりやすく記述できていました。\nまた、ヒアリングで得た内容や業務フローの変化についても事実としてすでに出ていたものなので、まとめられていました。</p>\n\n<p><strong>足りていなかったところ</strong></p>\n\n<p>事実としてまだ取得されていない情報が一切記載できなかった。粗利改善効果などは計算から求めることができたが、価格設定の工数削減割合や、業務の属人化が解消が期待できるなど、分析・予測から導き出せない効果が記載できなかった。工数の削減目安などは、業務を通して得られるものなのか、それともなんらかのロジックがあるのかといった解説がなかったのでそれらをどのように算出したのかは気になりました。</p>\n\n<p>次に足りていなかったことは、導入を検討する場合の予算の話です。これらもどのようにして計算しているのかは謎ですが、初期費用として？？？円、継続運用するのにかかる費用（サポート代）として？？？円などのような記載が足りていませんでした。確かに意思決定する場合は、予算の話があった方が導入判断は容易になるだろうと思います。</p>\n\n<p>また、予算に伴い今後プロジェクトを進めていくスケジュールも記載できていませんでした。ソフトがいつぐらいで完成して、そこから運用に定着するまでにどれくらいの期間が必要なのかも意思決定する際にはあった方が良いなと気がつきました。</p>\n\n<p>上記3つのことが足りていなかったことです。これら全てに共通して言えることは、分析・予測から求められない見込みが記載できていなかったということです。何を持ってその見込み計算を行っているのかはわかりませんが、見込みが足りていないということがわかったので、今後資料作成の際には、先輩方に頼りながら見込みの記載方法も勉強していきます。</p>\n\n<h1>第一タームを振り返る</h1>\n\n<p>第一タームを振り返り、「どのようにして機械学習をプロダクトに導入したら良いのか」が解決できたのか。\n機械学習モデルの作成する前のフロー（要件定義・PoC計画）とモデル作成後のフロー（スライド作成）では、フィードバックや他の参加者の方の方法を参考にすることで、自分自身に何が足りていなかったのかを学ぶことができました。ただ現在は、足りていないことに気がついた段階で、実践はできていない状況です。これらの足りていなかった部分を第二タームや業務で実践することで、身につけていきたいと考えています。</p>\n\n<p>具体的な行動としては、\n1. モデル作成前に目標数値を定める\n1. PoC計画を緻密に立てる（あくまで試行錯誤なので柔軟性は忘れてはいけない）\n1. プレゼン資料には、事実だけでなく、見込みも記載する</p>\n\n<p>明日の <a href=\"https://adventar.org/calendars/5560\">Feedforce Advent Calendar 2020</a> は、いくみんさんが「9月からダラダラと書き終わらなかったやつを納めます...」ということなので、楽しみですね。</p>\n","contentSnippet":"この記事は Feedforce Advent Calendar 2020 の 4 日目の記事です。昨日はますたかさんの優先度より優先順位のほうが偉いでした。優先度をつける時って高高高高高高ってなってしまうのわかります...。では、本題。今回AI Questという長期のプログラムに参加しました。そこで実際に行ったことや、学んだことを記録として残したいと思います。AI Questに参加してみたいと思っている方は、是非参考にしてください。（事実を列挙しただけなので読み物としては、面白味が少ないです...）先に書いておくと長文失礼しました自己紹介私は、2020年に株式会社フィードフォースに新卒で入社した八百です。データ分析チームで機械学習エンジニアとして、業務に携わっています。大学では、YOLOv2を用いて360度カメラで撮影された画像の物体検出を研究していました。最近、金魚を飼い始めました。twitter.com参加したきっかけ弊社のデータ分析チームには機械学習経験者が私しかおらず、その私も学部で機械学習の卒論を書いた程度の知識しかありませんでした。そのような状態で社内のプロダクトにいきなり機械学習を導入するということがとてもハードルが高く、実際どのような工程を踏んで進めたら良いのかわかりませんでした。そんな時に上司の加藤さんがAI Questというプログラムを紹介してくださいました。AI Questは、参加者同士が学び合い、高め合いながらAI活用を通した企業の課題解決方法を身につけるプログラムです。面白そうだし、現在抱えている「どのようにして機械学習をプロダクトに導入したら良いのか」という疑問が解消できる良い機会だと思い、応募しました。アセスメント課題を解く参加するにあたり、まずは自身のプロフィール（志望動機など）を提出し、その後アセスメント課題というものを解きました。このアセスメント課題は、任意課題となっていましたが、やった方が合格率が高いのではないかと思い、挑戦しました。（実際のところどうなのかはわかりません。）内容としては、「民泊サービスにおける物件データを利用した宿泊価格予測モデルの作成」でした。基本的にはkaggleのようなコンペ形式でファイルを提出するとスコアが返ってくるというようなものです。また、それらのスコアは参加者全員に公開されており、参加者全体のうち自分がに何位なのかということを確認することができます。私はこれまでNNしか扱ったことがなかったので、NNを用いて宿泊価格を回帰しました。しかし、思ったようにスコアは向上しませんでした。その時は精度が上がらない原因は、モデルにあるだろうと勝手に決めてしまって、モデルの最適化ばかりに時間を使っていました。（ネットワーク構造を変更してみたり、optimizarを調整してみたり）それでもスコアは向上せず、アセスメント課題の締め切りの日数がだんだんと迫ってきていました。焦りを感じた私は、大学の頃にお世話になっていた先生に連絡を取り、相談してみました。すると、「コンペは、入力データチューニングすることによってスコアが上がる」ということを教えていただきました。大学の卒論では画像処理を行っていたので、入力データをチューニングする？？？って感じでしたが、kaggle notebookや様々な方のブログを通して、Data engineeringというものを知りました。そこからは様々なデータ加工をkaggle courcesという無料の学習教材でData engineeringを学習し、実践してみました。その結果、無事AI Questに参加できるということが決定しました!!メールが届いた時は社内のいろんな人に自慢したい気持ちでしたが、弊社はほとんどの方がリモートワークなので席が近い複数の人に自慢したと思います。また、業務時間を用いてAI Questに挑戦してもよいということだったので、ここで落選してしまうと、、、って感じだったので良かったです。そこからは、AI QuestのSlackに招待されたり、SIGNATEアカウントを作成したりちょっとした雑務をやる感じでした。その中でも重要なのが、どのコースを受講するかを選択するところです。AI Questは大きく2つのタームに分かれていて、それぞれのタームごとに個人が参加したいコースを4つのうちから1つ選ぶというものです。ちなみに第一タームの４コースは以下のようになっていました。弊社は広告配信するためのフィードを作成しているプロダクトがあるので、自動的に最適な価格に変更できたらより良い広告配信に携われるのではないかと考えてPBL_02の小売価格最適化を選択しました。AI Questが始まった！！初回は、開会式と今後のについての説明をZoomで行いました。そこでアイスブレイクとして参加者同士で、自己紹介を行う時間が用意されていました。今後の流れとしては、ビジネス課題、AI課題、最終課題の3つのフェーズで区切られて、それぞれのフェーズごとに期限つきの課題が課せられるというものです。ビジネス課題を解く第一課題では主に要求定義、要件定義を行いました。私は新卒ということもあり業務上、そのようなことをやった経験がなく、ビジネス課題を通して新しい知識を身に付けたいと思っていました。課題の詳細としては以下のようなものです。要求定義必要な情報の洗い出し、情報の取得応報の検討要件定義AI化業務の具体化、PoCにおける要件定義項目の検討2.1で検討したPoCにおける要件定義項目ごとの要件検討といった感じです。まずPoCという言葉を初めて知りました。PoCとは、Proof of Conceptの略で、「概念実証」という意味です。新しい概念や理論、原理、アイディアの実証を目的とした、試作開発の前段階における検証やデモンストレーションを指します。なるほど。開発する前の試行錯誤のことをPoCというのか...といった知識レベルでの参加でした。ここは、自身で考えたものを提出することで、模範解答がダウンロードできるので、模範解答と自身で作成したものを比較することで、何が足りていなかったのかを確認する事ができました。この時私が学んだこととしては、まず機械学習を導入するにあたりどれくらいの精度または利益が生まれることを想定しているのかを事前に相談しておく必要があるということです。クライアントが機械学習への期待が高すぎると、実際に完成しても「それだけしか効果ないんですか？」みたいな感じになりかねないです。そのためにまず初めに、効果として具体的な目標数値をすり合わせておく必要があります。また、ヒアリングで聞いておいたら良いことや、モデル作成のために行わなければいけない工程を一通り体験できたのは、良い経験になったと思います。AI課題を解く私はPBL_02の小売価格最適化を選択していたので、粗利が最大となるように約700個の商品の価格を決めるという課題でした。また、提出回数は24回と定められており、この24回というのは6ヶ月×4週間ということを意味しています。そのため、6ヶ月間かけて週ごとのデータを用いてPoCを実施しているということですね。そして最終的な粗利が最大となった人がコンペ形式で入賞するというものになっています。データ収集編今回の課題で面白いところは、最初のデータは各商品につき一つずつしか存在していないということです。なんというデータ管理なんでしょうか...最適な価格を予想するとなると、各商品ごとに様々な価格で販売して、どれくらい売れたかのデータがすでにあることを想定していましたが、まさかの一個でした。言葉ではデータは伝えにくいので図示すると以下のような感じになっています。私がイメージしていたデータ構造実際に配布されたデータあれ？これ最適価格を予想できなく無いか...？？三日くらい、これどうやったら最適価格を作成できるんだと悩んでいました。しかし、ここで一人で悩むだけで終わらないのがAI Questです。同じ課題を解いている人がたくさんいるので、他の人に聞けば良いのです。AI Questでは毎週土曜日にサロンと言って参加者同士でコミュニケーションをとり、現在悩んでいることや解決の糸口の共有などを行う機会が設けられていました。こちらは自由参加で予定が空いている人が参加するという感じになっています。このサロンでどのようにして最適価格を予想しているのか、他の人に聞いてみると、以下の解答が得られました。「今回の課題は、予測価格を提出するといくら売れたかのフィードバックが得られます。そのため、初めは適当な数字で価格を設定して提出すると、データが増える。」ということでした。なるほど。最初はデータは一つしかないが、提出回数の24回の内の何回かはデータ集めのために適当な価格で提出する必要がありました。データ集めのイメージとしては下の画像の感じです。これらを複数回繰り返すことで機械学習に必要なデータを収集していきました。データ収集のイメージまた、適当な値といっても、ランダムな値を入力すると24回の提出を無駄にすることになるので、各商品の（原価×X）円というものを3回ほど行い最初のデータ収集を行いました。売り上げ個数予測モデル作成編次に売り上げ個数を予測するモデルを作成しました。粗利最大化するのに売り上げ個数を予測する理由は、粗利 = 販売個数×(価格 - 原価)なので、価格を入力として販売個数が出力されるモデルを作成すれば、粗利が最大となる各商品の値段がわかるからです。ここでは各商品ごとの属性データ（レビューや商品のサイズ、商品説明文の長さなど）と価格を入力することで売り上げ個数を予測するモデル(xgboost)を作成しました。この時、最適と思われる価格とその価格を提出した際に得られるフィードバックデータでデータ数がどんどん増えるので、予測精度は向上していきました。また、それに伴って粗利も向上していました。しかし、途中から精度、粗利が横這いで停滞してしまった時がありました。その時は、モデルを見直したり、他の予測モデルと組み合わせてみたりと様々なことをして粗利向上を目指しましたが、効果は得られませんでした。複数回粗利改善が失敗した時に、さすがに提出データがおかしいのではないか？と思い始め、改めてこれまで提出したデータとそれによって得られていたフィードバックを確認してみました。すると!!!同じ価格で提出しているではありませんか！！！すでに既知の価格で提出することは、その価格での販売個数はすでにわかっているものになるので、無意味な提出となってしまいます。このようなことを複数回にわたって行ってしまっていたので、精度は上がらず、粗利も停滞してしまいました。もう少し早めにデータ全体を確認しておけば、防げたミスだったので、データの確認を怠ってはいけないということを学びました。ルールベース編ここからはルールベースを用いて最適価格を求めていきました。この段階で改めてこれまでフィードバックで得られたデータを元に各商品ごとに価格と粗利をプロットして状況を確認しました。その結果大きく2種類の粗利曲線に分類できることがわかりました。画像a:粗利最大点が定まってきている画像b:粗利最大点が見つかっていない上の画像（画像a）では大体粗利最大の位置が定まってきています。下の画像（画像b）では、まだ粗利が最大点は見つかっていないようです。これら2パターンに場合分けを行い、それぞれにルールを用意することで次の提案価格を探索していきました。画像aに対する次の提案価格を探索するルール画像aのようになっている商品ではある程度、粗利の最大点がわかってきているので、現在粗利が最大となっている箇所の付近から探索しました。その方法としては、現在粗利が最大となっている価格点から両隣の価格点で間が広い価格との真ん中を新たな価格とするというものです。言葉で説明しても伝わりづらいので、図示すると以下の感じです。ルールベースで粗利が最大となる価格を求めるロジック価格点の差が大きい箇所の方が、探索が不十分であるという仮定のもとで成り立っています。画像bに対する次の提案価格を探索するルール画像bの場合は、まだ最大点が見えていないので、価格と粗利の関係を二次回帰で曲線を引き、最大点となる価格を探索していきました。ここで24回の提出が終えたという感じです。結果は....17位でした...70人参加していたので25%には入ったという感じでしょうか... 早めに同じ価格で提出して無意味なフィードバックを得ていることに気がついておけば...と後悔が残る結果になってしまいました。この課題でAI課題は終了です。AI課題を振り返ってやって良かったと思うことは、毎週サロンに参加したことです。サロンでは、自分一人では思いつかないようなアイデアを誰かが持っていたり、それらを共有し合うことで次のステップが見えてきたりします。見学しているだけでも楽しいので、とりあえず参加して見るのも良いかもしれないです。また、土曜日の昼過ぎのゴールデンタイムに開催されていたので、用事がある時などは、中抜けしたりもしていました。。。そのほかにも、実務では挑戦していなかった新しい手法にも挑戦することができました。これまでアンサンブルで予測モデルは作成したことがなかったんですが、作成できるようになったり、Optunaでパラメータ調整に挑戦することができたのは良かったと思います。（Kaggleでも学べますが...）逆によくなかったと思うこともあります。それは、データの確認とゴールまでの経路を考えるということです。データの確認に関しては、分析着手する際はかなり入念にやっていたのですが、フィードバックデータが増えてから、データの確認を行わずに進めてしまっていました。そんなこともあり、同じ価格で提出してしまうという凡ミスをしてしまいました。データは定期的に確認しないといけないということを学びました。もう一つは、ゴールまでの経路を考えるということです。PoCの実施なので右往左往することもありますが、私の場合は無計画すぎたと思います。特に今回は24回しか検証できないので、どこまでデータを集めて、どこまで機械学習を使い、どこからルールベースでやるのかといった大まかなロードマップは作成しておいた方が良いと思いました。これは、少なからず経験も必要だと思うので、あと何回でこの精度まで持っていくという目標を持ちながら、日々の分析業務を行っていきたいです。最終課題を解く最終課題では、これまでPoC結果を元に、実企業での意思決定の場を想定し、本番実装の意思決定をしてもらえるようなプレゼンテーション資料を作成するというものです。そしてプレゼン資料を作成し、提出すると次は他の参加者が作成したプレゼン資料を評価するというものでした。他の参加者の資料を見るまでは、自分のプレゼン資料はそれなりにわかりやすいと思っていたのですが、他の参加者のスライドを拝見して自身のスライドの未熟さに気がつきました。今回参加されている方々は、社会人歴が私よりも長い方が多かったので、実業務をやっている上で身についていることも多いと思います。また、他の参加者の方が作成したプレゼンをレビューすることで現在の私と何が違ったのかを何が足りなかったのかがわかったので良い経験になりました。具体的に私自身のプレゼンが何が良くて何が足りていなかったのか振り返って見たいと思います。よかったところ事実は簡潔にわかりやすくかけていた今回のプレゼンは、経営者向けのプレゼンなので機械学習の仕組みを解説することよりも全体的にどう動いているのかや、導入することでどれほどの粗利改善効果が見込めるのかといった、これまでの分析・予測から結果がはっきりしているものについては、丁寧にわかりやすく記述できていました。また、ヒアリングで得た内容や業務フローの変化についても事実としてすでに出ていたものなので、まとめられていました。足りていなかったところ事実としてまだ取得されていない情報が一切記載できなかった。粗利改善効果などは計算から求めることができたが、価格設定の工数削減割合や、業務の属人化が解消が期待できるなど、分析・予測から導き出せない効果が記載できなかった。工数の削減目安などは、業務を通して得られるものなのか、それともなんらかのロジックがあるのかといった解説がなかったのでそれらをどのように算出したのかは気になりました。次に足りていなかったことは、導入を検討する場合の予算の話です。これらもどのようにして計算しているのかは謎ですが、初期費用として？？？円、継続運用するのにかかる費用（サポート代）として？？？円などのような記載が足りていませんでした。確かに意思決定する場合は、予算の話があった方が導入判断は容易になるだろうと思います。また、予算に伴い今後プロジェクトを進めていくスケジュールも記載できていませんでした。ソフトがいつぐらいで完成して、そこから運用に定着するまでにどれくらいの期間が必要なのかも意思決定する際にはあった方が良いなと気がつきました。上記3つのことが足りていなかったことです。これら全てに共通して言えることは、分析・予測から求められない見込みが記載できていなかったということです。何を持ってその見込み計算を行っているのかはわかりませんが、見込みが足りていないということがわかったので、今後資料作成の際には、先輩方に頼りながら見込みの記載方法も勉強していきます。第一タームを振り返る第一タームを振り返り、「どのようにして機械学習をプロダクトに導入したら良いのか」が解決できたのか。機械学習モデルの作成する前のフロー（要件定義・PoC計画）とモデル作成後のフロー（スライド作成）では、フィードバックや他の参加者の方の方法を参考にすることで、自分自身に何が足りていなかったのかを学ぶことができました。ただ現在は、足りていないことに気がついた段階で、実践はできていない状況です。これらの足りていなかった部分を第二タームや業務で実践することで、身につけていきたいと考えています。具体的な行動としては、1. モデル作成前に目標数値を定める1. PoC計画を緻密に立てる（あくまで試行錯誤なので柔軟性は忘れてはいけない）1. プレゼン資料には、事実だけでなく、見込みも記載する明日の Feedforce Advent Calendar 2020 は、いくみんさんが「9月からダラダラと書き終わらなかったやつを納めます...」ということなので、楽しみですね。","link":"https://developer.feedforce.jp/entry/2020/12/04/083224","isoDate":"2020-12-03T23:32:24.000Z","dateMiliSeconds":1607038344000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20201201/20201201154012.png","authorName":"yaonyan"}]},"__N_SSG":true}