{"pageProps":{"member":{"name":"feedforce","bio":"Feedforce Developer Blog","avatarSrc":"/avatars/feedforce.jpg","sources":["https://developer.feedforce.jp/rss"]},"postItems":[{"title":"StreamlitとDeepLabv3を用いて物体切り抜きができるか検証したという話をした","content":"<p>こんにちは <a href=\"http://blog.hatena.ne.jp/newton800/\" class=\"hatena-id-icon\"><img src=\"https://cdn.profile-image.st-hatena.com/users/newton800/profile.png\" width=\"16\" height=\"16\" alt=\"\" class=\"hatena-id-icon\">id:newton800</a> です。</p>\n\n<p>先週末に社内勉強会 <a href=\"https://developer.feedforce.jp/archive/category/FFTT\">FFTT</a>で「StreamlitとDeepLabv3を用いて物体切り抜きができるか検証した」という話をしました。</p>\n\n<p>最近個人的に使用頻度が増加している<a href=\"https://streamlit.io/\">Streamlit</a>というフロントエンドの知識なしに実装ができるPythonのWebフレームワークの紹介をしてみたかったというのが背景です。ただし、Streamlitに触れている部分は本当に一部だけで、ほとんどはどんな感じで試行錯誤しながら試作を作っていくのかみたいな話になってしまいました。</p>\n\n<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQ6P2ZVvWxk_ZKgfgKVVQRSH1w1rZVfDojE0Khj2ZSp1Mrn9TKaz4n4-pDhHUr7s4Wg9xWbLpfJlv59/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>\n\n\n<p>年に1~2度ほど回ってくるFFTTの発表のために考えたネタなので、社内で何か活用したりする機会はないですが、自分的にはなかなか気に入っている自由研究です。</p>\n\n<p>次のFFTTではどんなネタを発表しようかな。。。</p>\n\n<p>P.S. Feedforce Developer Blogが <a href=\"http://blog.hatena.ne.jp/masutaka26/\" class=\"hatena-id-icon\"><img src=\"https://cdn.profile-image.st-hatena.com/users/masutaka26/profile.png\" width=\"16\" height=\"16\" alt=\"\" class=\"hatena-id-icon\">id:masutaka26</a>の記事ばかりになっているので、負けじと記事を書いていきたいです</p>\n","contentSnippet":"こんにちは id:newton800 です。先週末に社内勉強会 FFTTで「StreamlitとDeepLabv3を用いて物体切り抜きができるか検証した」という話をしました。最近個人的に使用頻度が増加しているStreamlitというフロントエンドの知識なしに実装ができるPythonのWebフレームワークの紹介をしてみたかったというのが背景です。ただし、Streamlitに触れている部分は本当に一部だけで、ほとんどはどんな感じで試行錯誤しながら試作を作っていくのかみたいな話になってしまいました。年に1~2度ほど回ってくるFFTTの発表のために考えたネタなので、社内で何か活用したりする機会はないですが、自分的にはなかなか気に入っている自由研究です。次のFFTTではどんなネタを発表しようかな。。。P.S. Feedforce Developer Blogが id:masutaka26の記事ばかりになっているので、負けじと記事を書いていきたいです","link":"https://developer.feedforce.jp/entry/2022/10/11/095230","isoDate":"2022-10-11T00:52:30.000Z","dateMiliSeconds":1665449550000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"feedforce"},{"title":"画像上の文字を自動削除する仕組みを作ってみた","content":"<p>こんにちは　 データサイエンティストの八百俊哉です。</p>\n\n<p><strong>今回は画像上に存在する文字を自動的に削除し、背景を補完する仕組みを作成しました。</strong>ただ、弊社のプロダクトに実装される可能性が極めて低いので、自由研究の結果としてここに残そうと思います。</p>\n\n<p>弊社のサービスはインターネット広告と深く関わりがあり、インターネット広告に関する分析を実施することが多いです。今回はインターネット広告の画像に関する調査を実施したので、その結果を共有します。</p>\n\n<h1>背景</h1>\n\n<p>インターネット広告では、商品画像上にプロモーションのロゴや行動を促すフレーズが入っている場合、不承認とされ広告表示されなくなる可能性があります。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"プロモーションが入った商品画像の例\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220330/20220330091344.png\" width=\"496\" height=\"426\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>プロモーションが入った商品画像の例</figcaption></figure></p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fsupport.ecbooster.jp%2Fja%2Farticles%2F4666716-google-merchant-center%25E3%2581%25A7%25E3%2582%25AA%25E3%2583%25BC%25E3%2583%2590%25E3%2583%25BC%25E3%2583%25AC%25E3%2582%25A4%25E3%2581%25AB%25E3%2582%2588%25E3%2582%258B%25E7%2594%25BB%25E5%2583%258F%25E4%25B8%258D%25E6%2589%25BF%25E8%25AA%258D%25E3%2581%25AB%25E5%25AF%25BE%25E5%25BF%259C%25E3%2581%2597%25E3%2581%259F%25E3%2581%2584%25E3%2581%25A8%25E3%2581%258D\" title=\"Google Merchant Centerでオーバーレイによる画像不承認に対応したいとき\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://support.ecbooster.jp/ja/articles/4666716-google-merchant-center%E3%81%A7%E3%82%AA%E3%83%BC%E3%83%90%E3%83%BC%E3%83%AC%E3%82%A4%E3%81%AB%E3%82%88%E3%82%8B%E7%94%BB%E5%83%8F%E4%B8%8D%E6%89%BF%E8%AA%8D%E3%81%AB%E5%AF%BE%E5%BF%9C%E3%81%97%E3%81%9F%E3%81%84%E3%81%A8%E3%81%8D\">support.ecbooster.jp</a></cite></p>\n\n<p>商品数が少ない場合や再度商品の写真を用意できる場合は、商品画像の差し替えが可能ですが、そうではない場合はプロモーションが入っていない画像をいちから用意するのは非常に困難です。</p>\n\n<p>一部画像においては、<a href=\"https://support.google.com/merchants/answer/9242973\">Googleが公開している画像の自動改善</a>を用いることで、プロモーションを削除することができますが、例には背景が白のもののみになっています。一方で商品画像は何らかの背景（机の上・壁紙）が写真に写り込んでいることが多く、白い背景以外にも対応したプロモーション削除方法が必要です。</p>\n\n<p>そこで今回は背景ありのプロモーションが掲載されている商品画像から自動的にプロモーションを削除する機能が必要ではないかと考え検証を行いました。</p>\n\n<h1>全体の流れ</h1>\n\n<p>サンプルとして以下の画像を用意しました。（自作なのでクオリティは低いです）</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"サンプル画像\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220330/20220330092012.png\" width=\"551\" height=\"551\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>サンプル画像</figcaption></figure></p>\n\n<p>今回はこのサンプル画像にある送料無料という文字を消すことを目的にしたいと思います。</p>\n\n<p>以下に示すフローで文字を消し、背景を補完します。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"全体のフロー\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220330/20220330100726.png\" width=\"742\" height=\"306\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>全体のフロー</figcaption></figure></p>\n\n<p>最終的には以下のような文字除去ができました。\n<figure class=\"figure-image figure-image-fotolife\" title=\"実行することで文字除去に成功した例\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220509/20220509105601.png\" width=\"1200\" height=\"568\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>実行することで文字除去に成功した例</figcaption></figure></p>\n\n<p>では実際にどのように文字を除去したのかを紹介します。</p>\n\n<h1>画像上のどこに文字があるか判定する・文字の部分だけ黒い画像を用意</h1>\n\n<p>まずはじめに画像の上のどこに文字があるかを判定します。今回はOCRを用いて文字を見つけていきます。</p>\n\n<blockquote><p>OCR（Optical Character Recognition/Reader、オーシーアール、光学的文字認識）とは、手書きや印刷された文字を、イメージスキャナやデジタルカメラによって読みとり、コンピュータが利用できるデジタルの文字コードに変換する技術です。\n<cite><a href=\"https://mediadrive.jp/technology/ocr\">https://mediadrive.jp/technology/ocr</a></cite></p></blockquote>\n\n<p>まずpythonでOCRを使用するために必要なライブリラリをインストールします。（今回はColabを用いた実行を想定しています。）</p>\n\n<pre class=\"code\" data-lang=\"\" data-unlink>!pip install --upgrade opencv-contrib-python\n!apt install tesseract-ocr libtesseract-dev tesseract-ocr-jpn\n!pip install pyocr</pre>\n\n\n<p>以下がOCRを用いて、文字を検知する関数になります。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>tools = pyocr.get_available_tools()\ntool = tools[<span class=\"synConstant\">0</span>]\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">get_box</span>(img, tool, show=<span class=\"synIdentifier\">True</span>):\n    <span class=\"synConstant\">&quot;&quot;&quot;</span>\n<span class=\"synConstant\">    img:PIL.Image.Image</span>\n<span class=\"synConstant\">    &quot;&quot;&quot;</span>\n\n    results = tool.image_to_string(\n        img,\n        lang=<span class=\"synConstant\">'jpn'</span>,\n        builder=pyocr.builders.LineBoxBuilder(tesseract_layout=<span class=\"synConstant\">6</span>)\n    )\n\n    img_np = np.array(img)\n\n    w, h = img_np.shape[<span class=\"synConstant\">0</span>], img_np.shape[<span class=\"synConstant\">1</span>]\n\n    <span class=\"synStatement\">for</span> box <span class=\"synStatement\">in</span> results:\n        <span class=\"synStatement\">if</span> box.content == <span class=\"synConstant\">' '</span> <span class=\"synStatement\">or</span> box.content == <span class=\"synConstant\">''</span>:\n            <span class=\"synStatement\">continue</span>\n        <span class=\"synStatement\">if</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">1</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">0</span>] == w) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">1</span>] == h):\n            <span class=\"synStatement\">continue</span>\n        cv2.rectangle(img_np, box.position[<span class=\"synConstant\">0</span>], box.position[<span class=\"synConstant\">1</span>], (<span class=\"synConstant\">0</span>, <span class=\"synConstant\">255</span>, <span class=\"synConstant\">0</span>), <span class=\"synConstant\">1</span>)\n    <span class=\"synStatement\">if</span> show:\n        display(Image.fromarray(img_np))\n    <span class=\"synStatement\">return</span> img_np, results\n</pre>\n\n\n<p>しかし、ただ画像を上記の関数に渡すだけでは文字を認識できる精度が低いという課題がありました。そこで以下のようにすることで文字認識の精度向上を行いました。</p>\n\n<ol>\n<li>画像を拡大する</li>\n<li>拡大した画像を5×5に分割する</li>\n<li>分割後の画像をそれぞれOCRにかける</li>\n<li>分割前の拡大した画像をOCRにかける</li>\n<li>3,4で文字と判定され部分（OR）を文字がある場所とする</li>\n</ol>\n\n\n<p>図で示すと以下のようになっています。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"文字検知の精度向上のための取り組み\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220405/20220405122649.png\" width=\"932\" height=\"403\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>文字検知の精度向上のための取り組み</figcaption></figure></p>\n\n<p>実際には以下のようにして求めます。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synStatement\">def</span> <span class=\"synIdentifier\">split_images</span>(img, h_split, w_split):\n    <span class=\"synConstant\">&quot;&quot;&quot;</span>\n<span class=\"synConstant\">    画像を分割し、分割後の画像と元画像における座標を返す</span>\n<span class=\"synConstant\">    img:numpy</span>\n<span class=\"synConstant\">    &quot;&quot;&quot;</span>\n\n    h,w = img.shape[<span class=\"synConstant\">0</span>],img.shape[<span class=\"synConstant\">1</span>]\n    images = []\n\n    new_h = <span class=\"synIdentifier\">int</span>(h / h_split)\n    new_w = <span class=\"synIdentifier\">int</span>(w / w_split)\n\n    start_coordinates = []\n\n    <span class=\"synStatement\">for</span> _h <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">range</span>(h_split):\n        h_start = _h * new_h\n        h_end = h_start + new_h\n\n        <span class=\"synStatement\">for</span> _w <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">range</span>(w_split):\n            w_start = _w * new_w\n            w_end = w_start + new_w\n            images.append(img[h_start:h_end, w_start:w_end, :])\n\n            coordinate = {}\n            coordinate[<span class=\"synConstant\">'h_start'</span>] = h_start\n            coordinate[<span class=\"synConstant\">'h_end'</span>] = h_end\n            coordinate[<span class=\"synConstant\">'w_start'</span>] = w_start\n            coordinate[<span class=\"synConstant\">'w_end'</span>] = w_end\n\n            start_coordinates.append(coordinate)\n    \n    <span class=\"synStatement\">return</span> images, start_coordinates\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">get_splitImages2originalPositins</span>(images, img_resize, start_coordinates, show=<span class=\"synIdentifier\">True</span>):\n    <span class=\"synConstant\">&quot;&quot;&quot;</span>\n<span class=\"synConstant\">    分割後の画像それぞれのどこに文字があるかどうかを識別する</span>\n<span class=\"synConstant\">    また、それらの結果を元の座標系に戻してpositionsとして返す</span>\n<span class=\"synConstant\">    images:list</span>\n<span class=\"synConstant\">    img_resize:numpy</span>\n<span class=\"synConstant\">    start_coordinates:list</span>\n<span class=\"synConstant\">    &quot;&quot;&quot;</span>\n    positions = []\n    <span class=\"synStatement\">for</span> i, (img, coordinate)  <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">enumerate</span>(<span class=\"synIdentifier\">zip</span>(images, start_coordinates)):\n\n        bb,results = get_box(Image.fromarray(img), tool, show=<span class=\"synIdentifier\">False</span>)\n        w,h = img.shape[<span class=\"synConstant\">0</span>],img.shape[<span class=\"synConstant\">1</span>]\n\n        <span class=\"synStatement\">for</span> box <span class=\"synStatement\">in</span> results:\n            <span class=\"synStatement\">if</span> box.content == <span class=\"synConstant\">' '</span> <span class=\"synStatement\">or</span> box.content == <span class=\"synConstant\">''</span>:\n                <span class=\"synStatement\">continue</span>\n            <span class=\"synStatement\">if</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">1</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">0</span>] == w) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">1</span>] == h):\n                <span class=\"synStatement\">continue</span>\n            \n            position = [[p[<span class=\"synConstant\">0</span>],p[<span class=\"synConstant\">1</span>]] <span class=\"synStatement\">for</span> p <span class=\"synStatement\">in</span> box.position]\n            position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">0</span>] += coordinate[<span class=\"synConstant\">'w_start'</span>]\n            position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">1</span>] += coordinate[<span class=\"synConstant\">'h_start'</span>]\n            position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">0</span>] += coordinate[<span class=\"synConstant\">'w_start'</span>]\n            position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">1</span>] += coordinate[<span class=\"synConstant\">'h_start'</span>]\n            cv2.rectangle(img_resize, position[<span class=\"synConstant\">0</span>], position[<span class=\"synConstant\">1</span>], (<span class=\"synConstant\">0</span>, <span class=\"synConstant\">255</span>, <span class=\"synConstant\">0</span>), <span class=\"synConstant\">1</span>)\n            positions.append(position)\n    <span class=\"synStatement\">if</span> show:\n        display(Image.fromarray(img_resize))\n    <span class=\"synStatement\">return</span> positions\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">get_mask</span>(img, positions):\n    mask = np.zeros_like(img)\n    <span class=\"synStatement\">for</span> p <span class=\"synStatement\">in</span> positions:\n        cv2.rectangle(mask, p[<span class=\"synConstant\">0</span>], p[<span class=\"synConstant\">1</span>], (<span class=\"synConstant\">255</span>, <span class=\"synConstant\">255</span>, <span class=\"synConstant\">255</span>), thickness=-<span class=\"synConstant\">1</span>)\n    <span class=\"synStatement\">return</span> mask\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">results2positions</span>(results):\n    positions = []\n    <span class=\"synStatement\">for</span> box <span class=\"synStatement\">in</span> results:\n        <span class=\"synStatement\">if</span> box.content == <span class=\"synConstant\">' '</span> <span class=\"synStatement\">or</span> box.content == <span class=\"synConstant\">''</span>:\n            <span class=\"synStatement\">continue</span>\n        <span class=\"synStatement\">if</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">0</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">0</span>][<span class=\"synConstant\">1</span>] == <span class=\"synConstant\">0</span>) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">0</span>] == w) <span class=\"synStatement\">and</span> (box.position[<span class=\"synConstant\">1</span>][<span class=\"synConstant\">1</span>] == h):\n            <span class=\"synStatement\">continue</span>\n        positions.append([box.position[<span class=\"synConstant\">0</span>], box.position[<span class=\"synConstant\">1</span>]])\n    <span class=\"synStatement\">return</span> positions\n\n<span class=\"synComment\"># 画像の拡大</span>\nimg_resize = cv2.resize(img_np, (<span class=\"synIdentifier\">int</span>(img_np.shape[<span class=\"synConstant\">0</span>] * <span class=\"synConstant\">4</span>), <span class=\"synIdentifier\">int</span>(img_np.shape[<span class=\"synConstant\">1</span>] * <span class=\"synConstant\">4</span>)), interpolation=cv2.INTER_CUBIC)\n\n<span class=\"synComment\"># 画像を分割する</span>\nimages, start_coordinates = split_images(np.array(img_resize), <span class=\"synConstant\">5</span>, <span class=\"synConstant\">5</span>)\n<span class=\"synComment\"># 分割画像のそれぞれをOCRに入れ、どこに文字が有るか判定する</span>\n<span class=\"synComment\"># また、このときに返ってくるpositionsには、分割前の座標系でどこに文字があったのかが示されている</span>\npositions = get_splitImages2originalPositins(images, img_resize, start_coordinates, show=<span class=\"synIdentifier\">False</span>)\n<span class=\"synComment\"># 分割画像から特定された文字の位置がマスクになるようにする</span>\nmask_resize = get_mask(img_resize, positions)\n<span class=\"synComment\"># 元の画像サイズに戻す</span>\nmask_resize = cv2.resize(mask_resize, (img_np.shape[<span class=\"synConstant\">0</span>],img_np.shape[<span class=\"synConstant\">1</span>]))\n\n<span class=\"synComment\"># リサイズ画像を分割せずに、どこに文字が有るかを求める</span>\nimg_np, results = get_box(img1, tool, show=<span class=\"synIdentifier\">False</span>)\npositions = results2positions(results)\n<span class=\"synComment\"># リサイズ画像から特定された文字の位置がマスクになるようにする</span>\noriginal_mask = get_mask(img_np, positions)\n\n<span class=\"synComment\"># mask_resize（分割画像）とoriginal_mask（リサイズ画像）のORを求める</span>\nbitwise_or = cv2.bitwise_or(original_mask[:,:,<span class=\"synConstant\">0</span>], mask_resize[:,:,<span class=\"synConstant\">0</span>])\n</pre>\n\n\n<p>上記を実行することで、画像上に文字があるとされた部分の画素だけが255のマスク画像がbitwise_orに入っていることになります。</p>\n\n<h1>画像上の黒い部分を周りの情報から補完</h1>\n\n<p>次に先程作成したマスク画像と元画像を用いることで、文字の部分を周りの状況から補完していきます。</p>\n\n<p><strong>今回はOpneCVのInpaintingを用いて、文字の除去を行っていきます。</strong>\n<iframe src=\"https://hatenablog-parts.com/embed?url=http%3A%2F%2Flabs.eecs.tottori-u.ac.jp%2Fsd%2FMember%2Foyamada%2FOpenCV%2Fhtml%2Fpy_tutorials%2Fpy_photo%2Fpy_inpainting%2Fpy_inpainting.html\" title=\"画像のInpainting — OpenCV-Python Tutorials 1 documentation\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_photo/py_inpainting/py_inpainting.html\">labs.eecs.tottori-u.ac.jp</a></cite></p>\n\n<p>実際に以下のようにして複数のパラメータでInpaintingを実行し、結果を確認できるようにしました。(参考：<a href=\"https://data-analysis-stats.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/opencv%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AEinpainting%E3%81%AE%E8%A7%A3%E8%AA%AC/\">OpenCVで画像のInpaintingの解説</a>)</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synStatement\">def</span> <span class=\"synIdentifier\">get_inpainting</span>(img, mask):\n    img = np.array(img)\n    mask = np.array(mask)\n\n    <span class=\"synStatement\">if</span> mask.ndim == <span class=\"synConstant\">3</span>:\n        mask = mask[:,:,<span class=\"synConstant\">0</span>]\n    \n    dst11 = cv2.inpaint(img, mask , <span class=\"synConstant\">0</span>, cv2.INPAINT_TELEA)\n    dst12 = cv2.inpaint(img, mask , <span class=\"synConstant\">3</span>, cv2.INPAINT_TELEA)\n    dst13 = cv2.inpaint(img, mask , <span class=\"synConstant\">10</span>, cv2.INPAINT_TELEA)\n    dst21 = cv2.inpaint(img, mask , <span class=\"synConstant\">0</span>, cv2.INPAINT_NS)\n    dst22 = cv2.inpaint(img, mask , <span class=\"synConstant\">3</span>, cv2.INPAINT_NS)\n    dst23 = cv2.inpaint(img, mask , <span class=\"synConstant\">10</span>, cv2.INPAINT_NS)\n\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_TELEA, radius=0) : dst11&quot;</span>)\n    display(Image.fromarray(dst11))\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_TELEA, radius=3) : dst12&quot;</span>)\n    display(Image.fromarray(dst11))\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_TELEA, radius=10) : dst13&quot;</span>)\n    display(Image.fromarray(dst11))\n\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_NS, radius=0) : dst21&quot;</span>)\n    display(Image.fromarray(dst21))\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_NS, radius=3) : dst22&quot;</span>)\n    display(Image.fromarray(dst22))\n    <span class=\"synIdentifier\">print</span>(<span class=\"synConstant\">&quot;</span><span class=\"synSpecial\">\\n</span><span class=\"synConstant\">Output (INPAINT_NS, radius=10) : dst23&quot;</span>)\n    display(Image.fromarray(dst23))\n\n    <span class=\"synStatement\">return</span> dst11,dst12,dst13,dst21,dst22,dst23\n\ndst11,dst12,dst13,dst21,dst22,dst23 = get_inpainting(img_np, bitwise_or)\n</pre>\n\n\n<p>こちらを実行すると以下の出力が得られます。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"結果の出力例\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220509/20220509103312.png\" width=\"855\" height=\"1200\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>結果の出力例</figcaption></figure></p>\n\n<p>出力画像を確認するとしっかりと文字が消えており、文字があった部分は背景色と同じ色が補完されていることがわかります。</p>\n\n<h1>まとめ</h1>\n\n<p>今回は画像上に存在する文字を自動的に削除し、背景を補完する仕組みの紹介を行いました。</p>\n\n<p>はじめに画像上の文字を検知し、その検知結果をもとにマスク画像を作成します。そのマスク画像と元画像をInpaintingに入力することで、画像の補完を行いました。</p>\n\n<p><strong>特に文字を検知する部分で、文字検知の取りこぼしが多く見られたので画像を拡大・分割するなどの工夫を実施したところがポイントです。ただし、現状の文字検知の方法でもかなり取りこぼしが多い状態なので、実践で使用する場合にはもう少し工夫が必要になります。</strong></p>\n\n<p>大学の頃は画像系の卒論を書いたこともあり、とても楽しめながら作成することができました。\n最後までお付き合いいただき、ありがとうございます。</p>\n","contentSnippet":"こんにちは　 データサイエンティストの八百俊哉です。今回は画像上に存在する文字を自動的に削除し、背景を補完する仕組みを作成しました。ただ、弊社のプロダクトに実装される可能性が極めて低いので、自由研究の結果としてここに残そうと思います。弊社のサービスはインターネット広告と深く関わりがあり、インターネット広告に関する分析を実施することが多いです。今回はインターネット広告の画像に関する調査を実施したので、その結果を共有します。背景インターネット広告では、商品画像上にプロモーションのロゴや行動を促すフレーズが入っている場合、不承認とされ広告表示されなくなる可能性があります。プロモーションが入った商品画像の例support.ecbooster.jp商品数が少ない場合や再度商品の写真を用意できる場合は、商品画像の差し替えが可能ですが、そうではない場合はプロモーションが入っていない画像をいちから用意するのは非常に困難です。一部画像においては、Googleが公開している画像の自動改善を用いることで、プロモーションを削除することができますが、例には背景が白のもののみになっています。一方で商品画像は何らかの背景（机の上・壁紙）が写真に写り込んでいることが多く、白い背景以外にも対応したプロモーション削除方法が必要です。そこで今回は背景ありのプロモーションが掲載されている商品画像から自動的にプロモーションを削除する機能が必要ではないかと考え検証を行いました。全体の流れサンプルとして以下の画像を用意しました。（自作なのでクオリティは低いです）サンプル画像今回はこのサンプル画像にある送料無料という文字を消すことを目的にしたいと思います。以下に示すフローで文字を消し、背景を補完します。全体のフロー最終的には以下のような文字除去ができました。実行することで文字除去に成功した例では実際にどのように文字を除去したのかを紹介します。画像上のどこに文字があるか判定する・文字の部分だけ黒い画像を用意まずはじめに画像の上のどこに文字があるかを判定します。今回はOCRを用いて文字を見つけていきます。OCR（Optical Character Recognition/Reader、オーシーアール、光学的文字認識）とは、手書きや印刷された文字を、イメージスキャナやデジタルカメラによって読みとり、コンピュータが利用できるデジタルの文字コードに変換する技術です。https://mediadrive.jp/technology/ocrまずpythonでOCRを使用するために必要なライブリラリをインストールします。（今回はColabを用いた実行を想定しています。）!pip install --upgrade opencv-contrib-python!apt install tesseract-ocr libtesseract-dev tesseract-ocr-jpn!pip install pyocr以下がOCRを用いて、文字を検知する関数になります。tools = pyocr.get_available_tools()tool = tools[0]def get_box(img, tool, show=True):    \"\"\"    img:PIL.Image.Image    \"\"\"    results = tool.image_to_string(        img,        lang='jpn',        builder=pyocr.builders.LineBoxBuilder(tesseract_layout=6)    )    img_np = np.array(img)    w, h = img_np.shape[0], img_np.shape[1]    for box in results:        if box.content == ' ' or box.content == '':            continue        if (box.position[0][0] == 0) and (box.position[0][1] == 0) and (box.position[1][0] == w) and (box.position[1][1] == h):            continue        cv2.rectangle(img_np, box.position[0], box.position[1], (0, 255, 0), 1)    if show:        display(Image.fromarray(img_np))    return img_np, resultsしかし、ただ画像を上記の関数に渡すだけでは文字を認識できる精度が低いという課題がありました。そこで以下のようにすることで文字認識の精度向上を行いました。画像を拡大する拡大した画像を5×5に分割する分割後の画像をそれぞれOCRにかける分割前の拡大した画像をOCRにかける3,4で文字と判定され部分（OR）を文字がある場所とする図で示すと以下のようになっています。文字検知の精度向上のための取り組み実際には以下のようにして求めます。def split_images(img, h_split, w_split):    \"\"\"    画像を分割し、分割後の画像と元画像における座標を返す    img:numpy    \"\"\"    h,w = img.shape[0],img.shape[1]    images = []    new_h = int(h / h_split)    new_w = int(w / w_split)    start_coordinates = []    for _h in range(h_split):        h_start = _h * new_h        h_end = h_start + new_h        for _w in range(w_split):            w_start = _w * new_w            w_end = w_start + new_w            images.append(img[h_start:h_end, w_start:w_end, :])            coordinate = {}            coordinate['h_start'] = h_start            coordinate['h_end'] = h_end            coordinate['w_start'] = w_start            coordinate['w_end'] = w_end            start_coordinates.append(coordinate)        return images, start_coordinatesdef get_splitImages2originalPositins(images, img_resize, start_coordinates, show=True):    \"\"\"    分割後の画像それぞれのどこに文字があるかどうかを識別する    また、それらの結果を元の座標系に戻してpositionsとして返す    images:list    img_resize:numpy    start_coordinates:list    \"\"\"    positions = []    for i, (img, coordinate)  in enumerate(zip(images, start_coordinates)):        bb,results = get_box(Image.fromarray(img), tool, show=False)        w,h = img.shape[0],img.shape[1]        for box in results:            if box.content == ' ' or box.content == '':                continue            if (box.position[0][0] == 0) and (box.position[0][1] == 0) and (box.position[1][0] == w) and (box.position[1][1] == h):                continue                        position = [[p[0],p[1]] for p in box.position]            position[0][0] += coordinate['w_start']            position[0][1] += coordinate['h_start']            position[1][0] += coordinate['w_start']            position[1][1] += coordinate['h_start']            cv2.rectangle(img_resize, position[0], position[1], (0, 255, 0), 1)            positions.append(position)    if show:        display(Image.fromarray(img_resize))    return positionsdef get_mask(img, positions):    mask = np.zeros_like(img)    for p in positions:        cv2.rectangle(mask, p[0], p[1], (255, 255, 255), thickness=-1)    return maskdef results2positions(results):    positions = []    for box in results:        if box.content == ' ' or box.content == '':            continue        if (box.position[0][0] == 0) and (box.position[0][1] == 0) and (box.position[1][0] == w) and (box.position[1][1] == h):            continue        positions.append([box.position[0], box.position[1]])    return positions# 画像の拡大img_resize = cv2.resize(img_np, (int(img_np.shape[0] * 4), int(img_np.shape[1] * 4)), interpolation=cv2.INTER_CUBIC)# 画像を分割するimages, start_coordinates = split_images(np.array(img_resize), 5, 5)# 分割画像のそれぞれをOCRに入れ、どこに文字が有るか判定する# また、このときに返ってくるpositionsには、分割前の座標系でどこに文字があったのかが示されているpositions = get_splitImages2originalPositins(images, img_resize, start_coordinates, show=False)# 分割画像から特定された文字の位置がマスクになるようにするmask_resize = get_mask(img_resize, positions)# 元の画像サイズに戻すmask_resize = cv2.resize(mask_resize, (img_np.shape[0],img_np.shape[1]))# リサイズ画像を分割せずに、どこに文字が有るかを求めるimg_np, results = get_box(img1, tool, show=False)positions = results2positions(results)# リサイズ画像から特定された文字の位置がマスクになるようにするoriginal_mask = get_mask(img_np, positions)# mask_resize（分割画像）とoriginal_mask（リサイズ画像）のORを求めるbitwise_or = cv2.bitwise_or(original_mask[:,:,0], mask_resize[:,:,0])上記を実行することで、画像上に文字があるとされた部分の画素だけが255のマスク画像がbitwise_orに入っていることになります。画像上の黒い部分を周りの情報から補完次に先程作成したマスク画像と元画像を用いることで、文字の部分を周りの状況から補完していきます。今回はOpneCVのInpaintingを用いて、文字の除去を行っていきます。labs.eecs.tottori-u.ac.jp実際に以下のようにして複数のパラメータでInpaintingを実行し、結果を確認できるようにしました。(参考：OpenCVで画像のInpaintingの解説)def get_inpainting(img, mask):    img = np.array(img)    mask = np.array(mask)    if mask.ndim == 3:        mask = mask[:,:,0]        dst11 = cv2.inpaint(img, mask , 0, cv2.INPAINT_TELEA)    dst12 = cv2.inpaint(img, mask , 3, cv2.INPAINT_TELEA)    dst13 = cv2.inpaint(img, mask , 10, cv2.INPAINT_TELEA)    dst21 = cv2.inpaint(img, mask , 0, cv2.INPAINT_NS)    dst22 = cv2.inpaint(img, mask , 3, cv2.INPAINT_NS)    dst23 = cv2.inpaint(img, mask , 10, cv2.INPAINT_NS)    print(\"\\nOutput (INPAINT_TELEA, radius=0) : dst11\")    display(Image.fromarray(dst11))    print(\"\\nOutput (INPAINT_TELEA, radius=3) : dst12\")    display(Image.fromarray(dst11))    print(\"\\nOutput (INPAINT_TELEA, radius=10) : dst13\")    display(Image.fromarray(dst11))    print(\"\\nOutput (INPAINT_NS, radius=0) : dst21\")    display(Image.fromarray(dst21))    print(\"\\nOutput (INPAINT_NS, radius=3) : dst22\")    display(Image.fromarray(dst22))    print(\"\\nOutput (INPAINT_NS, radius=10) : dst23\")    display(Image.fromarray(dst23))    return dst11,dst12,dst13,dst21,dst22,dst23dst11,dst12,dst13,dst21,dst22,dst23 = get_inpainting(img_np, bitwise_or)こちらを実行すると以下の出力が得られます。結果の出力例出力画像を確認するとしっかりと文字が消えており、文字があった部分は背景色と同じ色が補完されていることがわかります。まとめ今回は画像上に存在する文字を自動的に削除し、背景を補完する仕組みの紹介を行いました。はじめに画像上の文字を検知し、その検知結果をもとにマスク画像を作成します。そのマスク画像と元画像をInpaintingに入力することで、画像の補完を行いました。特に文字を検知する部分で、文字検知の取りこぼしが多く見られたので画像を拡大・分割するなどの工夫を実施したところがポイントです。ただし、現状の文字検知の方法でもかなり取りこぼしが多い状態なので、実践で使用する場合にはもう少し工夫が必要になります。大学の頃は画像系の卒論を書いたこともあり、とても楽しめながら作成することができました。最後までお付き合いいただき、ありがとうございます。","link":"https://developer.feedforce.jp/entry/2022/05/09/114054","isoDate":"2022-05-09T02:40:54.000Z","dateMiliSeconds":1652064054000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"feedforce"},{"title":"ECサイトのタイトルと説明文から取り扱いカテゴリの予測を行った","content":"<p>こんにちは　 データサイエンティストの八百俊哉です。最近は家でカクテルを作ることにはまっています。</p>\n\n<p><strong>今回はEC BoosterのクライントのECサイトのタイトルと説明文を用いることで、どのカテゴリの商品を扱っているのか予測する仕組みを作成しました</strong>ので、その方法を紹介します。</p>\n\n<p>弊社のサービス<a href=\"https://ecbooster.jp/\">EC Booster</a>は、Google ショッピング広告の自動運用による自社EC自動集客サービスです。主要ECシステムと連携することで、Google の検索結果画面に画像付きで自社商品を訴求することが可能となります。</p>\n\n<h1>分析背景</h1>\n\n<p>EC Boosterを利用いただくことで、Google ショッピング広告を自動運用することができます。\nGoogleショッピング広告では広告出稿する際に、<strong>Googleが自動的にそれぞれの商品にgoogle product categoryというカテゴリ情報を割り当てます。</strong></p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fsupport.google.com%2Fmerchants%2Fanswer%2F6324436%3Fhl%3Dja\" title=\"Google 商品カテゴリ [google_product_category] - Google Merchant Center ヘルプ\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://support.google.com/merchants/answer/6324436?hl=ja\">support.google.com</a></cite></p>\n\n<p>しかしながら、<strong>Googleが自動的に割り振った商品ごとのカテゴリ情報はEC Booster側からは確認することができません。</strong></p>\n\n<blockquote><p>Google's category of the item (see Google product taxonomy). When querying products, this field will contain the user provided value. There is currently no way to get back the auto assigned google product categories through the API.\n<cite><a href=\"https://developers.google.com/shopping-content/reference/rest/v2.1/products?hl=en#Product.FIELDS.google_product_category\">https://developers.google.com/shopping-content/reference/rest/v2.1/products?hl=en#Product.FIELDS.google_product_category</a></cite></p></blockquote>\n\n<p>商品ごとに割り振られたカテゴリがわからないと、それぞれのクライントがどのような商品を扱っているのかわからなくなります。</p>\n\n<p>その結果、<strong>クライアントの実績を取扱商品カテゴリごとに集計したり、扱っているカテゴリごとのサポートを行ったりすることができなくなってしまいます。</strong></p>\n\n<p>そこで今回は<strong>クライントごとに取扱商品カテゴリをEC Booster側で予測する</strong>ことで、上に挙げたような課題を解決するような仕組みを作成しました。</p>\n\n<h1>学習・予測に必要なデータの収集</h1>\n\n<p>EC Boosterは現在有料プラン約300のクライントとフリープラン約2000のクライントにご利用いただいております。（<a href=\"https://ssl4.eir-parts.net/doc/7068/tdnet/2065834/00.pdf\">2022年5月第2四半期 決算説明資料より</a>）</p>\n\n<p><strong>学習データ生成のために、有料プランユーザーを中心に約470のクライントに対して手動でカテゴリを割り振るという作業を実施しました。</strong>また、このデータの作成はそれぞれのクライントのホームページを目視で確認し、適切なカテゴリを割り振りました。</p>\n\n<p>次に学習・予測に必要な入力データの収集を行いました。<strong>今回はそれぞれのクライントのタイトルと説明文を用いて学習をします。</strong>\n<strong>そのため以下に示すようにそれぞれのホームページからtitleとdescriptionのスクレイピングを行いました。\n</strong>\n<figure class=\"figure-image figure-image-fotolife\" title=\"タイトル(title)、説明文(description)の例\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220215/20220215113752.png\" alt=\"f:id:newton800:20220215113752p:plain\" width=\"1200\" height=\"575\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>タイトル(title)、説明文(description)の例</figcaption></figure></p>\n\n<p>それぞれのtitleとdescriptionが収集できると以下のようなデータになります。</p>\n\n<table>\n<thead>\n<tr>\n<th>  shop_id  </th>\n<th>  手入力カテゴリ  </th>\n<th> title</th>\n<th> description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>  A  </td>\n<td>  カテゴリA  </td>\n<td> レトロな服を買うならA  </td>\n<td> レトロな服を買うならAで決まり!!お問い合わせは...  </td>\n</tr>\n<tr>\n<td>  B  </td>\n<td>  カテゴリB  </td>\n<td> 高級食材のB </td>\n<td> 高級食材を買うならBが安い!!  </td>\n</tr>\n<tr>\n<td>  C  </td>\n<td>  カテゴリC  </td>\n<td> 車のパーツはC </td>\n<td> パーツの取付も行います!!  </td>\n</tr>\n</tbody>\n</table>\n\n\n<p>すべてのtitleとdescriptionを収集すると、手動でカテゴリを定めたデータ数と等しいデータ数になるので約470の学習データが集まったことになります。</p>\n\n<h1>学習</h1>\n\n<p>今回はBERTを用いて学習・予測を行いました。</p>\n\n<p><a href=\"https://arxiv.org/abs/1810.04805\">BERT（Bidirectional Encoder Representations from Transformers）</a>とは2018年にGoogleから発表された自然言語モデルです。</p>\n\n<p>また、私自身BERTに精通しているわけではないので、今回は勉強も兼ねて「<a href=\"https://www.amazon.co.jp/dp/B098J9M4PP/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1\">BERTによる自然言語処理入門 ―Transformersを使った実践プログラミング</a>」を参考に学習を実施しました。</p>\n\n<p>第6章にBERTによる文章分類というものがあるので、そちらを一部書き換えて使用しました。</p>\n\n<h1>工夫したポイント</h1>\n\n<p>モデルやpre-trained modelは本で紹介されている通りのものを使用しましたが、<strong>18カテゴリへの分類問題で正解率が62%前後しか確保できていないため、この精度では運用できない</strong>と判断し、以下のような工夫を行いました。</p>\n\n<h2>工夫したポイント①　2種類の予測モデルを用いる</h2>\n\n<p>精度改善のために元々1つの予測モデルで学習・予測行っていたものを、2つの予測モデルで行うように変更しました。それぞれの予測モデルは以下のような役割を担っています。</p>\n\n<table>\n<thead>\n<tr>\n<th>  予測モデル  </th>\n<th>  役割  </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>  予測モデル1  </td>\n<td>  データ量が多いカテゴリを予測するモデル。簡単な予測問題を間違えない。また、データ数が少ないカテゴリはまとめて「その他」と予測する。  </td>\n</tr>\n<tr>\n<td>  予測モデル2  </td>\n<td>  予測モデル1で「その他」と予測されたデータを、それぞれのカテゴリに割り振る予測モデル。データ数が少なく難しいカテゴリに対して予測を行うので正解率は低い。  </td>\n</tr>\n</tbody>\n</table>\n\n\n<p>手動で割り振られた約470データのカテゴリは以下のようになっており、<strong>学習に使用できるデータ量がカテゴリごとにばらつきが大きいことがわかります。</strong> <strong>またデータ量が少ないカテゴリは、多くの場合低い精度を示していました。</strong></p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220216/20220216105400.png\" alt=\"f:id:newton800:20220216105400p:plain\" width=\"601\" height=\"372\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p><br></p>\n\n<p>予測モデル1では、データ量が少ないカテゴリは「その他」とまとめてしまい学習を行いました。予測モデル2では「その他」をより細分化するための学習を行いました。</p>\n\n<p><br></p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220216/20220216111039.png\" alt=\"f:id:newton800:20220216111039p:plain\" width=\"951\" height=\"393\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p><br>\nしかしながら、<strong>この方法だけではデータ量が少ないカテゴリを学習している予測モデル2はこれまで通り低い精度でした</strong>ので、以下に示す工夫ポイント2を導入することにしました。</p>\n\n<h2>工夫したポイント②　 自信のない予測は、後で人手で確認するようなフローにする</h2>\n\n<p>これまでは出力されたカテゴリの全てを信頼するようにしていましたが、<strong>一定の信頼値以下の予測に関しては後で人手で確認するというフローを導入</strong>しました。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220216/20220216111955.png\" alt=\"f:id:newton800:20220216111955p:plain\" width=\"951\" height=\"441\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>そうすることで、<strong>予測結果の信頼値が高いものは予測結果がそのまま採用され、信頼値が低いものは人が後で確認する</strong>という流れになりました。また人が後で確認してカテゴリを定めたデータは、学習モデルの精度向上のための再学習に使用することができます。</p>\n\n<h2>工夫したポイントを導入した結果</h2>\n\n<p>元々正解率が62%だったものが上に示したような<strong>工夫ポイントを導入することで正解率は91%まで向上しました。</strong>また、そのうち2割ほどが人手での再確認を要するものとなりました。</p>\n\n<h1>さいごに</h1>\n\n<p>工夫したポイント②の「自信のない予測は、後で人手で確認するようなフローにする」はちょっとせこいのではないか？と思われる方もいるかもしれません。しかし、残りの全てのクライントに手動で割り振ることと比較すると業務量としてはかなり削減できることになっています。（8割軽減）</p>\n\n<p><strong>「機械にできる簡単なタスクは機械に任せてしまって、人間は本当に考えないといけないような業務に集中できる」ような分析やサービス作りに携わっていきたいなぁと改めて感じた業務でした。</strong></p>\n\n<p>最後までありがとうございました。</p>\n\n<p>P.S. 工夫ポイント1はlossの設計をうまくやれば不要だったりするのかなぁなんて思ったり...</p>\n","contentSnippet":"こんにちは　 データサイエンティストの八百俊哉です。最近は家でカクテルを作ることにはまっています。今回はEC BoosterのクライントのECサイトのタイトルと説明文を用いることで、どのカテゴリの商品を扱っているのか予測する仕組みを作成しましたので、その方法を紹介します。弊社のサービスEC Boosterは、Google ショッピング広告の自動運用による自社EC自動集客サービスです。主要ECシステムと連携することで、Google の検索結果画面に画像付きで自社商品を訴求することが可能となります。分析背景EC Boosterを利用いただくことで、Google ショッピング広告を自動運用することができます。Googleショッピング広告では広告出稿する際に、Googleが自動的にそれぞれの商品にgoogle product categoryというカテゴリ情報を割り当てます。support.google.comしかしながら、Googleが自動的に割り振った商品ごとのカテゴリ情報はEC Booster側からは確認することができません。Google's category of the item (see Google product taxonomy). When querying products, this field will contain the user provided value. There is currently no way to get back the auto assigned google product categories through the API.https://developers.google.com/shopping-content/reference/rest/v2.1/products?hl=en#Product.FIELDS.google_product_category商品ごとに割り振られたカテゴリがわからないと、それぞれのクライントがどのような商品を扱っているのかわからなくなります。その結果、クライアントの実績を取扱商品カテゴリごとに集計したり、扱っているカテゴリごとのサポートを行ったりすることができなくなってしまいます。そこで今回はクライントごとに取扱商品カテゴリをEC Booster側で予測することで、上に挙げたような課題を解決するような仕組みを作成しました。学習・予測に必要なデータの収集EC Boosterは現在有料プラン約300のクライントとフリープラン約2000のクライントにご利用いただいております。（2022年5月第2四半期 決算説明資料より）学習データ生成のために、有料プランユーザーを中心に約470のクライントに対して手動でカテゴリを割り振るという作業を実施しました。また、このデータの作成はそれぞれのクライントのホームページを目視で確認し、適切なカテゴリを割り振りました。次に学習・予測に必要な入力データの収集を行いました。今回はそれぞれのクライントのタイトルと説明文を用いて学習をします。そのため以下に示すようにそれぞれのホームページからtitleとdescriptionのスクレイピングを行いました。タイトル(title)、説明文(description)の例それぞれのtitleとdescriptionが収集できると以下のようなデータになります。  shop_id    手入力カテゴリ   title description  A    カテゴリA   レトロな服を買うならA   レトロな服を買うならAで決まり!!お問い合わせは...    B    カテゴリB   高級食材のB  高級食材を買うならBが安い!!    C    カテゴリC   車のパーツはC  パーツの取付も行います!!  すべてのtitleとdescriptionを収集すると、手動でカテゴリを定めたデータ数と等しいデータ数になるので約470の学習データが集まったことになります。学習今回はBERTを用いて学習・予測を行いました。BERT（Bidirectional Encoder Representations from Transformers）とは2018年にGoogleから発表された自然言語モデルです。また、私自身BERTに精通しているわけではないので、今回は勉強も兼ねて「BERTによる自然言語処理入門 ―Transformersを使った実践プログラミング」を参考に学習を実施しました。第6章にBERTによる文章分類というものがあるので、そちらを一部書き換えて使用しました。工夫したポイントモデルやpre-trained modelは本で紹介されている通りのものを使用しましたが、18カテゴリへの分類問題で正解率が62%前後しか確保できていないため、この精度では運用できないと判断し、以下のような工夫を行いました。工夫したポイント①　2種類の予測モデルを用いる精度改善のために元々1つの予測モデルで学習・予測行っていたものを、2つの予測モデルで行うように変更しました。それぞれの予測モデルは以下のような役割を担っています。  予測モデル    役割    予測モデル1    データ量が多いカテゴリを予測するモデル。簡単な予測問題を間違えない。また、データ数が少ないカテゴリはまとめて「その他」と予測する。    予測モデル2    予測モデル1で「その他」と予測されたデータを、それぞれのカテゴリに割り振る予測モデル。データ数が少なく難しいカテゴリに対して予測を行うので正解率は低い。  手動で割り振られた約470データのカテゴリは以下のようになっており、学習に使用できるデータ量がカテゴリごとにばらつきが大きいことがわかります。 またデータ量が少ないカテゴリは、多くの場合低い精度を示していました。予測モデル1では、データ量が少ないカテゴリは「その他」とまとめてしまい学習を行いました。予測モデル2では「その他」をより細分化するための学習を行いました。しかしながら、この方法だけではデータ量が少ないカテゴリを学習している予測モデル2はこれまで通り低い精度でしたので、以下に示す工夫ポイント2を導入することにしました。工夫したポイント②　 自信のない予測は、後で人手で確認するようなフローにするこれまでは出力されたカテゴリの全てを信頼するようにしていましたが、一定の信頼値以下の予測に関しては後で人手で確認するというフローを導入しました。そうすることで、予測結果の信頼値が高いものは予測結果がそのまま採用され、信頼値が低いものは人が後で確認するという流れになりました。また人が後で確認してカテゴリを定めたデータは、学習モデルの精度向上のための再学習に使用することができます。工夫したポイントを導入した結果元々正解率が62%だったものが上に示したような工夫ポイントを導入することで正解率は91%まで向上しました。また、そのうち2割ほどが人手での再確認を要するものとなりました。さいごに工夫したポイント②の「自信のない予測は、後で人手で確認するようなフローにする」はちょっとせこいのではないか？と思われる方もいるかもしれません。しかし、残りの全てのクライントに手動で割り振ることと比較すると業務量としてはかなり削減できることになっています。（8割軽減）「機械にできる簡単なタスクは機械に任せてしまって、人間は本当に考えないといけないような業務に集中できる」ような分析やサービス作りに携わっていきたいなぁと改めて感じた業務でした。最後までありがとうございました。P.S. 工夫ポイント1はlossの設計をうまくやれば不要だったりするのかなぁなんて思ったり...","link":"https://developer.feedforce.jp/entry/2022/02/21/104757","isoDate":"2022-02-21T01:47:57.000Z","dateMiliSeconds":1645408077000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"feedforce"},{"title":"操作ログを用いた行動パターン変化の検知","content":"<p>こんにちは　\nデータサイエンティストの<a href=\"https://twitter.com/feed_yao\">八百俊哉</a>です。\n最近は花粉が飛んでいることを感じています。目がかゆい。</p>\n\n<p>今回はdfplus.ioの操作ログを用いてクライアントの行動パターン変化を検知する仕組みを作成し、社内で有効活用された事例がありますので、その手法と成果を紹介します。</p>\n\n<p>弊社のサービス<a href=\"https://dfplus.io/\">dfplus.io</a>はマーケティング・広告運用チームのためのデータフィード管理ツールになります。様々な商品・商材データをマーケティングでフル活用できるようになるサービスです。</p>\n\n<h1>分析背景</h1>\n\n<p>dfplus.ioでの解約理由の一つに<strong>「クライアントの担当者が変更する際に引き継ぎがうまくできず、dfplus.ioの使い方がわからなくなってしまう」</strong>というケースがありました。</p>\n\n<p>dfplus.ioカスタマーサクセスチーム（※以下CSチーム）では、<strong>クライアント側から事前に担当者の変更のご連絡を頂いた場合は新しいご担当者様にdfplus.ioの操作や活用に関するご案内をしております。しかし、クライアント側からご連絡がない場合は担当者の変更に気付くことができずそのご案内ができないという課題</strong>がありました。</p>\n\n<p>その為なんらかの方法でクライアントの担当者の変更を検知することができると、新しい担当者にCSチームからdfplus.ioの操作や活用に関するご案内をすることができます。</p>\n\n<p>そこで今回は<strong>操作ログを用いて行動パターンの変化を検知するという分析を実施し、クライアントの担当者の変更に気がつけるようにしました。</strong></p>\n\n<h1>操作ログを用いて行動パターンを可視化</h1>\n\n<p>dfplus.ioには全部で90個のAPIが存在します。どのアカウントがいつどのAPIを使用したのかのデータがすべて蓄積されている状態です。以下が実際に今回の分析で使用したデータになります。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"実際に使用した元データ\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220208/20220208141429.png\" alt=\"f:id:newton800:20220208141429p:plain\" width=\"643\" height=\"173\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>実際に使用した元データ</figcaption></figure></p>\n\n<p>今回はこちらのデータを用いて各クライアントの行動パターンを作成しました。以下が実際の操作ログを用いて作成できた行動パターンの可視化例です。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220209/20220209092518.png\" alt=\"f:id:newton800:20220209092518p:plain\" width=\"778\" height=\"531\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>それぞれの<strong>APIに0~89の通し番号を割り振り、x軸</strong>としました。また、<strong>y軸には日数を使用</strong>して上のように可視化しました。</p>\n\n<p>1日のうち1度でもAPIを操作すれば、(x(API番号), y(操作日)) = 1となるようにしており、画素値は0か1しか取らないようになっています。(画像で表現するときは1を255としている。)</p>\n\n<p><strong>それらのデータを月ごとに切り分け、前月とその前の月の行動パターンを比較することで、行動パターンの変化を確認しました。</strong></p>\n\n<p>上の画像の例だと2021年12月と2022年01月を比較することで、変化の検知を行うということになります。</p>\n\n<h1>行動パターン変化の定義</h1>\n\n<p>ここから2つの月の行動パターンを比較していきます。</p>\n\n<p>はじめは画像同士をそのまま引き算することで変化量を求めようと思いました。しかし、仮にdfplus.ioを使用する業務が1日遅れになってしまったクライアントがいた場合、同じ作業をしているにも関わらず、行動パターン変化量が大きくなってしまいます。</p>\n\n<p>そこで行動パターン変化量(<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20d\" alt=\" d\"/>)を以下のように定義しました。</p>\n\n<div align=\"center\">\n<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20d%20%3D%20%20%5Cdfrac%7B%5Csum%20%5E%7B89%7D_%7Bx%3D0%7D%5Cleft%28%20%5Csum%20%5E%7B30%7D_%7By%3D0%7Dp_%7Bx%2Cy%7D%5E%7Bbetore%7D-%5Csum%20%5E%7B30%7D_%7By%3D0%7Dp_%7Bx%2Cy%7D%5E%7Bafter%7D%5Cright%29%20%5E%7B2%7D%7D%7B%5Cleft%28%20%5Csum%20%5E%7B89%7D_%7Bx%3D0%7D%5Csum%20%5E%7B30%7D_%7By%3D0%7Dp_%7Bx%2Cy%7D%5E%7Bbetore%7D%2B%5Csum%20%5E%7B89%7D_%7Bx%3D0%7D%5Csum%20%5E%7B30%7D_%7By%3D0%7Dp_%7Bx%2Cy%7D%5E%7Bafter%7D%5Cright%29%20%7D%5C%5C%20\" alt=\" d =  \\dfrac{\\sum ^{89}_{x=0}\\left( \\sum ^{30}_{y=0}p_{x,y}^{betore}-\\sum ^{30}_{y=0}p_{x,y}^{after}\\right) ^{2}}{\\left( \\sum ^{89}_{x=0}\\sum ^{30}_{y=0}p_{x,y}^{betore}+\\sum ^{89}_{x=0}\\sum ^{30}_{y=0}p_{x,y}^{after}\\right) }\\\\ \"/>\n</div>\n\n\n<p></p></p>\n\n<div align=\"center\">\n<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%0Ap_%7Bx%2Cy%7D%20%3D%20%5Cleft%5C%7B%0A%5Cbegin%7Barray%7D%7Bll%7D%0A1%20%26%20%E6%93%8D%E4%BD%9C%E3%81%8C%E3%81%82%E3%81%A3%E3%81%9F%E5%A0%B4%E5%90%88%5C%5C%0A0%20%26%20%E6%93%8D%E4%BD%9C%E3%81%8C%E3%81%AA%E3%81%8B%E3%81%A3%E3%81%9F%E5%A0%B4%E5%90%88%0A%5Cend%7Barray%7D%0A%5Cright.%0A\" alt=\" \np_{x,y} = \\left\\{\n\\begin{array}{ll}\n1 &amp; &#x64CD;&#x4F5C;&#x304C;&#x3042;&#x3063;&#x305F;&#x5834;&#x5408;\\\\\n0 &amp; &#x64CD;&#x4F5C;&#x304C;&#x306A;&#x304B;&#x3063;&#x305F;&#x5834;&#x5408;\n\\end{array}\n\\right.\n\"/>\n</div>\n\n\n<p></p></p>\n\n<p>もともとは分子の部分だけを用いて行動パターン変化量と定義していましたが、そうすると必然的に操作頻度が多いユーザーの変化量が多くなってしまい、月に数度しか使用しないクライアントの行動パターン変化量がとても小さくなってしまうという課題がありました。</p>\n\n<p>そこで上のように分母に全体の操作頻度を導入することで、操作頻度による評価のばらつきを削減しました。</p>\n\n<p>行動パターン変化量(<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20d\" alt=\" d\"/>)がいくつ以上になれば、行動パターンが変化したと言えるかについては、CSチームと<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20d\" alt=\" d\"/>と行動パターンの可視化画像を確認の上、決定しております。今後もその閾値については調整を行っていく予定になります。</p>\n\n<h1>実際に検知できた例</h1>\n\n<p><strong>これらの仕組みを使用することで毎月10件前後の変化検知があります。\nその検知結果をCSチームで確認し、ご案内が必要だと判断した場合にCSチームからクライアントに連絡を実施するという流れになっています。</strong></p>\n\n<p>以下は実際に担当者の変更の検知に成功した事例です。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220208/20220208160800.png\" alt=\"f:id:newton800:20220208160800p:plain\" width=\"721\" height=\"361\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>可視化された行動パターンからも確認できるように、<strong>2021年11月までは頻繁に使用されていましたが、2021月12月の前半以降は使用回数が激減しています。</strong></p>\n\n<p><strong>実際にこちらのクライアントは担当者が退職されており、担当者の変更検知の成功例となりました。</strong></p>\n\n<p>以下は、実際にCSチームから頂いた連絡です。\n<span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20220208/20220208161732.png\" alt=\"f:id:newton800:20220208161732p:plain\" width=\"883\" height=\"251\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h1>さいごに</h1>\n\n<p>今回は操作ログを用いた行動パターン変化の検知について紹介しました。\nこちらの分析案件は、ヒアリングから現在まで1年ほどかけて現状の形になりました。入社2年目の私にとってはかなり長い間実施した業務になります。</p>\n\n<p>今回は触れられていませんが、現在に至るまで紆余曲折ありました。最後まで諦めずに現在の形にできて非常に嬉しく感じます。</p>\n\n<p>現在運用から4ヶ月ほど経過しており、実際に担当者の変更が検知できた事例が2件となっています。引き続き運用を続けていくことで、より多くの成果が残せると考えています。</p>\n\n<p>最後までお付き合いいただき、ありがとうございました。</p>\n","contentSnippet":"こんにちは　データサイエンティストの八百俊哉です。最近は花粉が飛んでいることを感じています。目がかゆい。今回はdfplus.ioの操作ログを用いてクライアントの行動パターン変化を検知する仕組みを作成し、社内で有効活用された事例がありますので、その手法と成果を紹介します。弊社のサービスdfplus.ioはマーケティング・広告運用チームのためのデータフィード管理ツールになります。様々な商品・商材データをマーケティングでフル活用できるようになるサービスです。分析背景dfplus.ioでの解約理由の一つに「クライアントの担当者が変更する際に引き継ぎがうまくできず、dfplus.ioの使い方がわからなくなってしまう」というケースがありました。dfplus.ioカスタマーサクセスチーム（※以下CSチーム）では、クライアント側から事前に担当者の変更のご連絡を頂いた場合は新しいご担当者様にdfplus.ioの操作や活用に関するご案内をしております。しかし、クライアント側からご連絡がない場合は担当者の変更に気付くことができずそのご案内ができないという課題がありました。その為なんらかの方法でクライアントの担当者の変更を検知することができると、新しい担当者にCSチームからdfplus.ioの操作や活用に関するご案内をすることができます。そこで今回は操作ログを用いて行動パターンの変化を検知するという分析を実施し、クライアントの担当者の変更に気がつけるようにしました。操作ログを用いて行動パターンを可視化dfplus.ioには全部で90個のAPIが存在します。どのアカウントがいつどのAPIを使用したのかのデータがすべて蓄積されている状態です。以下が実際に今回の分析で使用したデータになります。実際に使用した元データ今回はこちらのデータを用いて各クライアントの行動パターンを作成しました。以下が実際の操作ログを用いて作成できた行動パターンの可視化例です。それぞれのAPIに0~89の通し番号を割り振り、x軸としました。また、y軸には日数を使用して上のように可視化しました。1日のうち1度でもAPIを操作すれば、(x(API番号), y(操作日)) = 1となるようにしており、画素値は0か1しか取らないようになっています。(画像で表現するときは1を255としている。)それらのデータを月ごとに切り分け、前月とその前の月の行動パターンを比較することで、行動パターンの変化を確認しました。上の画像の例だと2021年12月と2022年01月を比較することで、変化の検知を行うということになります。行動パターン変化の定義ここから2つの月の行動パターンを比較していきます。はじめは画像同士をそのまま引き算することで変化量を求めようと思いました。しかし、仮にdfplus.ioを使用する業務が1日遅れになってしまったクライアントがいた場合、同じ作業をしているにも関わらず、行動パターン変化量が大きくなってしまいます。そこで行動パターン変化量()を以下のように定義しました。もともとは分子の部分だけを用いて行動パターン変化量と定義していましたが、そうすると必然的に操作頻度が多いユーザーの変化量が多くなってしまい、月に数度しか使用しないクライアントの行動パターン変化量がとても小さくなってしまうという課題がありました。そこで上のように分母に全体の操作頻度を導入することで、操作頻度による評価のばらつきを削減しました。行動パターン変化量()がいくつ以上になれば、行動パターンが変化したと言えるかについては、CSチームとと行動パターンの可視化画像を確認の上、決定しております。今後もその閾値については調整を行っていく予定になります。実際に検知できた例これらの仕組みを使用することで毎月10件前後の変化検知があります。その検知結果をCSチームで確認し、ご案内が必要だと判断した場合にCSチームからクライアントに連絡を実施するという流れになっています。以下は実際に担当者の変更の検知に成功した事例です。可視化された行動パターンからも確認できるように、2021年11月までは頻繁に使用されていましたが、2021月12月の前半以降は使用回数が激減しています。実際にこちらのクライアントは担当者が退職されており、担当者の変更検知の成功例となりました。以下は、実際にCSチームから頂いた連絡です。さいごに今回は操作ログを用いた行動パターン変化の検知について紹介しました。こちらの分析案件は、ヒアリングから現在まで1年ほどかけて現状の形になりました。入社2年目の私にとってはかなり長い間実施した業務になります。今回は触れられていませんが、現在に至るまで紆余曲折ありました。最後まで諦めずに現在の形にできて非常に嬉しく感じます。現在運用から4ヶ月ほど経過しており、実際に担当者の変更が検知できた事例が2件となっています。引き続き運用を続けていくことで、より多くの成果が残せると考えています。最後までお付き合いいただき、ありがとうございました。","link":"https://developer.feedforce.jp/entry/2022/02/09/110824","isoDate":"2022-02-09T02:08:24.000Z","dateMiliSeconds":1644372504000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"feedforce"},{"title":"Causal Impactを用いた入札単価調整の効果検証","content":"<p>こんにちは　機械学習エンジニアの<a href=\"https://twitter.com/feed_yao\">八百俊哉</a>です。最近はバレーボールをやることにハマっており、激しめに運動しています。今日も筋肉痛です。</p>\n\n<p>今回はGoogle広告の入札単価調整が広告成果にどのような影響を与えるのかCausal Impactを用いて検証を実施しましたので、その結果を共有したいと思います。</p>\n\n<h1>分析背景</h1>\n\n<p>弊社のサービス<a href=\"https://ecbooster.jp/\">EC Booster</a>は、Google ショッピング広告の自動運用による自社EC自動集客サービスです。主要ECシステムと連携することで、Google の検索結果画面に画像付きで自社商品を訴求することが可能となります。</p>\n\n<p>Google 広告には<a href=\"https://support.google.com/google-ads/answer/2732132?hl=ja\">入札単価調整</a>という機能があり、それをショッピング広告でも使用することによって効率的に広告配信を行うことができるとされています。\nそこで<strong>今回はEC Boosterで入札単価調整を実施することで、どれほど広告の実績がよくなるのかを検証しました。</strong></p>\n\n<h1>Causal Impactを用いた効果検証</h1>\n\n<p>今回使用した効果検証の方法である<a href=\"https://google.github.io/CausalImpact/CausalImpact.html\">Causal Impact</a>をご紹介します。</p>\n\n<p><strong>Causal ImpactとはGoogleが作成したベイズ構造時系列モデルを使用した因果推論のためのパッケージになります。</strong></p>\n\n<p>仕組みを簡単に表現すると、あるイベントが介入した日を境に「過去の実績から推定される反事実」と「実際に観測された事実」を比較することによって、イベントの効果の大きさを測ると言うものです。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211215/20211215120354.png\" alt=\"f:id:newton800:20211215120354p:plain\" width=\"761\" height=\"261\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>今回は実際に検証したことを紹介する記事になりますので、これ以上詳しくCausal Impactの仕組みについては言及しないです。\nより詳しくCausal Impactについて知りたい方は以下の書籍が参考になるので是非読んでみてください。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fwww.amazon.co.jp%2F%25E3%2583%2587%25E3%2583%25BC%25E3%2582%25BF%25E8%25A7%25A3%25E6%259E%2590%25E3%2581%25AE%25E3%2581%259F%25E3%2582%2581%25E3%2581%25AE%25E7%25B5%25B1%25E8%25A8%2588%25E3%2583%25A2%25E3%2583%2587%25E3%2583%25AA%25E3%2583%25B3%25E3%2582%25B0%25E5%2585%25A5%25E9%2596%2580%25E2%2580%2595%25E2%2580%2595%25E4%25B8%2580%25E8%2588%25AC%25E5%258C%2596%25E7%25B7%259A%25E5%25BD%25A2%25E3%2583%25A2%25E3%2583%2587%25E3%2583%25AB%25E3%2583%25BB%25E9%259A%258E%25E5%25B1%25A4%25E3%2583%2599%25E3%2582%25A4%25E3%2582%25BA%25E3%2583%25A2%25E3%2583%2587%25E3%2583%25AB%25E3%2583%25BBMCMC-%25E7%25A2%25BA%25E7%258E%2587%25E3%2581%25A8%25E6%2583%2585%25E5%25A0%25B1%25E3%2581%25AE%25E7%25A7%2591%25E5%25AD%25A6-%25E4%25B9%2585%25E4%25BF%259D-%25E6%258B%2593%25E5%25BC%25A5%2Fdp%2F400006973X\" title=\"データ解析のための統計モデリング入門――一般化線形モデル・階層ベイズモデル・MCMC (確率と情報の科学) | 久保 拓弥 |本 | 通販 | Amazon\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://www.amazon.co.jp/%E3%83%87%E3%83%BC%E3%82%BF%E8%A7%A3%E6%9E%90%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E7%B5%B1%E8%A8%88%E3%83%A2%E3%83%87%E3%83%AA%E3%83%B3%E3%82%B0%E5%85%A5%E9%96%80%E2%80%95%E2%80%95%E4%B8%80%E8%88%AC%E5%8C%96%E7%B7%9A%E5%BD%A2%E3%83%A2%E3%83%87%E3%83%AB%E3%83%BB%E9%9A%8E%E5%B1%A4%E3%83%99%E3%82%A4%E3%82%BA%E3%83%A2%E3%83%87%E3%83%AB%E3%83%BBMCMC-%E7%A2%BA%E7%8E%87%E3%81%A8%E6%83%85%E5%A0%B1%E3%81%AE%E7%A7%91%E5%AD%A6-%E4%B9%85%E4%BF%9D-%E6%8B%93%E5%BC%A5/dp/400006973X\">www.amazon.co.jp</a></cite></p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fwww.amazon.co.jp%2F%25E6%2599%2582%25E7%25B3%25BB%25E5%2588%2597%25E5%2588%2586%25E6%259E%2590%25E3%2581%25A8%25E7%258A%25B6%25E6%2585%258B%25E7%25A9%25BA%25E9%2596%2593%25E3%2583%25A2%25E3%2583%2587%25E3%2583%25AB%25E3%2581%25AE%25E5%259F%25BA%25E7%25A4%258E-R%25E3%2581%25A8Stan%25E3%2581%25A7%25E5%25AD%25A6%25E3%2581%25B6%25E7%2590%2586%25E8%25AB%2596%25E3%2581%25A8%25E5%25AE%259F%25E8%25A3%2585-%25E9%25A6%25AC%25E5%25A0%25B4-%25E7%259C%259F%25E5%2593%2589%2Fdp%2F4903814874\" title=\"時系列分析と状態空間モデルの基礎: RとStanで学ぶ理論と実装 | 真哉, 馬場 |本 | 通販 | Amazon\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://www.amazon.co.jp/%E6%99%82%E7%B3%BB%E5%88%97%E5%88%86%E6%9E%90%E3%81%A8%E7%8A%B6%E6%85%8B%E7%A9%BA%E9%96%93%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%9F%BA%E7%A4%8E-R%E3%81%A8Stan%E3%81%A7%E5%AD%A6%E3%81%B6%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%A3%85-%E9%A6%AC%E5%A0%B4-%E7%9C%9F%E5%93%89/dp/4903814874\">www.amazon.co.jp</a></cite></p>\n\n<h1>入札単価調整はコンバージョン率に良い効果をもたらしている</h1>\n\n<p>入札単価調整を実施することで、広告効果の高いユーザーに対して積極的に入札を行うようになり、逆に広告効果の低いユーザーに対しては入札を控えるようになります。</p>\n\n<p>そのようにすることで、コンバージョン率の向上が期待されていました。コンバージョン率が向上するとコンバージョン数が上がり、最終的には費用対効果であるROASの向上までが考えられます。</p>\n\n<p>実際に分析前に立てられた仮説は以下です。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"分析前に立てた仮説\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211221/20211221153325.png\" alt=\"f:id:newton800:20211221153325p:plain\" width=\"691\" height=\"171\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>分析前に立てた仮説</figcaption></figure></p>\n\n<p>それでは実際に入札単価調整を実施することで、広告成果がどう変化したのかを紹介します。</p>\n\n<p>入札単価調整を行う前の実績と入札単価調整を行った後の実績を用いてCausal Impactを実施しました。</p>\n\n<p>その結果以下のようにそれぞれの広告成果に入札単価調整が影響を与えているということがわかりました。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"分析を実施することで分かった結果の一部抜粋\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211221/20211221153346.png\" alt=\"f:id:newton800:20211221153346p:plain\" width=\"691\" height=\"171\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>分析を実施することで分かった結果の一部抜粋</figcaption></figure></p>\n\n<p>結果から<strong>コンバージョン率が向上することによってコンバージョン数が向上していることが確認されました。</strong>（他の要素も向上していますが、今回はコンバージョン率に焦点を当てています）</p>\n\n<p>実際にCVRに関してのCausal Impactの実行結果は以下のようになっていました。（一部公開できない部分は黒くしてます）</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211215/20211215123528.png\" alt=\"f:id:newton800:20211215123528p:plain\" width=\"1180\" height=\"590\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>上段に図示された実線が実際に観測されたコンバージョン率で青い点線が過去の傾向を元にした予測コンバージョン率になります。入札単価調を開始した縦の点線の位置から、予測コンバージョン率（入札単価調整を仮にやっていない場合）よりも実際に観測されたコンバージョン率（入札単価調整を実施した場合）の方が大きくなっていることがわかります。</p>\n\n<p>中段に図示されたものが入札単価調整の効果となります。入札単価調整を実施してから、コンバージョン率が増加したといえそうです。</p>\n\n<p>下段に図示されたものは増加したコンバージョン率の累計値になります。</p>\n\n<p><strong>このようなことから入札単価調整は、コンバージョン率向上に貢献していることがわかりました。</strong></p>\n\n<h1>そして現在入札単価調整は</h1>\n\n<p>このような分析もあり、EC Boosterでは現在入札単価調整は全てのユーザーに対して適用されるように設定されています。</p>\n\n<p>このようにEC Boosterの裏側では効果検証を繰り返し、より広告成果が良くなるように最適化を実施しています。</p>\n\n<h1>さいごに</h1>\n\n<p>最後までお付き合いいただきありがとうございます。\n弊社のデータ分析チームに新卒入社してはや2年弱ほど経過し、少しずつ成果が出せるようになってきました。</p>\n\n<p>今後はより積極的にデータ分析に関する情報発信を行っていきたいと考えています。よろしくお願いします。</p>\n\n<p>P.S.祖父母に自分の仕事伝えるのが難しいです。</p>\n","contentSnippet":"こんにちは　機械学習エンジニアの八百俊哉です。最近はバレーボールをやることにハマっており、激しめに運動しています。今日も筋肉痛です。今回はGoogle広告の入札単価調整が広告成果にどのような影響を与えるのかCausal Impactを用いて検証を実施しましたので、その結果を共有したいと思います。分析背景弊社のサービスEC Boosterは、Google ショッピング広告の自動運用による自社EC自動集客サービスです。主要ECシステムと連携することで、Google の検索結果画面に画像付きで自社商品を訴求することが可能となります。Google 広告には入札単価調整という機能があり、それをショッピング広告でも使用することによって効率的に広告配信を行うことができるとされています。そこで今回はEC Boosterで入札単価調整を実施することで、どれほど広告の実績がよくなるのかを検証しました。Causal Impactを用いた効果検証今回使用した効果検証の方法であるCausal Impactをご紹介します。Causal ImpactとはGoogleが作成したベイズ構造時系列モデルを使用した因果推論のためのパッケージになります。仕組みを簡単に表現すると、あるイベントが介入した日を境に「過去の実績から推定される反事実」と「実際に観測された事実」を比較することによって、イベントの効果の大きさを測ると言うものです。今回は実際に検証したことを紹介する記事になりますので、これ以上詳しくCausal Impactの仕組みについては言及しないです。より詳しくCausal Impactについて知りたい方は以下の書籍が参考になるので是非読んでみてください。www.amazon.co.jpwww.amazon.co.jp入札単価調整はコンバージョン率に良い効果をもたらしている入札単価調整を実施することで、広告効果の高いユーザーに対して積極的に入札を行うようになり、逆に広告効果の低いユーザーに対しては入札を控えるようになります。そのようにすることで、コンバージョン率の向上が期待されていました。コンバージョン率が向上するとコンバージョン数が上がり、最終的には費用対効果であるROASの向上までが考えられます。実際に分析前に立てられた仮説は以下です。分析前に立てた仮説それでは実際に入札単価調整を実施することで、広告成果がどう変化したのかを紹介します。入札単価調整を行う前の実績と入札単価調整を行った後の実績を用いてCausal Impactを実施しました。その結果以下のようにそれぞれの広告成果に入札単価調整が影響を与えているということがわかりました。分析を実施することで分かった結果の一部抜粋結果からコンバージョン率が向上することによってコンバージョン数が向上していることが確認されました。（他の要素も向上していますが、今回はコンバージョン率に焦点を当てています）実際にCVRに関してのCausal Impactの実行結果は以下のようになっていました。（一部公開できない部分は黒くしてます）上段に図示された実線が実際に観測されたコンバージョン率で青い点線が過去の傾向を元にした予測コンバージョン率になります。入札単価調を開始した縦の点線の位置から、予測コンバージョン率（入札単価調整を仮にやっていない場合）よりも実際に観測されたコンバージョン率（入札単価調整を実施した場合）の方が大きくなっていることがわかります。中段に図示されたものが入札単価調整の効果となります。入札単価調整を実施してから、コンバージョン率が増加したといえそうです。下段に図示されたものは増加したコンバージョン率の累計値になります。このようなことから入札単価調整は、コンバージョン率向上に貢献していることがわかりました。そして現在入札単価調整はこのような分析もあり、EC Boosterでは現在入札単価調整は全てのユーザーに対して適用されるように設定されています。このようにEC Boosterの裏側では効果検証を繰り返し、より広告成果が良くなるように最適化を実施しています。さいごに最後までお付き合いいただきありがとうございます。弊社のデータ分析チームに新卒入社してはや2年弱ほど経過し、少しずつ成果が出せるようになってきました。今後はより積極的にデータ分析に関する情報発信を行っていきたいと考えています。よろしくお願いします。P.S.祖父母に自分の仕事伝えるのが難しいです。","link":"https://developer.feedforce.jp/entry/2021/12/21/154206","isoDate":"2021-12-21T06:42:06.000Z","dateMiliSeconds":1640068926000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20211215/20211215120354.png","authorName":"feedforce"},{"title":"Amazon EKS で高負荷時に CoreDNS が原因で稀にネットワークエラーが発生していた時のトラブルシュート","content":"<p>ソーシャルPLUS の開発チームでインフラエンジニア をやっています <a href=\"http://blog.hatena.ne.jp/mayuki123/\" class=\"hatena-id-icon\"><img src=\"https://cdn.profile-image.st-hatena.com/users/mayuki123/profile.png\" width=\"16\" height=\"16\" alt=\"\" class=\"hatena-id-icon\">id:mayuki123</a> です。今月からフィードフォースから分社化をした株式会社ソーシャルPLUS の所属となりましたが、仕事内容は変わらずにサービスのインフラ改善を進めていく事になるかと思います。</p>\n\n<p>2019年11月に技術スタックを整理してみたという記事から2年弱経過していますが、ソーシャルPLUSのインフラ環境は、一部アプリケーションについてはコンテナ環境を Amazon EKS にホスティングして本番運用するようになりました。あと数ヶ月ほどで全ての環境がEC2からコンテナに置き換えられると良いなと思っています(願望)。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fdeveloper.feedforce.jp%2Fentry%2F2019%2F11%2F25%2F120000\" title=\"ソーシャルPLUS の技術スタックを整理してみた - Feedforce Developer Blog\" class=\"embed-card embed-blogcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 190px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://developer.feedforce.jp/entry/2019/11/25/120000\">developer.feedforce.jp</a></cite></p>\n\n<p>そして、既に利用されている機能の一部を Amazon EKS に移行して、しばらく経過した時にアプリケーションでネットワークエラーが稀に発生していました。原因調査をした結果が CoreDNS の負荷によるものと発覚するまでのトラブルシュートの流れについて、記事として書き残しておきます。</p>\n\n<ul class=\"table-of-contents\">\n    <li><a href=\"#発生していた事象\">発生していた事象</a></li>\n    <li><a href=\"#Datadog-を活用した原因調査\">Datadog を活用した原因調査</a><ul>\n            <li><a href=\"#アプリケーションの負荷状況\">アプリケーションの負荷状況</a></li>\n            <li><a href=\"#EKS-上のコンテナの調査\">EKS 上のコンテナの調査</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#EKS-のCoreDNS-の調査\">EKS のCoreDNS の調査</a><ul>\n            <li><a href=\"#CoreDNS-のデバッグログの有効化\">CoreDNS のデバッグログの有効化</a></li>\n            <li><a href=\"#Kubernetes-の名前解決について\">Kubernetes の名前解決について</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#CoreDNS-の負荷軽減\">CoreDNS の負荷軽減</a><ul>\n            <li><a href=\"#ドメインの末尾にドット--を追加する\">ドメインの末尾にドット (.) を追加する</a></li>\n            <li><a href=\"#etcresolvconf-で-ndots1-の設定をする\">/etc/resolv.conf で ndots:1 の設定をする</a></li>\n            <li><a href=\"#その他の-CoreDNS-の負荷軽減の方法\">その他の CoreDNS の負荷軽減の方法</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#最終的な結果\">最終的な結果</a></li>\n    <li><a href=\"#おわりに\">おわりに</a></li>\n    <li><a href=\"#おまけ\">おまけ</a></li>\n</ul>\n\n<h2 id=\"発生していた事象\">発生していた事象</h2>\n\n<p>ソーシャルPLUSでは、バックエンドのアプリケーションでエラーが発生した時に、Bugsnag を利用して Slack 通知するようにしています。ある時に<code>Mysql2::Error::ConnectionError</code> が発生しました。単発のネットワークエラーの場合はアプリケーションがリトライする事でサービス影響がない事も多く、一時的な問題と思って静観する事があるかと思います。しかし、また数日後に同じ事象が発生しました。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/m/mayuki123/20210830/20210830152912.png\" alt=\"f:id:mayuki123:20210830152912p:plain\" width=\"667\" height=\"258\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p><a href=\"https://ja.wikipedia.org/wiki/%E3%83%8F%E3%82%A4%E3%83%B3%E3%83%AA%E3%83%83%E3%83%92%E3%81%AE%E6%B3%95%E5%89%87\">ハインリッヒの1：29：300の法則</a>のように、ちょっとした異常を見落としていると重大なサービス障害となってしまう可能性があるので、原因調査を始めます。</p>\n\n<h2 id=\"Datadog-を活用した原因調査\">Datadog を活用した原因調査</h2>\n\n<p>ソーシャルPLUSでは、モニタリングサービスの Datadog を利用しているのでメトリクスやログの調査を出来るようになっています。どこが原因かを探り始めました。</p>\n\n<h3 id=\"アプリケーションの負荷状況\">アプリケーションの負荷状況</h3>\n\n<p>まずはアプリケーションで利用するサーバの負荷状況を確認する所から始めました。<code>Mysql2::Error::ConnectionError</code> が発生した時刻は EKS の Node の CPU 使用率が 70% ほどで、アプリケーションで負荷のかかる処理の最中でした。また、データベースの負荷は少し前に負荷対策の改善をした事もあって、今回の事件の犯人ではなさそうです。他にもEC2 と DB 間でネットワークのボトルネックがないかなどの確認はしましたが、CPU 使用率が高い以外の問題は特に見つかりませんでした。完全犯罪でしょうか。</p>\n\n<h3 id=\"EKS-上のコンテナの調査\">EKS 上のコンテナの調査</h3>\n\n<p>サーバ単体の問題ではないとすると、Amazon EKS で何か起きている事を疑うことにしました。EKSで動かしているコンテナのログは Datadog Logs に送っているので、<strong>エラーが発生していたアプリケーション以外のログ</strong> を確認していると、MySQL の ConnectionError が発生した時間帯に下記の Warning のメッセージが出ている事に気づきました。このログは Amazon Kinesis Data Firehose にログを送る Fluent Bit のコンテナで発生しており、エラーが発生してたアプリケーションとは異なるノードに存在してました。</p>\n\n<blockquote><p>[yyyy/mm/dd hh:mm:ss] [ warn] [net] getaddrinfo(host='kinesis.ap-northeast-1.amazonaws.com'): Name or service not known</p></blockquote>\n\n<p>同時刻に特定のアプリケーション以外のコンテナも影響を受けていることから、EKS の中で問題がありそうです。元々、EKSに関する技術ブログは目を通すようにしていた事もあり、Kubernetes の DNS の名前解決で問題が発生する場合があるというのは知っていたので、CoreDNSに焦点を当てて調べることにしました。アウトプットをしてくれる人たちには、いつも感謝をしています。</p>\n\n<ul>\n<li><a href=\"https://medium.com/cloutive/production-ready-eks-coredns-configuration-6fea830606f8\">Production Ready EKS CoreDNS Configuration | by Serkan Capkan | Cloutive Technology Solutions - Tech Blog | Medium</a></li>\n<li><a href=\"https://creators-note.chatwork.com/entry/2021/01/05/104206#%E4%B8%80%E5%AE%9A%E6%95%B0%E3%81%AEPod%E4%BB%A5%E4%B8%8A%E3%81%AB%E3%81%AA%E3%82%8B%E3%81%A8%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%8C%E4%B8%8D%E5%AE%89%E5%AE%9A%E3%81%AB%E3%81%AA%E3%82%8B\">EKS&#x3067;DNS&#x3092;&#x5B89;&#x5B9A;&#x3055;&#x305B;&#x308B;&#x305F;&#x3081;&#x306B;&#x5BFE;&#x5FDC;&#x3057;&#x305F;&#x3053;&#x3068; - Chatwork Creator&#39;s Note</a></li>\n<li><a href=\"https://labs.gree.jp/blog/2020/01/20271/\">&#x30B9;&#x30DE;&#x30DB;&#x30B2;&#x30FC;&#x30E0;&#x306E; API &#x30B5;&#x30FC;&#x30D0;&#x306B;&#x304A;&#x3051;&#x308B; EKS &#x306E;&#x904B;&#x7528;&#x4E8B;&#x4F8B; | &#x30A8;&#x30F3;&#x30B8;&#x30CB;&#x30A2;&#x30D6;&#x30ED;&#x30B0; | GREE Engineering</a></li>\n</ul>\n\n\n<h2 id=\"EKS-のCoreDNS-の調査\">EKS のCoreDNS の調査</h2>\n\n<p>Datadog Agent で Kurbernetes の各種メトリクスを収集していて、EKS の CoreDNS の状況も Datadog の Metric Explorer で確認する事が出来るようになっています。</p>\n\n<ul>\n<li><a href=\"https://docs.datadoghq.com/ja/integrations/coredns/?tab=docker#%E3%83%A1%E3%83%88%E3%83%AA%E3%82%AF%E3%82%B9\">Datadog で取得可能な CoreDNS のメトリクス</a></li>\n</ul>\n\n\n<p><code>coredns.request_count</code> を確認すると特定の時間帯で CoreDNS へのリクエストが多い状態で、このタイミングでの CoreDNS Pod の CPU 負荷は10％前後でしたが、それ以外に不審なメトリクスは存在しませんでした。まだ事象の原因との確信は持てないですが、負荷がそれなりにかかっていることは確かなのでリクエストが多くなる理由を調べます。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/m/mayuki123/20210831/20210831151005.png\" alt=\"f:id:mayuki123:20210831151005p:plain\" width=\"522\" height=\"200\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h3 id=\"CoreDNS-のデバッグログの有効化\">CoreDNS のデバッグログの有効化</h3>\n\n<p>まずは CoreDNS のデバッグログを確認したいとなるかと思いますが、EKS の CoreDNS はデフォルトだとデバッグログの出力がオフの状態のため、どのようなリクエストが到達しているのかは確認する事ができません。CoreDNS のログを有効化する方法は AWS のナレッジベースにある記事に方法が記載されています。</p>\n\n<ul>\n<li><a href=\"https://aws.amazon.com/jp/premiumsupport/knowledge-center/eks-dns-failure/\">Amazon EKS &#x3067;&#x306E; DNS &#x969C;&#x5BB3;&#x306E;&#x30C8;&#x30E9;&#x30D6;&#x30EB;&#x30B7;&#x30E5;&#x30FC;&#x30C6;&#x30A3;&#x30F3;&#x30B0;</a></li>\n</ul>\n\n\n<p>この記事によると、Namespace(<code>kube-system</code>) に Configmap (<code>coredns</code>) があるので、Corefile 設定に <code>log</code> を追加するとデバッグログ が出力されるようになります。</p>\n\n<pre class=\"code\" data-lang=\"\" data-unlink># kubectl -n kube-system edit configmap coredns\nkind: ConfigMap\napiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        log    # Enabling CoreDNS Logging\n        errors\n        health\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n          pods insecure\n          upstream\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        ...</pre>\n\n\n<p>上記の設定をすると CoreDNS のPod の標準出力にデバッグログ が出力されるようになります。私の触っていた EKS の環境の場合は、数分ほどで CoreDNS の Pod で reload が発生して元の設定（デバッグログ がオフ）に戻るようになってました。</p>\n\n<h3 id=\"Kubernetes-の名前解決について\">Kubernetes の名前解決について</h3>\n\n<p>次にKubernetes 上のコンテナはどのように名前解決するのかを知っておく必要があります。Kurbernetes の Pod の DNS リゾルバー(<code>/etc/resolv.conf</code>) のデフォルト設定は下記のようになっています。</p>\n\n<pre class=\"code\" data-lang=\"\" data-unlink>% kubectl exec fluent-bit-46zvl -- cat /etc/resolv.conf\nnameserver 172.20.0.10\nsearch logging.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5</pre>\n\n\n<p>この状態で Fluent Bit のコンテナから Amazon Kinesis の API エンドポイントに疎通する場合は、CoreDNS に8回のリクエストが発生します。これは、IPv4 , IPv6 の2種類の名前解決を <code>search</code> の数だけ名前解決を試みた後で EKS 外に名前解決をする設定になっているからです。この設定になっているおかげで Kubernetes の Service を使った名前解決が出来るようになっています。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/m/mayuki123/20210831/20210831154909.png\" alt=\"f:id:mayuki123:20210831154909p:plain\" width=\"1200\" height=\"328\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>また、<code>options ndots:5</code> の設定は <code>.</code> の数が 5個以上の時は最初から外部に名前解決するようになります。そのため、Amazon Aurora や ElastiCache などのデータベースへの クラスターエンドポイントは <code>.</code> の数が五個以上あるので、CoreDNSへのリクエスト回数は少なくて済みます。ここを意識しなくてよいのはありがたいですね。</p>\n\n<p>ソーシャルPLUSというプロダクトの特性上、EKS 内のアプリケーションから外部サービスの API を実行する機会が多々あります。特定のタイミングで外部のサービスに大量のAPIリクエストを実行した際に、CoreDNS へのリクエストが増大してしまい不安定になってしまったのではと考えられます。</p>\n\n<h2 id=\"CoreDNS-の負荷軽減\">CoreDNS の負荷軽減</h2>\n\n<p>Kurbernetes 上のコンテナの名前解決を知ると、外部サービスのAPI を実行する際には CoreDNS へのリクエストが多くなる事が分かりました。ここで、CoreDNS へのリクエスト数を減らす方法は下記の二つがあります。これも AWS のナレッジベースに方法が記載されているので、詳細は下記の記事を読む方が良いと思います。</p>\n\n<ul>\n<li><a href=\"https://aws.amazon.com/jp/premiumsupport/knowledge-center/eks-dns-failure/\">Amazon EKS &#x3067;&#x306E; DNS &#x969C;&#x5BB3;&#x306E;&#x30C8;&#x30E9;&#x30D6;&#x30EB;&#x30B7;&#x30E5;&#x30FC;&#x30C6;&#x30A3;&#x30F3;&#x30B0;</a></li>\n</ul>\n\n\n<h3 id=\"ドメインの末尾にドット--を追加する\">ドメインの末尾にドット (.) を追加する</h3>\n\n<p>接続先のドメインの最後に <code>.</code> をつけると、EKS の内部で名前解決を複数回しないようになり、CoreDNS へのリクエストの総数が減ります。一例をあげると、<code>example.com</code> ではなく、 <code>example.com.</code> とする事で最初から EKS の外部に名前解決をしてくれるようになります。ドメインが SDK の内部で定義されているような場合など、変更出来ない場合はこの方法は利用出来ないかと思います。</p>\n\n<h3 id=\"etcresolvconf-で-ndots1-の設定をする\"><code>/etc/resolv.conf</code> で ndots:1 の設定をする</h3>\n\n<p><code>/etc/resolv.conf</code> に <code>options ndots:5</code> とデフォルトで設定されている数値を <code>1</code> にする事で、ドメインに <code>.</code> が含まれている場合は常に EKS の外部に名前解決するようになります。Kubernetes の Manifest に <code>spec.dnsConfig</code> パラメータを設定する事で Pod 単位で変更が出来ます。ただし、この設定をすると EKS 内部で名前解決をしなくなってしまいますが、<code>&lt;name&gt;.&lt;namespace&gt;.svc.cluster.local.</code> のように最後に <code>.</code> をつけると名前解決出来ました。Kurbernetes の Service の数が多いとこの方法を周知させるのも大変だと思います。</p>\n\n<pre class=\"code\" data-lang=\"\" data-unlink>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hoge\nspec:\n  template:\n    spec:\n      dnsConfig:\n        options:\n          - name: ndots\n            value: &#34;1&#34;</pre>\n\n\n<h3 id=\"その他の-CoreDNS-の負荷軽減の方法\">その他の CoreDNS の負荷軽減の方法</h3>\n\n<p>上記の二つの方法は CoreDNS へのリクエスト数を減らすことで、負荷を軽減するようなアプローチでした。CoreDNS の Pod 数はデフォルトで 2個となりますが、CoreDNS のPod をオートスケールする手段もあります。</p>\n\n<ul>\n<li><a href=\"https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/\">Autoscale the DNS Service in a Cluster</a></li>\n</ul>\n\n\n<p>また、Daemonset で DNS キャッシュをノード単位で配置するという方法もあります。</p>\n\n<ul>\n<li><a href=\"https://kubernetes.io/ja/docs/tasks/administer-cluster/nodelocaldns/\">KubernetesクラスターでNodeLocal DNSキャッシュを使用する</a></li>\n</ul>\n\n\n<p>この辺りは他の方が書いた技術ブログも多くあるかと思うので、この記事では特に説明はしないです。</p>\n\n<h2 id=\"最終的な結果\">最終的な結果</h2>\n\n<p>ソーシャルPLUSでは最終的に根本原因の CoreDNS へのリクエスト数を減らすために <code>/etc/resolv.conf</code> で <code>ndots:1</code> の設定をするようにしました。この設定をアプリケーションの Pod に適応した所、CoreDNS へのリクエスト数は 25% ほどと目に見えて減少させる事が出来ました。キャプチャは載せてないですが、CoreDNS の Pod の CPU使用率も 以前の半分ほどになったので、負荷軽減の目的は達成しました。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/m/mayuki123/20210831/20210831171734.png\" alt=\"f:id:mayuki123:20210831171734p:plain\" width=\"527\" height=\"187\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>ここまで、確信を持てないまま CoreDNS の負荷軽減に取り組みましたが、元々のネットワークエラーであった <code>Mysql2::Error::ConnectionError</code> のエラーは再発しなくなりました。また、EKS 上の他のコンテナも <code>Name or service not known</code> のような名前解決が出来ないといったエラーも発生しなくなりました。CoreDNS の負荷を減らす事で悩まされていた問題の解消が出来たと思います。今回のように比較的早い段階で気づく事が出来たので、お客さんへのサービス影響のある問題に発展せずに済みました。</p>\n\n<p>今後、利用者数が増えてより負荷のかかる状況になってきた時には再発する可能性はありますが、早い段階で気付けるように日々確認するダッシュボードにメトリクスを追加するようにしています。その時がきた場合は CoreDNS の Pod 数の調整や DNS キャッシュの導入が必要になりそうです。</p>\n\n<h2 id=\"おわりに\">おわりに</h2>\n\n<p>最終的には Pod の DNS 設定を調整するだけでネットワークエラーは解決しました。この記事では、結果だけではなくて解決に至るまでの経緯をメインにまとめてみました。実施していて良かったと思うことを下記にまとめます。これらの事が出来ていなければ、今回のようなネットワークエラーはたまに発生する事象として、根本原因の追及は出来なかったと思うので、サービスのオブザーバビリティを整備する事や日々の情報収集は大事ですね。</p>\n\n<ul>\n<li>アプリケーションのエラーを Slack に通知していた</li>\n<li>Kurbernetes のメトリクスを Datadog で確認できる状態だった</li>\n<li>コンテナのログを一元的に Datadog Logs  で閲覧できるようにしていた</li>\n<li>他の人の技術ブログから Kubernetes の CoreDNS が不安定になることを知っていた</li>\n</ul>\n\n\n<p>この記事に間違っている内容や、もっと良い改善方法がある事をご存知の方がいましたら、優しく教えてください。</p>\n\n<h2 id=\"おまけ\">おまけ</h2>\n\n<p>現在、ソーシャルPLUS では作りたい機能が山ほどある状況でまだまだ成長するサービスになると思うので、成長を続けるサービスに携わりたいエンジニアやデザイナーのご応募をお待ちしております！サイトにはまだないですが、インフラエンジニアも近いうちに募集をする事にはなると思います。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fopen.talentio.com%2F1%2Fc%2Ffeedforce%2Frequisitions%2Fdetail%2F21802\" title=\"Railsエンジニア【Shopify App開発/ID連携サービス】 / 株式会社フィードフォース\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://open.talentio.com/1/c/feedforce/requisitions/detail/21802\">open.talentio.com</a></cite></p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fopen.talentio.com%2F1%2Fc%2Ffeedforce%2Frequisitions%2Fdetail%2F21755\" title=\"フロントエンドエンジニア【Shopifyアプリ開発/ID連携サービス/React/TypeScript】 / 株式会社フィードフォース\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://open.talentio.com/1/c/feedforce/requisitions/detail/21755\">open.talentio.com</a></cite></p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fopen.talentio.com%2F1%2Fc%2Ffeedforce%2Frequisitions%2Fdetail%2F21760\" title=\"UI/UXデザイナー【ID連携サービス/マーケティング支援SaaS】 / 株式会社フィードフォース\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://open.talentio.com/1/c/feedforce/requisitions/detail/21760\">open.talentio.com</a></cite></p>\n\n<p>フィードフォース の他のサービスもエンジニアを募集してますので、興味があればご応募お待ちしております！</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fengineers.recruit.feedforce.jp%2F%3F_ga%3D2.157559610.1029003260.1630297434-1923366822.1626416415%23entry\" title=\"フィードフォース エンジニア・デザイナー採用サイト\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://engineers.recruit.feedforce.jp/?_ga=2.157559610.1029003260.1630297434-1923366822.1626416415#entry\">engineers.recruit.feedforce.jp</a></cite></p>\n","contentSnippet":"ソーシャルPLUS の開発チームでインフラエンジニア をやっています id:mayuki123 です。今月からフィードフォースから分社化をした株式会社ソーシャルPLUS の所属となりましたが、仕事内容は変わらずにサービスのインフラ改善を進めていく事になるかと思います。2019年11月に技術スタックを整理してみたという記事から2年弱経過していますが、ソーシャルPLUSのインフラ環境は、一部アプリケーションについてはコンテナ環境を Amazon EKS にホスティングして本番運用するようになりました。あと数ヶ月ほどで全ての環境がEC2からコンテナに置き換えられると良いなと思っています(願望)。developer.feedforce.jpそして、既に利用されている機能の一部を Amazon EKS に移行して、しばらく経過した時にアプリケーションでネットワークエラーが稀に発生していました。原因調査をした結果が CoreDNS の負荷によるものと発覚するまでのトラブルシュートの流れについて、記事として書き残しておきます。発生していた事象Datadog を活用した原因調査アプリケーションの負荷状況EKS 上のコンテナの調査EKS のCoreDNS の調査CoreDNS のデバッグログの有効化Kubernetes の名前解決についてCoreDNS の負荷軽減ドメインの末尾にドット (.) を追加する/etc/resolv.conf で ndots:1 の設定をするその他の CoreDNS の負荷軽減の方法最終的な結果おわりにおまけ発生していた事象ソーシャルPLUSでは、バックエンドのアプリケーションでエラーが発生した時に、Bugsnag を利用して Slack 通知するようにしています。ある時にMysql2::Error::ConnectionError が発生しました。単発のネットワークエラーの場合はアプリケーションがリトライする事でサービス影響がない事も多く、一時的な問題と思って静観する事があるかと思います。しかし、また数日後に同じ事象が発生しました。ハインリッヒの1：29：300の法則のように、ちょっとした異常を見落としていると重大なサービス障害となってしまう可能性があるので、原因調査を始めます。Datadog を活用した原因調査ソーシャルPLUSでは、モニタリングサービスの Datadog を利用しているのでメトリクスやログの調査を出来るようになっています。どこが原因かを探り始めました。アプリケーションの負荷状況まずはアプリケーションで利用するサーバの負荷状況を確認する所から始めました。Mysql2::Error::ConnectionError が発生した時刻は EKS の Node の CPU 使用率が 70% ほどで、アプリケーションで負荷のかかる処理の最中でした。また、データベースの負荷は少し前に負荷対策の改善をした事もあって、今回の事件の犯人ではなさそうです。他にもEC2 と DB 間でネットワークのボトルネックがないかなどの確認はしましたが、CPU 使用率が高い以外の問題は特に見つかりませんでした。完全犯罪でしょうか。EKS 上のコンテナの調査サーバ単体の問題ではないとすると、Amazon EKS で何か起きている事を疑うことにしました。EKSで動かしているコンテナのログは Datadog Logs に送っているので、エラーが発生していたアプリケーション以外のログ を確認していると、MySQL の ConnectionError が発生した時間帯に下記の Warning のメッセージが出ている事に気づきました。このログは Amazon Kinesis Data Firehose にログを送る Fluent Bit のコンテナで発生しており、エラーが発生してたアプリケーションとは異なるノードに存在してました。[yyyy/mm/dd hh:mm:ss] [ warn] [net] getaddrinfo(host='kinesis.ap-northeast-1.amazonaws.com'): Name or service not known同時刻に特定のアプリケーション以外のコンテナも影響を受けていることから、EKS の中で問題がありそうです。元々、EKSに関する技術ブログは目を通すようにしていた事もあり、Kubernetes の DNS の名前解決で問題が発生する場合があるというのは知っていたので、CoreDNSに焦点を当てて調べることにしました。アウトプットをしてくれる人たちには、いつも感謝をしています。Production Ready EKS CoreDNS Configuration | by Serkan Capkan | Cloutive Technology Solutions - Tech Blog | MediumEKSでDNSを安定させるために対応したこと - Chatwork Creator's Noteスマホゲームの API サーバにおける EKS の運用事例 | エンジニアブログ | GREE EngineeringEKS のCoreDNS の調査Datadog Agent で Kurbernetes の各種メトリクスを収集していて、EKS の CoreDNS の状況も Datadog の Metric Explorer で確認する事が出来るようになっています。Datadog で取得可能な CoreDNS のメトリクスcoredns.request_count を確認すると特定の時間帯で CoreDNS へのリクエストが多い状態で、このタイミングでの CoreDNS Pod の CPU 負荷は10％前後でしたが、それ以外に不審なメトリクスは存在しませんでした。まだ事象の原因との確信は持てないですが、負荷がそれなりにかかっていることは確かなのでリクエストが多くなる理由を調べます。CoreDNS のデバッグログの有効化まずは CoreDNS のデバッグログを確認したいとなるかと思いますが、EKS の CoreDNS はデフォルトだとデバッグログの出力がオフの状態のため、どのようなリクエストが到達しているのかは確認する事ができません。CoreDNS のログを有効化する方法は AWS のナレッジベースにある記事に方法が記載されています。Amazon EKS での DNS 障害のトラブルシューティングこの記事によると、Namespace(kube-system) に Configmap (coredns) があるので、Corefile 設定に log を追加するとデバッグログ が出力されるようになります。# kubectl -n kube-system edit configmap corednskind: ConfigMapapiVersion: v1data:  Corefile: |    .:53 {        log    # Enabling CoreDNS Logging        errors        health        kubernetes cluster.local in-addr.arpa ip6.arpa {          pods insecure          upstream          fallthrough in-addr.arpa ip6.arpa        }        ...上記の設定をすると CoreDNS のPod の標準出力にデバッグログ が出力されるようになります。私の触っていた EKS の環境の場合は、数分ほどで CoreDNS の Pod で reload が発生して元の設定（デバッグログ がオフ）に戻るようになってました。Kubernetes の名前解決について次にKubernetes 上のコンテナはどのように名前解決するのかを知っておく必要があります。Kurbernetes の Pod の DNS リゾルバー(/etc/resolv.conf) のデフォルト設定は下記のようになっています。% kubectl exec fluent-bit-46zvl -- cat /etc/resolv.confnameserver 172.20.0.10search logging.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5この状態で Fluent Bit のコンテナから Amazon Kinesis の API エンドポイントに疎通する場合は、CoreDNS に8回のリクエストが発生します。これは、IPv4 , IPv6 の2種類の名前解決を search の数だけ名前解決を試みた後で EKS 外に名前解決をする設定になっているからです。この設定になっているおかげで Kubernetes の Service を使った名前解決が出来るようになっています。また、options ndots:5 の設定は . の数が 5個以上の時は最初から外部に名前解決するようになります。そのため、Amazon Aurora や ElastiCache などのデータベースへの クラスターエンドポイントは . の数が五個以上あるので、CoreDNSへのリクエスト回数は少なくて済みます。ここを意識しなくてよいのはありがたいですね。ソーシャルPLUSというプロダクトの特性上、EKS 内のアプリケーションから外部サービスの API を実行する機会が多々あります。特定のタイミングで外部のサービスに大量のAPIリクエストを実行した際に、CoreDNS へのリクエストが増大してしまい不安定になってしまったのではと考えられます。CoreDNS の負荷軽減Kurbernetes 上のコンテナの名前解決を知ると、外部サービスのAPI を実行する際には CoreDNS へのリクエストが多くなる事が分かりました。ここで、CoreDNS へのリクエスト数を減らす方法は下記の二つがあります。これも AWS のナレッジベースに方法が記載されているので、詳細は下記の記事を読む方が良いと思います。Amazon EKS での DNS 障害のトラブルシューティングドメインの末尾にドット (.) を追加する接続先のドメインの最後に . をつけると、EKS の内部で名前解決を複数回しないようになり、CoreDNS へのリクエストの総数が減ります。一例をあげると、example.com ではなく、 example.com. とする事で最初から EKS の外部に名前解決をしてくれるようになります。ドメインが SDK の内部で定義されているような場合など、変更出来ない場合はこの方法は利用出来ないかと思います。/etc/resolv.conf で ndots:1 の設定をする/etc/resolv.conf に options ndots:5 とデフォルトで設定されている数値を 1 にする事で、ドメインに . が含まれている場合は常に EKS の外部に名前解決するようになります。Kubernetes の Manifest に spec.dnsConfig パラメータを設定する事で Pod 単位で変更が出来ます。ただし、この設定をすると EKS 内部で名前解決をしなくなってしまいますが、<name>.<namespace>.svc.cluster.local. のように最後に . をつけると名前解決出来ました。Kurbernetes の Service の数が多いとこの方法を周知させるのも大変だと思います。apiVersion: apps/v1kind: Deploymentmetadata:  name: hogespec:  template:    spec:      dnsConfig:        options:          - name: ndots            value: \"1\"その他の CoreDNS の負荷軽減の方法上記の二つの方法は CoreDNS へのリクエスト数を減らすことで、負荷を軽減するようなアプローチでした。CoreDNS の Pod 数はデフォルトで 2個となりますが、CoreDNS のPod をオートスケールする手段もあります。Autoscale the DNS Service in a Clusterまた、Daemonset で DNS キャッシュをノード単位で配置するという方法もあります。KubernetesクラスターでNodeLocal DNSキャッシュを使用するこの辺りは他の方が書いた技術ブログも多くあるかと思うので、この記事では特に説明はしないです。最終的な結果ソーシャルPLUSでは最終的に根本原因の CoreDNS へのリクエスト数を減らすために /etc/resolv.conf で ndots:1 の設定をするようにしました。この設定をアプリケーションの Pod に適応した所、CoreDNS へのリクエスト数は 25% ほどと目に見えて減少させる事が出来ました。キャプチャは載せてないですが、CoreDNS の Pod の CPU使用率も 以前の半分ほどになったので、負荷軽減の目的は達成しました。ここまで、確信を持てないまま CoreDNS の負荷軽減に取り組みましたが、元々のネットワークエラーであった Mysql2::Error::ConnectionError のエラーは再発しなくなりました。また、EKS 上の他のコンテナも Name or service not known のような名前解決が出来ないといったエラーも発生しなくなりました。CoreDNS の負荷を減らす事で悩まされていた問題の解消が出来たと思います。今回のように比較的早い段階で気づく事が出来たので、お客さんへのサービス影響のある問題に発展せずに済みました。今後、利用者数が増えてより負荷のかかる状況になってきた時には再発する可能性はありますが、早い段階で気付けるように日々確認するダッシュボードにメトリクスを追加するようにしています。その時がきた場合は CoreDNS の Pod 数の調整や DNS キャッシュの導入が必要になりそうです。おわりに最終的には Pod の DNS 設定を調整するだけでネットワークエラーは解決しました。この記事では、結果だけではなくて解決に至るまでの経緯をメインにまとめてみました。実施していて良かったと思うことを下記にまとめます。これらの事が出来ていなければ、今回のようなネットワークエラーはたまに発生する事象として、根本原因の追及は出来なかったと思うので、サービスのオブザーバビリティを整備する事や日々の情報収集は大事ですね。アプリケーションのエラーを Slack に通知していたKurbernetes のメトリクスを Datadog で確認できる状態だったコンテナのログを一元的に Datadog Logs  で閲覧できるようにしていた他の人の技術ブログから Kubernetes の CoreDNS が不安定になることを知っていたこの記事に間違っている内容や、もっと良い改善方法がある事をご存知の方がいましたら、優しく教えてください。おまけ現在、ソーシャルPLUS では作りたい機能が山ほどある状況でまだまだ成長するサービスになると思うので、成長を続けるサービスに携わりたいエンジニアやデザイナーのご応募をお待ちしております！サイトにはまだないですが、インフラエンジニアも近いうちに募集をする事にはなると思います。open.talentio.comopen.talentio.comopen.talentio.comフィードフォース の他のサービスもエンジニアを募集してますので、興味があればご応募お待ちしております！engineers.recruit.feedforce.jp","link":"https://developer.feedforce.jp/entry/2021/09/02/134725","isoDate":"2021-09-02T04:47:25.000Z","dateMiliSeconds":1630558045000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"feedforce"},{"title":"Firestore エミュレーターを使ったテスト同士の競合が起きないようにしていい感じにテストできるようにした話","content":"<p>こんにちは、エンジニアの <a href=\"http://blog.hatena.ne.jp/len_prog/\" class=\"hatena-id-icon\"><img src=\"https://cdn.profile-image.st-hatena.com/users/len_prog/profile.png\" width=\"16\" height=\"16\" alt=\"\" class=\"hatena-id-icon\">id:len_prog</a> です。</p>\n\n<p>私が所属している <a href=\"https://ecbooster.jp/\">EC Booster</a> チームでは、「<a href=\"https://support.ecbooster.jp/ja/articles/4854572-%E3%82%AB%E3%82%A4%E3%82%BC%E3%83%B3%E3%82%AB%E3%83%BC%E3%83%89%E3%81%AE%E6%A6%82%E8%A6%81%E3%81%A8%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\">カイゼンカード</a>」機能の開発に Firebase を採用しています。<br />\nその中でも特に Cloud Functions for Firebase と Cloud Firestore をメインで使用しており、これらの採用により短い開発期間で機能をリリースすることができました 🎉</p>\n\n<p>しかし、Firebase を採用したことで苦労したことが全く無かったわけではありません。<br />\n特に、テスト周りはインターネット上にもあまり情報が多くない状況で、色々ハマりながら開発をしてきました。</p>\n\n<p>そこで、今回の記事では、いくつかあったハマりごとの中でも特に厄介だったものについて対策を書いていきます。</p>\n\n<h1>Firestore Emulator のプロジェクト共有時のデータ競合</h1>\n\n<p><a href=\"https://firebase.google.com/docs/emulator-suite?hl=ja\">Firebase Local Emulator Suite</a> を使って Firestore に接続するテストを書いていた際に、<br />\nテストを単体で実行した場合には通るのに、他のテストと並列に実行した場合のみドキュメントの状態が予期せぬものになりテストが落ちてしまうことに悩まされました。</p>\n\n<p>調査の結果、これは、接続先プロジェクトがすべてのテストで同じになってしまっているのが原因ということが分かりました。</p>\n\n<p>この状態で同じドキュメントを書き換えるテストが並列で走ってしまった場合、実行タイミングによってはドキュメントが予期せぬ状態になってしまいます。<br />\nまた、テスト結果が不安定だとテストが信用できず、実装を保証するものになりません。</p>\n\n<p>このままでは役に立つテストが書けないと思い試行錯誤した結果、<strong>テストごとに違うプロジェクトの Firestore に接続する</strong>ことでそれぞれのテストが独立した状態で実行でき、結果としてデータ競合が防げることが分かりました。</p>\n\n<p>以下、サンプルアプリケーションを用いてこの方法について書いていきます。</p>\n\n<h1>サンプルアプリケーションの概要</h1>\n\n<p>今回は、サンプルとして簡易的な RPG を開発することを想定します。<br />\nゲームに登場するキャラクターは、以下のような構造のドキュメントを持つ <code>characters</code> コレクションで管理されています。</p>\n\n<pre class=\"code lang-typescript\" data-lang=\"typescript\" data-unlink><span class=\"synIdentifier\">{</span>\n  name: <span class=\"synType\">string</span><span class=\"synStatement\">;</span>\n  level: <span class=\"synType\">number</span><span class=\"synStatement\">;</span>\n  job: <span class=\"synType\">string</span><span class=\"synStatement\">;</span>\n<span class=\"synIdentifier\">}</span>\n</pre>\n\n\n<p>また、このゲームでは以下の行動のみが可能と仮定します(これだけじゃゲームとして成り立たないと思いますが、簡単のためということでお許しください)</p>\n\n<ul>\n<li>キャラクターは、レベルアップすることができる</li>\n<li>キャラクターは、転職することができる\n\n<ul>\n<li>転職すると、キャラクターのレベルが1に戻る</li>\n</ul>\n</li>\n</ul>\n\n\n<p>なお、アプリケーション上においてキャラクターのレベルアップは、<code>characterLevelUpUseCase</code>、キャラクターの転職は <code>characterJobChangeUseCase</code> という関数を呼ぶことで行えることとします。</p>\n\n<p>ここからは、実際にこれら2つの関数のテストコードが競合する様子を見ていきます。</p>\n\n<h1>データ競合発生時の構成</h1>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/l/len_prog/20210624/20210624165133.png\" alt=\"f:id:len_prog:20210624165133p:plain:w500\" width=\"1200\" height=\"790\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" style=\"width:500px\" itemprop=\"image\"></span></p>\n\n<p><code>characterJobChangeUseCase</code> と <code>characterLevelUpUseCase</code> が <code>my-game</code> プロジェクトの Firestore を共有してしまっています。<br />\nこの状態で両方の関数から同じドキュメントを書き換えてしまった場合、データ競合が発生する可能性があります。<br />\nこの場合、実際のコードは以下のようになります。</p>\n\n<pre class=\"code lang-typescript\" data-lang=\"typescript\" data-unlink><span class=\"synComment\">// functions/src/usecases/characterJobChangeUseCase.spec.ts</span>\n<span class=\"synStatement\">import</span> * <span class=\"synStatement\">as</span> admin <span class=\"synStatement\">from</span> <span class=\"synConstant\">&quot;firebase-admin&quot;</span><span class=\"synStatement\">;</span>\n<span class=\"synStatement\">import</span> <span class=\"synIdentifier\">{</span> characterJobChangeUseCase <span class=\"synIdentifier\">}</span> <span class=\"synStatement\">from</span> <span class=\"synConstant\">&quot;@/usecases/characterJobChangeUseCase&quot;</span><span class=\"synStatement\">;</span>\n\nadmin.initializeApp<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n  projectId: <span class=\"synConstant\">&quot;my-game&quot;</span><span class=\"synStatement\">,</span> <span class=\"synComment\">// ここが問題</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n<span class=\"synStatement\">const</span> charactersCollection <span class=\"synStatement\">=</span> admin\n  .firestore<span class=\"synStatement\">()</span>\n  .collection<span class=\"synStatement\">(</span><span class=\"synConstant\">&quot;characters&quot;</span><span class=\"synStatement\">);</span>\n\ndescribe<span class=\"synStatement\">(</span>characterJobChangeUseCase<span class=\"synStatement\">,</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n  <span class=\"synStatement\">const</span> targetCharacterId <span class=\"synStatement\">=</span> <span class=\"synConstant\">&quot;target-character-id&quot;</span><span class=\"synStatement\">;</span>\n\n  beforeEach<span class=\"synStatement\">(async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.set<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n        name: <span class=\"synConstant\">&quot;アルス&quot;</span><span class=\"synStatement\">,</span>\n        level: <span class=\"synConstant\">10</span><span class=\"synStatement\">,</span>\n        job: <span class=\"synConstant\">&quot;すっぴん&quot;</span><span class=\"synStatement\">;</span>\n    <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n  afterEach<span class=\"synStatement\">(async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.<span class=\"synStatement\">delete();</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n  it<span class=\"synStatement\">(</span><span class=\"synConstant\">&quot;キャラクターが転職した場合、レベルが1に戻ること&quot;</span><span class=\"synStatement\">,</span> <span class=\"synStatement\">async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> characterJobChangeUseCase<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">);</span> <span class=\"synComment\">// characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1に戻る</span>\n    <span class=\"synStatement\">const</span> jobChangedCharacter <span class=\"synStatement\">=</span> <span class=\"synStatement\">(await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.get<span class=\"synStatement\">())</span>.data<span class=\"synStatement\">();</span>\n\n    expect<span class=\"synStatement\">(</span>jobChangedCharacter.level<span class=\"synStatement\">)</span>.toBe<span class=\"synStatement\">(</span><span class=\"synConstant\">1</span><span class=\"synStatement\">);</span> <span class=\"synComment\">// 実行タイミング次第では、1になるはずが11になってしまう！</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n</pre>\n\n\n\n\n<pre class=\"code lang-typescript\" data-lang=\"typescript\" data-unlink><span class=\"synComment\">// functions/src/usecases/characterLevelUpUseCase.spec.ts</span>\n<span class=\"synStatement\">import</span> * <span class=\"synStatement\">as</span> admin <span class=\"synStatement\">from</span> <span class=\"synConstant\">&quot;firebase-admin&quot;</span><span class=\"synStatement\">;</span>\n<span class=\"synStatement\">import</span> <span class=\"synIdentifier\">{</span> characterLevelUpUseCase <span class=\"synIdentifier\">}</span> <span class=\"synStatement\">from</span> <span class=\"synConstant\">&quot;@/usecases/characterLevelUpUseCase&quot;</span><span class=\"synStatement\">;</span>\n\nadmin.initializeApp<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n  projectId: <span class=\"synConstant\">&quot;my-game&quot;</span><span class=\"synStatement\">,</span> <span class=\"synComment\">// ここが問題</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n<span class=\"synStatement\">const</span> charactersCollection <span class=\"synStatement\">=</span> admin\n  .firestore<span class=\"synStatement\">()</span>\n  .collection<span class=\"synStatement\">(</span><span class=\"synConstant\">&quot;characters&quot;</span><span class=\"synStatement\">);</span>\n\ndescribe<span class=\"synStatement\">(</span>characterLevelUpUseCase<span class=\"synStatement\">,</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n  <span class=\"synStatement\">const</span> targetCharacterId <span class=\"synStatement\">=</span> <span class=\"synConstant\">&quot;target-character-id&quot;</span><span class=\"synStatement\">;</span>\n\n  beforeEach<span class=\"synStatement\">(async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.set<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n        name: <span class=\"synConstant\">&quot;アルス&quot;</span><span class=\"synStatement\">,</span>\n        level: <span class=\"synConstant\">10</span><span class=\"synStatement\">,</span>\n        job: <span class=\"synConstant\">&quot;すっぴん&quot;</span><span class=\"synStatement\">;</span>\n    <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n  afterEach<span class=\"synStatement\">(async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.<span class=\"synStatement\">delete();</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n  it<span class=\"synStatement\">(</span><span class=\"synConstant\">&quot;キャラクターがレベルアップした場合、レベルが1上がること&quot;</span><span class=\"synStatement\">,</span> <span class=\"synStatement\">async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> characterLevelUpUseCase<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">);</span> <span class=\"synComment\">// characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1上がる</span>\n    <span class=\"synStatement\">const</span> grownCharacter <span class=\"synStatement\">=</span> <span class=\"synStatement\">(await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.get<span class=\"synStatement\">())</span>.data<span class=\"synStatement\">();</span>\n\n    expect<span class=\"synStatement\">(</span>grownCharacter.level<span class=\"synStatement\">)</span>.toBe<span class=\"synStatement\">(</span><span class=\"synConstant\">11</span><span class=\"synStatement\">);</span> <span class=\"synComment\">// 実行タイミング次第では、11になるはずが1に戻ってしまう！</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n</pre>\n\n\n<p>見ての通り、両方のテストが <code>my-game</code> プロジェクトの Firestore の、ID: <code>target-character-id</code> のドキュメントを更新してしまっています。<br />\nこれらのテストコードを並列で実行した場合、<strong>キャラクターが転職したのにレベルが1に戻らない</strong>、<strong>キャラクターがレベルアップしたはずなのになぜかレベル1に戻ってしまう</strong>など予期せぬ状態になってしまい、\nテストが落ちてしまう可能性があります。</p>\n\n<p>この状態ではテストコードが信用できないので、テストごとに向き先プロジェクトを変えてこの問題を解決していきます。</p>\n\n<h1>データ競合解決後の構成</h1>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/l/len_prog/20210705/20210705113933.png\" alt=\"f:id:len_prog:20210705113933p:plain:w500\" width=\"1200\" height=\"779\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" style=\"width:500px\" itemprop=\"image\"></span></p>\n\n<p>上図②③のようにテストごとに接続先プロジェクトを独立させることで、他のテストとの並列実行が原因のデータ競合を防ぐことができます。<br />\n具体的には、以下のように <code>admin.initializeApp()</code>の第一引数に他のテストと重複しないプロジェクトID を渡すようにします。</p>\n\n<pre class=\"code lang-typescript\" data-lang=\"typescript\" data-unlink><span class=\"synComment\">// functions/src/usecases/characterJobChangeUseCase.spec.ts</span>\n\nadmin.initializeApp<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n  projectId: <span class=\"synConstant\">&quot;character-job-change-use-case-spec&quot;</span><span class=\"synStatement\">,</span> <span class=\"synComment\">//  図の②に対応</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n<span class=\"synComment\">// functions/src/usecases/characterLevelUpUseCase.spec.ts</span>\n\nadmin.initializeApp<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n  projectId: <span class=\"synConstant\">&quot;character-level-up-use-case-spec&quot;</span><span class=\"synStatement\">,</span> <span class=\"synComment\">// 図の③に対応</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n</pre>\n\n\n<p>変更後のコードの全体像は以下のようになります。</p>\n\n<pre class=\"code lang-typescript\" data-lang=\"typescript\" data-unlink><span class=\"synComment\">// functions/src/usecases/characterJobChangeUseCase.spec.ts</span>\n<span class=\"synStatement\">import</span> * <span class=\"synStatement\">as</span> admin <span class=\"synStatement\">from</span> <span class=\"synConstant\">&quot;firebase-admin&quot;</span><span class=\"synStatement\">;</span>\n<span class=\"synStatement\">import</span> <span class=\"synIdentifier\">{</span> characterJobChangeUseCase <span class=\"synIdentifier\">}</span> <span class=\"synStatement\">from</span> <span class=\"synConstant\">&quot;@/usecases/characterJobChangeUseCase&quot;</span><span class=\"synStatement\">;</span>\n\nadmin.initializeApp<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n  projectId: <span class=\"synConstant\">&quot;character-job-change-use-case-spec&quot;</span><span class=\"synStatement\">,</span> <span class=\"synComment\">//  図の②に対応</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n<span class=\"synComment\">// ここから下は構成変更前のコードと同じ</span>\n\n<span class=\"synStatement\">const</span> charactersCollection <span class=\"synStatement\">=</span> admin\n  .firestore<span class=\"synStatement\">()</span>\n  .collection<span class=\"synStatement\">(</span><span class=\"synConstant\">&quot;characters&quot;</span><span class=\"synStatement\">);</span>\n\ndescribe<span class=\"synStatement\">(</span>characterJobChangeUseCase<span class=\"synStatement\">,</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n  <span class=\"synStatement\">const</span> targetCharacterId <span class=\"synStatement\">=</span> <span class=\"synConstant\">&quot;target-character-id&quot;</span><span class=\"synStatement\">;</span>\n\n  beforeEach<span class=\"synStatement\">(async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.set<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n        name: <span class=\"synConstant\">&quot;アルス&quot;</span><span class=\"synStatement\">,</span>\n        level: <span class=\"synConstant\">10</span><span class=\"synStatement\">,</span>\n        job: <span class=\"synConstant\">&quot;すっぴん&quot;</span><span class=\"synStatement\">;</span>\n    <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n  afterEach<span class=\"synStatement\">(async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.<span class=\"synStatement\">delete();</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n  it<span class=\"synStatement\">(</span><span class=\"synConstant\">&quot;キャラクターが転職した場合、レベルが1に戻ること&quot;</span><span class=\"synStatement\">,</span> <span class=\"synStatement\">async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> characterJobChangeUseCase<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">);</span> <span class=\"synComment\">// characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1に戻る</span>\n    <span class=\"synStatement\">const</span> jobChangedCharacter <span class=\"synStatement\">=</span> <span class=\"synStatement\">(await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.get<span class=\"synStatement\">())</span>.data<span class=\"synStatement\">();</span>\n\n    expect<span class=\"synStatement\">(</span>jobChangedCharacter.level<span class=\"synStatement\">)</span>.toBe<span class=\"synStatement\">(</span><span class=\"synConstant\">1</span><span class=\"synStatement\">);</span> <span class=\"synComment\">// 転職するとレベルが1に戻ることを検証できるようになった</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n</pre>\n\n\n\n\n<pre class=\"code lang-typescript\" data-lang=\"typescript\" data-unlink><span class=\"synComment\">// functions/src/usecases/characterLevelUpUseCase.spec.ts</span>\n<span class=\"synStatement\">import</span> * <span class=\"synStatement\">as</span> admin <span class=\"synStatement\">from</span> <span class=\"synConstant\">&quot;firebase-admin&quot;</span><span class=\"synStatement\">;</span>\n<span class=\"synStatement\">import</span> <span class=\"synIdentifier\">{</span> characterLevelUpUseCase <span class=\"synIdentifier\">}</span> <span class=\"synStatement\">from</span> <span class=\"synConstant\">&quot;@/usecases/characterLevelUpUseCase&quot;</span><span class=\"synStatement\">;</span>\n\nadmin.initializeApp<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n  projectId: <span class=\"synConstant\">&quot;character-level-up-use-case-spec&quot;</span><span class=\"synStatement\">,</span> <span class=\"synComment\">// 図の③に対応</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n<span class=\"synComment\">// ここから下は構成変更前のコードと同じ</span>\n\n<span class=\"synStatement\">const</span> charactersCollection <span class=\"synStatement\">=</span> admin\n  .firestore<span class=\"synStatement\">()</span>\n  .collection<span class=\"synStatement\">(</span><span class=\"synConstant\">&quot;characters&quot;</span><span class=\"synStatement\">);</span>\n\ndescribe<span class=\"synStatement\">(</span>characterLevelUpUseCase<span class=\"synStatement\">,</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n  <span class=\"synStatement\">const</span> targetCharacterId <span class=\"synStatement\">=</span> <span class=\"synConstant\">&quot;target-character-id&quot;</span><span class=\"synStatement\">;</span>\n\n  beforeEach<span class=\"synStatement\">(async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.set<span class=\"synStatement\">(</span><span class=\"synIdentifier\">{</span>\n        name: <span class=\"synConstant\">&quot;アルス&quot;</span><span class=\"synStatement\">,</span>\n        level: <span class=\"synConstant\">10</span><span class=\"synStatement\">,</span>\n        job: <span class=\"synConstant\">&quot;すっぴん&quot;</span><span class=\"synStatement\">;</span>\n    <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n  afterEach<span class=\"synStatement\">(async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.<span class=\"synStatement\">delete();</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n\n  it<span class=\"synStatement\">(</span><span class=\"synConstant\">&quot;キャラクターがレベルアップした場合、レベルが1上がること&quot;</span><span class=\"synStatement\">,</span> <span class=\"synStatement\">async</span> <span class=\"synStatement\">()</span> <span class=\"synStatement\">=&gt;</span> <span class=\"synIdentifier\">{</span>\n    <span class=\"synStatement\">await</span> characterLevelUpUseCase<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">);</span> <span class=\"synComment\">// characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1上がる</span>\n    <span class=\"synStatement\">const</span> grownCharacter <span class=\"synStatement\">=</span> <span class=\"synStatement\">(await</span> charactersCollection.doc<span class=\"synStatement\">(</span>targetCharacterId<span class=\"synStatement\">)</span>.get<span class=\"synStatement\">())</span>.data<span class=\"synStatement\">();</span>\n\n    expect<span class=\"synStatement\">(</span>grownCharacter.level<span class=\"synStatement\">)</span>.toBe<span class=\"synStatement\">(</span><span class=\"synConstant\">11</span><span class=\"synStatement\">);</span> <span class=\"synComment\">// レベルアップした場合にレベルが1上がることを検証できるようになった</span>\n  <span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n<span class=\"synIdentifier\">}</span><span class=\"synStatement\">);</span>\n</pre>\n\n\n<p>このようにテストごとに向き先プロジェクトを変えることで、それぞれのテストで担保したいことをちゃんと担保できるようになります。</p>\n\n<h1>ちょっと微妙な点</h1>\n\n<p>上記の方法でテストごとに独立した環境の Firestore を操作できるようになり、データ競合を防げるようになりました。</p>\n\n<p>しかし、この方法にはひとつだけ微妙な点があります。<br />\n問題の説明のために、先程掲載した<code>競合解決後の構成図</code>を再掲します。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/l/len_prog/20210705/20210705113933.png\" alt=\"f:id:len_prog:20210705113933p:plain:w500\" width=\"1200\" height=\"779\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" style=\"width:500px\" itemprop=\"image\"></span></p>\n\n<p>上図①の接続先は、<code>$ firebase use</code> で指定したプロジェクトか、<code>$ firebase emulators:start</code> に <code>--project</code>を渡した場合にはそのプロジェクトになり、そのほかの方法で変えることは今のところできないようです。</p>\n\n<p>そのため、プロジェクトをテストごとに分けた場合、上図②③のテスト中にテスト自体は動くものの、Firebase Emulator の UI からデータの内容を見ることはできなくなります。</p>\n\n<p>一応、接続先を <code>$ firebase use</code> で指定しているものに切り替えるようコードを書き換えたりすればデバッグはできますが、\nいちいち書き換えの手間が生じるので若干面倒です。</p>\n\n<p>また、これは Firebase Enulator の UI で立ち上がっているすべてのプロジェクトの Firestore を見られるようになれば解決する問題ではあり、実際に <a href=\"https://github.com/firebase/firebase-tools-ui\">firebase/firebase-tools-ui</a> リポジトリに <a href=\"https://github.com/firebase/firebase-tools-ui/issues/281\">issue</a> も立っていますが、すぐに対応が終わりそうには見えない状況なので、しばらくは不便な状況が続くことが予想されます。</p>\n\n<h1>所感</h1>\n\n<p>Firebase は便利ですが、当然ながら全くハマらずに開発できる銀の弾丸ではないですね。<br />\nしかし、基本的には便利でドキュメントもそれなりに読みやすく、個人的には使っていて満足感があります。</p>\n\n<p>今後も日々の開発で得た Firebase や GCP 周りの TIPS を書いていけたらと思っておりますので、よろしくお願いいたします 🙏</p>\n","contentSnippet":"こんにちは、エンジニアの id:len_prog です。私が所属している EC Booster チームでは、「カイゼンカード」機能の開発に Firebase を採用しています。しかし、Firebase を採用したことで苦労したことが全く無かったわけではありません。そこで、今回の記事では、いくつかあったハマりごとの中でも特に厄介だったものについて対策を書いていきます。Firestore Emulator のプロジェクト共有時のデータ競合Firebase Local Emulator Suite を使って Firestore に接続するテストを書いていた際に、調査の結果、これは、接続先プロジェクトがすべてのテストで同じになってしまっているのが原因ということが分かりました。この状態で同じドキュメントを書き換えるテストが並列で走ってしまった場合、実行タイミングによってはドキュメントが予期せぬ状態になってしまいます。このままでは役に立つテストが書けないと思い試行錯誤した結果、テストごとに違うプロジェクトの Firestore に接続することでそれぞれのテストが独立した状態で実行でき、結果としてデータ競合が防げることが分かりました。以下、サンプルアプリケーションを用いてこの方法について書いていきます。サンプルアプリケーションの概要今回は、サンプルとして簡易的な RPG を開発することを想定します。characters コレクションで管理されています。{  name: string;  level: number;  job: string;}また、このゲームでは以下の行動のみが可能と仮定します(これだけじゃゲームとして成り立たないと思いますが、簡単のためということでお許しください)キャラクターは、レベルアップすることができるキャラクターは、転職することができる転職すると、キャラクターのレベルが1に戻るなお、アプリケーション上においてキャラクターのレベルアップは、characterLevelUpUseCase、キャラクターの転職は characterJobChangeUseCase という関数を呼ぶことで行えることとします。ここからは、実際にこれら2つの関数のテストコードが競合する様子を見ていきます。データ競合発生時の構成characterJobChangeUseCase と characterLevelUpUseCase が my-game プロジェクトの Firestore を共有してしまっています。// functions/src/usecases/characterJobChangeUseCase.spec.tsimport * as admin from \"firebase-admin\";import { characterJobChangeUseCase } from \"@/usecases/characterJobChangeUseCase\";admin.initializeApp({  projectId: \"my-game\", // ここが問題});const charactersCollection = admin  .firestore()  .collection(\"characters\");describe(characterJobChangeUseCase, () => {  const targetCharacterId = \"target-character-id\";  beforeEach(async () => {    await charactersCollection.doc(targetCharacterId).set({        name: \"アルス\",        level: 10,        job: \"すっぴん\";    });  });  afterEach(async () => {    await charactersCollection.doc(targetCharacterId).delete();  });  it(\"キャラクターが転職した場合、レベルが1に戻ること\", async () => {    await characterJobChangeUseCase(targetCharacterId); // characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1に戻る    const jobChangedCharacter = (await charactersCollection.doc(targetCharacterId).get()).data();    expect(jobChangedCharacter.level).toBe(1); // 実行タイミング次第では、1になるはずが11になってしまう！  });});// functions/src/usecases/characterLevelUpUseCase.spec.tsimport * as admin from \"firebase-admin\";import { characterLevelUpUseCase } from \"@/usecases/characterLevelUpUseCase\";admin.initializeApp({  projectId: \"my-game\", // ここが問題});const charactersCollection = admin  .firestore()  .collection(\"characters\");describe(characterLevelUpUseCase, () => {  const targetCharacterId = \"target-character-id\";  beforeEach(async () => {    await charactersCollection.doc(targetCharacterId).set({        name: \"アルス\",        level: 10,        job: \"すっぴん\";    });  });  afterEach(async () => {    await charactersCollection.doc(targetCharacterId).delete();  });  it(\"キャラクターがレベルアップした場合、レベルが1上がること\", async () => {    await characterLevelUpUseCase(targetCharacterId); // characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1上がる    const grownCharacter = (await charactersCollection.doc(targetCharacterId).get()).data();    expect(grownCharacter.level).toBe(11); // 実行タイミング次第では、11になるはずが1に戻ってしまう！  });});見ての通り、両方のテストが my-game プロジェクトの Firestore の、ID: target-character-id のドキュメントを更新してしまっています。キャラクターが転職したのにレベルが1に戻らない、キャラクターがレベルアップしたはずなのになぜかレベル1に戻ってしまうなど予期せぬ状態になってしまい、テストが落ちてしまう可能性があります。この状態ではテストコードが信用できないので、テストごとに向き先プロジェクトを変えてこの問題を解決していきます。データ競合解決後の構成上図②③のようにテストごとに接続先プロジェクトを独立させることで、他のテストとの並列実行が原因のデータ競合を防ぐことができます。admin.initializeApp()の第一引数に他のテストと重複しないプロジェクトID を渡すようにします。// functions/src/usecases/characterJobChangeUseCase.spec.tsadmin.initializeApp({  projectId: \"character-job-change-use-case-spec\", //  図の②に対応});// functions/src/usecases/characterLevelUpUseCase.spec.tsadmin.initializeApp({  projectId: \"character-level-up-use-case-spec\", // 図の③に対応});変更後のコードの全体像は以下のようになります。// functions/src/usecases/characterJobChangeUseCase.spec.tsimport * as admin from \"firebase-admin\";import { characterJobChangeUseCase } from \"@/usecases/characterJobChangeUseCase\";admin.initializeApp({  projectId: \"character-job-change-use-case-spec\", //  図の②に対応});// ここから下は構成変更前のコードと同じconst charactersCollection = admin  .firestore()  .collection(\"characters\");describe(characterJobChangeUseCase, () => {  const targetCharacterId = \"target-character-id\";  beforeEach(async () => {    await charactersCollection.doc(targetCharacterId).set({        name: \"アルス\",        level: 10,        job: \"すっぴん\";    });  });  afterEach(async () => {    await charactersCollection.doc(targetCharacterId).delete();  });  it(\"キャラクターが転職した場合、レベルが1に戻ること\", async () => {    await characterJobChangeUseCase(targetCharacterId); // characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1に戻る    const jobChangedCharacter = (await charactersCollection.doc(targetCharacterId).get()).data();    expect(jobChangedCharacter.level).toBe(1); // 転職するとレベルが1に戻ることを検証できるようになった  });});// functions/src/usecases/characterLevelUpUseCase.spec.tsimport * as admin from \"firebase-admin\";import { characterLevelUpUseCase } from \"@/usecases/characterLevelUpUseCase\";admin.initializeApp({  projectId: \"character-level-up-use-case-spec\", // 図の③に対応});// ここから下は構成変更前のコードと同じconst charactersCollection = admin  .firestore()  .collection(\"characters\");describe(characterLevelUpUseCase, () => {  const targetCharacterId = \"target-character-id\";  beforeEach(async () => {    await charactersCollection.doc(targetCharacterId).set({        name: \"アルス\",        level: 10,        job: \"すっぴん\";    });  });  afterEach(async () => {    await charactersCollection.doc(targetCharacterId).delete();  });  it(\"キャラクターがレベルアップした場合、レベルが1上がること\", async () => {    await characterLevelUpUseCase(targetCharacterId); // characterJobChangeUsecase#handle に渡された引数の ID を持つユーザーのレベルが1上がる    const grownCharacter = (await charactersCollection.doc(targetCharacterId).get()).data();    expect(grownCharacter.level).toBe(11); // レベルアップした場合にレベルが1上がることを検証できるようになった  });});このようにテストごとに向き先プロジェクトを変えることで、それぞれのテストで担保したいことをちゃんと担保できるようになります。ちょっと微妙な点上記の方法でテストごとに独立した環境の Firestore を操作できるようになり、データ競合を防げるようになりました。しかし、この方法にはひとつだけ微妙な点があります。競合解決後の構成図を再掲します。上図①の接続先は、$ firebase use で指定したプロジェクトか、$ firebase emulators:start に --projectを渡した場合にはそのプロジェクトになり、そのほかの方法で変えることは今のところできないようです。そのため、プロジェクトをテストごとに分けた場合、上図②③のテスト中にテスト自体は動くものの、Firebase Emulator の UI からデータの内容を見ることはできなくなります。一応、接続先を $ firebase use で指定しているものに切り替えるようコードを書き換えたりすればデバッグはできますが、いちいち書き換えの手間が生じるので若干面倒です。また、これは Firebase Enulator の UI で立ち上がっているすべてのプロジェクトの Firestore を見られるようになれば解決する問題ではあり、実際に firebase/firebase-tools-ui リポジトリに issue も立っていますが、すぐに対応が終わりそうには見えない状況なので、しばらくは不便な状況が続くことが予想されます。所感Firebase は便利ですが、当然ながら全くハマらずに開発できる銀の弾丸ではないですね。今後も日々の開発で得た Firebase や GCP 周りの TIPS を書いていけたらと思っておりますので、よろしくお願いいたします 🙏","link":"https://developer.feedforce.jp/entry/2021/07/07/103917","isoDate":"2021-07-07T01:39:17.000Z","dateMiliSeconds":1625621957000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/l/len_prog/20210705/20210705113933.png","authorName":"feedforce"},{"title":"【2021年夏】半期に1度の Engineer’s Principles Award 受賞者を紹介します","content":"<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/f/feedforce_recruit/20210611/20210611163915.jpg\" alt=\"f:id:feedforce_recruit:20210611163915j:plain\" width=\"1200\" height=\"700\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>こんにちは。人事の今岡と申します。\n2021年もあっという間に6月ですね。</p>\n\n<p>フィードフォースでは先日オンライン納会が開催され、半期に一度の「Engineer’s Principles Award 2021 Summer」の受賞者が発表されました。\n今回アワードを受賞した開発メンバーと表彰内容をご紹介します。</p>\n\n<p>前回の表彰者紹介はコチラ</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fdeveloper.feedforce.jp%2Fentry%2F2020%2F12%2F28%2F131042\" title=\"半期に1度の Engineer’s Principles Award 受賞者を紹介します - Feedforce Developer Blog\" class=\"embed-card embed-blogcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 190px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://developer.feedforce.jp/entry/2020/12/28/131042\">developer.feedforce.jp</a></cite></p>\n\n<h2>Engineer’s Principles Award とは</h2>\n\n<p>Engineer’s Principles とは、フィードフォースの開発メンバー向けに現場が主体となって設定した、5つの行動指針です。\n半期に一度、開発メンバー同士で投票を行い、行動指針の項目ごとに最も体現しているメンバーが選ばれ表彰されます。</p>\n\n<p>Engineer’s Principles についてはこちら</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fmedia.feedforce.jp%2Fn%2Fnd1f2236470b3\" title=\"フィードフォースが目指すエンジニア像とは。「Engineer’s Principles」を紹介します｜フィードフォースのnote\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://media.feedforce.jp/n/nd1f2236470b3\">media.feedforce.jp</a></cite></p>\n\n<h2>受賞者紹介</h2>\n\n<p>※表彰コメントは本来社内向けのものであるため一部変更させていただいています。受賞者によって各種アカウントを載せています。</p>\n\n<h3>🏆「Stay Humble; 常に謙虚であるべし」受賞者</h3>\n\n<h4>@len_prog さん</h4>\n\n<p>表彰コメント：<br />\n社内でメジャーな Rails 以外でバックエンドを実装する際に、なぜそうするのかという理由やレイヤーの切り方を他のメンバーにわかりやすく説明していました。\n一方、自分自身で苦手なことがあった場合に、他の人に相談したり、フィードバックを求めそれを受け入れる姿勢は、まさに Stay Humble だと思いました。</p>\n\n<p><i class=\"blogicon-twitter\"></i> <a href=\"https://twitter.com/len_prog\">Len (@len_prog)</a> , <i class=\"blogicon-entry\"></i> <a href=\"https://len-prog.hatenablog.com/\">Blog</a></p>\n\n<h4>@katsunn さん</h4>\n\n<p>表彰コメント：<br />\n事前に色々なアイデアを用意しつつも、相談の過程でお互いの認識や意図を踏まえたうえで改善を進めていく一方、ただ受け入れるだけではなく、\nプロフェッショナルとして自分なりに咀嚼したアウトプットにしていく姿勢が非常に素晴らしく、ベンチマークにすべきだと感じました。</p>\n\n<p><i class=\"blogicon-twitter\"></i>  <a href=\"https://twitter.com/nomo_017\">のもち(@nomo_017)</a></p>\n\n<h3>🏆「Be Positive &amp; Proactive; 常に肯定的・主体的であるべし」受賞者</h3>\n\n<h4>@sukechannnn さん</h4>\n\n<p>表彰コメント：<br />\nエンジニアとして様々なチームビルディングや開発手法を試しているだけではなく、ビジネス視点からも方向性を考え、\nプロダクトオーナーとしてプロダクトを成長させようとしている姿勢は、まさにこの言葉にぴったりだと思います。</p>\n\n<p><i class=\"blogicon-twitter\"></i> <a href=\"https://twitter.com/sukechannnn\"> sukechannnn (@sukechannnn)</a> , <i class=\"fa fa-github\" aria-hidden=\"true\"></i> <a href=\"https://github.com/sukechannnn\">sukechannnn</a></p>\n\n<h4>@daido1976 さん</h4>\n\n<p>表彰コメント：<br />\n分野を問わず新しいことに前向きに挑戦し、気になったことはどんどん質問するのに加え、\n育休中のメンバーに代わって、率先してチームを引っ張っている行動力が素晴らしいと思いました。</p>\n\n<p><i class=\"blogicon-twitter\"></i>  <a href=\"https://twitter.com/daido1976\">Daido Shota (@daido1976)</a> , <i class=\"fa fa-github\" aria-hidden=\"true\"></i>  <a href=\"https://github.com/daido1976\"> daido1976</a></p>\n\n<h3>🏆「Be Prepared; 常に来たるべき機会に備えるべし」受賞者</h3>\n\n<h4>@daido1976 さん</h4>\n\n<p>表彰コメント：<br />\n自分のキャリアや目指すべき方向を踏まえつつ、常にアンテナを立てて知識を広く持とうとしている姿勢がよいと感じています。\nさらに、そうして蓄積したスキルを開発だけではなく、自ら手を挙げ講師をつとめた新卒向け Web 研修にも活かしている点がまさに Be Prepared だと思いました。</p>\n\n<p><i class=\"blogicon-twitter\"></i>  <a href=\"https://twitter.com/daido1976\">Daido Shota (@daido1976)</a> , <i class=\"fa fa-github\" aria-hidden=\"true\"></i>  <a href=\"https://github.com/daido1976\"> daido1976</a></p>\n\n<h4>@namikingsoft さん</h4>\n\n<p>表彰コメント：<br />\nOmni Hub の開発において、あまり開発経験がなかったはずの Rust を使いこなしつつ WAF を含めたインフラ構築をしていて、\n@namikingsoft さんの強みが発揮される局面でした。また dfplus.io でもパフォーマンス改善でコアな知識を活かすなど、まさにこれまでの準備の賜物だと思います。</p>\n\n<p><i class=\"fa fa-github\" aria-hidden=\"true\"></i> <a href=\"https://github.com/namikingsoft\">namikingsoft</a></p>\n\n<h3>🏆「Share All; 己の知見、試行、失敗、遍く共有すべし」受賞者</h3>\n\n<h4>@masutaka さん</h4>\n\n<p>表彰コメント：<br />\nLooker 導入において知見や失敗など社内共有しているほか、そもそも  esa にどう記録すべきかといった、「共有のための知見の共有」にまで配慮しています。\nSlack や esa 、Blog への共有はエンジニアのみならず、全社的にプラスの影響を与えていて、まさに共有の神様と言えるでしょう。</p>\n\n<p><i class=\"blogicon-twitter\"></i> <a href=\"https://twitter.com/masutaka\">Takashi Masuda (@masutaka)</a> , <i class=\"fa fa-github\" aria-hidden=\"true\"></i> <a href=\"https://github.com/masutaka\">masutaka</a> , <i class=\"blogicon-entry\"></i> <a href=\"https://masutaka.net/\">Blog</a></p>\n\n<h4>@kogai さん</h4>\n\n<p>表彰コメント：<br />\nShopify 周りでは、社内だけでなく社外に対してのプレゼンスを示しています。また Omni Hub の開発で多忙な中、\n社内勉強会 Rust の会では実際の新規事業のプロダクトコードを題材に実践的な知見を共有するなど、その共有力はフィードフォースエンジニアの鑑（かがみ）だと思います。</p>\n\n<p><i class=\"blogicon-twitter\"></i> <a href=\"https://twitter.com/iamchawan\">茶碗 (@iamchawan)</a> ,<i class=\"fa fa-github\" aria-hidden=\"true\"></i> <a href=\"https://github.com/kogai\">kogai</a> , <i class=\"blogicon-entry\"></i> <a href=\"https://k9bookshelf.com/blogs/development\">Blog</a></p>\n\n<h3>🏆「Just Do It; 全力でやりきるべし」受賞者</h3>\n\n<h4>@namikingsoftさん</h4>\n\n<p>表彰コメント：<br />\nOmni  Hub リリースまでの道筋をきちんと立ててスケジュール以上の速さで完走して去っていくその姿は、まさに Just Do It でした。</p>\n\n<p><i class=\"fa fa-github\" aria-hidden=\"true\"></i> <a href=\"https://github.com/namikingsoft\">namikingsoft</a></p>\n\n<h2>周囲の賞賛・承認を共有するよい機会に</h2>\n\n<p>以上、延べ9名の受賞者でした。</p>\n\n<p>表彰コメントは、<strong>開発メンバー同士の投票時に自由記述できるコメントがもとになっているので</strong>、周囲からの賞賛・承認の声を全社で共有できるよい機会となっています。</p>\n\n<p>前回に引き続き連続受賞しているメンバーもいますが、投票コメントには毎回違ったエピソードが集まっており、日ごろから継続的に実践をしているからこそ周りのエンジニアの目に留まるのだと感じました。</p>\n\n<p>受賞者のみなさん、おめでとうございました！</p>\n","contentSnippet":"こんにちは。人事の今岡と申します。2021年もあっという間に6月ですね。フィードフォースでは先日オンライン納会が開催され、半期に一度の「Engineer’s Principles Award 2021 Summer」の受賞者が発表されました。今回アワードを受賞した開発メンバーと表彰内容をご紹介します。前回の表彰者紹介はコチラdeveloper.feedforce.jpEngineer’s Principles Award とはEngineer’s Principles とは、フィードフォースの開発メンバー向けに現場が主体となって設定した、5つの行動指針です。半期に一度、開発メンバー同士で投票を行い、行動指針の項目ごとに最も体現しているメンバーが選ばれ表彰されます。Engineer’s Principles についてはこちらmedia.feedforce.jp受賞者紹介※表彰コメントは本来社内向けのものであるため一部変更させていただいています。受賞者によって各種アカウントを載せています。🏆「Stay Humble; 常に謙虚であるべし」受賞者@len_prog さん表彰コメント： Len (@len_prog) ,  Blog@katsunn さん表彰コメント：  のもち(@nomo_017)🏆「Be Positive & Proactive; 常に肯定的・主体的であるべし」受賞者@sukechannnn さん表彰コメント：  sukechannnn (@sukechannnn) ,  sukechannnn@daido1976 さん表彰コメント：  Daido Shota (@daido1976) ,    daido1976🏆「Be Prepared; 常に来たるべき機会に備えるべし」受賞者@daido1976 さん表彰コメント：  Daido Shota (@daido1976) ,    daido1976@namikingsoft さん表彰コメント： namikingsoft🏆「Share All; 己の知見、試行、失敗、遍く共有すべし」受賞者@masutaka さん表彰コメント： Takashi Masuda (@masutaka) ,  masutaka ,  Blog@kogai さん表彰コメント： 茶碗 (@iamchawan) , kogai ,  Blog🏆「Just Do It; 全力でやりきるべし」受賞者@namikingsoftさん表彰コメント： namikingsoft周囲の賞賛・承認を共有するよい機会に以上、延べ9名の受賞者でした。表彰コメントは、開発メンバー同士の投票時に自由記述できるコメントがもとになっているので、周囲からの賞賛・承認の声を全社で共有できるよい機会となっています。前回に引き続き連続受賞しているメンバーもいますが、投票コメントには毎回違ったエピソードが集まっており、日ごろから継続的に実践をしているからこそ周りのエンジニアの目に留まるのだと感じました。受賞者のみなさん、おめでとうございました！","link":"https://developer.feedforce.jp/entry/2021/06/11/164253","isoDate":"2021-06-11T07:42:53.000Z","dateMiliSeconds":1623397373000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"feedforce"},{"title":"プランニングの難しさを乗り越えて...スクラム開発が良い感じになった話","content":"<p>こんにちは。フィードフォースの <a href=\"https://ecbooster.jp/\">EC Booster</a> チームで開発（主にプロダクトオーナー）をしている <a href=\"https://twitter.com/sukechannnn\">@sukechannnn</a> です。元々ずっとバックエンドエンジニアでしたが、最近プロダクトオーナーをやるようになりました（理由はのちほど！）。</p>\n\n<p>昨年のアドベントカレンダーで <a href=\"https://developer.feedforce.jp/entry/2020/12/11/172338\">半年モブプロしたらチームが大きく成長した話</a> というブログを書いたのですが、2021年3月から <strong>モブプロを取り入れたスクラム開発</strong> をしています。それに伴って、\"モブプロ\" と \"個人タスク⇢レビュー\" の両軸で開発するようになりました（<a href=\"https://prtimes.jp/main/html/rd/p/000000040.000071307.html\">先日リリースしたカイゼンカード</a> はスクラムで開発しました）。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fdeveloper.feedforce.jp%2Fentry%2F2020%2F12%2F11%2F172338\" title=\"半年モブプロしたらチームが大きく成長した話 - Feedforce Developer Blog\" class=\"embed-card embed-blogcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 190px; max-width: 500px; margin: 10px 0px;\"></iframe></p>\n\n<p>今は良い感じに回っていますが、そうなるまでに色々と試行錯誤したので、そこで得た学びをお伝えできればと思います。全員リモートワークで開発するなら、モブプロを取り入れたスクラムはおすすめです！</p>\n\n<ul class=\"table-of-contents\">\n    <li><a href=\"#モブプロの良さと難しさ\">モブプロの良さと難しさ</a></li>\n    <li><a href=\"#そうだスクラムしよう\">そうだ、スクラムしよう！</a></li>\n    <li><a href=\"#プランニングが終わらない問題\">プランニングが終わらない問題</a></li>\n    <li><a href=\"#原因はissue-が散らかっていることだった\">原因は「issue が散らかっていること」だった</a></li>\n    <li><a href=\"#issue-をグルーピング優先順位はそれぞれで\">issue をグルーピング、優先順位はそれぞれで</a></li>\n    <li><a href=\"#まとめ\">まとめ</a></li>\n</ul>\n\n<h2 id=\"モブプロの良さと難しさ\">モブプロの良さと難しさ</h2>\n\n<p>モブプロ中心の開発を初めた当初は、以下の利点を感じていました。</p>\n\n<ul>\n<li>ドメイン知識の共有がしやすい</li>\n<li>コンテキストの共有がしやすい（\"何をどう作るか\" という議論もしやすい）</li>\n<li>レビューが要らない</li>\n<li>リモートワークでもさみしくない（だいじ）</li>\n</ul>\n\n\n<p>しばらくモブプロを続ける中で、開発メンバー全員がドメイン知識やフロント〜バックエンド全体の技術的な知識を共有している状態になりました。なので、なにか悩みがあってモブプロで共有すると「わかる〜」となるし、何より単純に仲良くなったと思います（ﾖｼｯ!!）。</p>\n\n<p>一方で、だんだんと <strong>モブプロだけ</strong> の開発が窮屈になってきました。</p>\n\n<ul>\n<li>知識の共有が進んできて \"全員でやらなくても良くない？\" というタスクが増えてきた</li>\n<li>個人でじっくり考えた方が良いタスクもあるのが分かった（新しい技術の調査、設計の見直しなど）</li>\n</ul>\n\n\n<p>これはチームが成長したことで出てきた嬉しい悩みなのですが、とはいえ完全にモブプロを辞めるのも上述したメリットを失いそうで怖い...。チーム全員で「今後どう開発していこう？」というのを話し合い、<strong>モブプロを取り入れたスクラム開発</strong> を試してみることにしました。</p>\n\n<h2 id=\"そうだスクラムしよう\">そうだ、スクラムしよう！</h2>\n\n<p>スクラム開発をしようと思ったのは、ストーリーポイント<a href=\"#f-9495249b\" name=\"fn-9495249b\" title=\"ストーリーポイント：プロダクトバックログ（タスク）を見積もるためにチームが使う単位で、前回の見積もりに対する相対評価を用いる\">*1</a>で見積もって <strong>ベロシティ<a href=\"#f-33d76d3d\" name=\"fn-33d76d3d\" title=\"ベロシティ：スプリントの期間でチームが届けることができる見積もり（ストーリーポイント）の合計のこと\">*2</a>を測りたい</strong> という別の目的もありました。</p>\n\n<p>モブプロで開発していると新機能のメイン開発は着実に進んでいくのですが、それ以外の細かいタスク（主に保守系）が見積もりづらい状況で、空いた時間にやるという形になってしまっていました（それ用に時間は設けていましたが）。</p>\n\n<p>モブプロ以外の個人タスクを計画的にやりたい、見積もりもしっかりやりたい、ということで、スクラムを導入することで、<strong>モブプロと個人開発のいいとこ取り</strong> をしようと考えました。</p>\n\n<ul>\n<li>新機能開発などのコンテキストの共有が重要なタスクは引き続きモブプロでやる\n\n<ul>\n<li>ストーリーポイントで見積もる</li>\n</ul>\n</li>\n<li>それ以外は個人タスクとして各自で進められるように、プランニングでしっかり整理する\n\n<ul>\n<li>個人タスクもストーリーポイントで見積もる</li>\n</ul>\n</li>\n<li>全てのタスクをストーリーポイントで見積もるのでベロシティが測れるようになる\n\n<ul>\n<li>振り返りで見積もりの精度を上げられる</li>\n</ul>\n</li>\n</ul>\n\n\n<p>めっちゃ良さそう...そう思っていざやってみたところ、１つ大きな壁にぶち当たってしまいました。</p>\n\n<h2 id=\"プランニングが終わらない問題\">プランニングが終わらない問題</h2>\n\n<p><a href=\"https://www.shoeisha.co.jp/book/detail/9784798130507\">エッセンシャルスクラム</a>にもある通り、１週間のプランニングに２時間以上かけるべきではありません。僕らは「１スプリント=１週間」で回しているため、２時間の予定で始めたプランニングですが、これが終わらない...。最初から何回かは４時間以上かかり、全員ヘトヘトになってしまいました。</p>\n\n<p>モブプロはプランニングが簡単です。全員やることが同じなので、基本的にタスクが直列で繋がっていきます。そのため「今スプリントはここから⇢ここまで」という感じで Sprint Backlog 的なものを決めることができました。</p>\n\n<p>しかし、スクラムの見積もりはもっと横断的なものです。単純に、今取り組んでいるものだけ見れば良いのではなく、これから取り組むものをたくさんある issue から選ぶ必要があります。そう、この <strong>たくさんある issue の中から今スプリントにやるタスクを選ぶこと</strong> に時間がかかってしまうのです。</p>\n\n<p>以前にもスクラム開発を試したことがあるのですが、その時もこれが原因でプランニングがとても大変でした。気にするトピックが多すぎてだんだん何について議論してるか分からなくなり、空中戦になってしまうんですよね...。</p>\n\n<p>その原因は、主に以下の２つでした。</p>\n\n<ul>\n<li>バックログの整理/管理に責任を持つ人（プロダクトオーナー<a href=\"#f-339964b7\" name=\"fn-339964b7\" title=\"プロダクトオーナー：プロダクトバックログの管理をする人で、優先順位を付けることに責任を持つ（１人の人間が務める、委員会ではない）\">*3</a>）がいなかった</li>\n<li>issue の数と種類が多く、バックログリファインメント<a href=\"#f-f2375d03\" name=\"fn-f2375d03\" title=\"バックログリファインメント：プランニングの前にプロダクトバックログを見直し、プランニング可能な状態にしておくこと\">*4</a>をしても整理しきれなかった</li>\n</ul>\n\n\n<p>プロダクトオーナー不在の問題は、元々それっぽいことをしていた僕が、改めてプロダクトオーナーやりますと手を上げ、バックログ管理の責任を持つことになりました。</p>\n\n<p>それでも、バックログリファインメントが上手く行かない問題は残っていました。リファインメントの概念は理解していて、しっかり時間も取っていたのに、いざプランニングをすると色々な issue を見すぎて伸びてしまう...。過去に何度も直面したこの問題に、改めて取り組むことにしました。</p>\n\n<h2 id=\"原因はissue-が散らかっていることだった\">原因は「issue が散らかっていること」だった</h2>\n\n<p>僕たちが開発している EC Booster は、ショッピング広告の自動運用やデータフィードの更新など、様々なジョブが裏で動いています。そのため、運用作業が日々発生し、運用の中で見つかる例外ケースやバグの修正が多々あります。\nまた、フロントエンドとバックエンドを全員が開発するため、１つのリポジトリで管理していることもあり、色々な種類の issue が１つのレーンに入り乱れてしまっていました。</p>\n\n<p>そのため、優先順位を付けるのも難しく、また「次スプリントで何をどこまでやるか？」を判断するのが難しくなってしまっていました。</p>\n\n<p>プロダクトバックログを整理しなければ、というのは分かっているのですが、スクラムに関する本やブログには整理の方法は書いてありません。どうやって整理したら分かりやすくなるかな...と考えていたところ、同僚が共有してくれた以下の記事が参考になりました。</p>\n\n<p><a href=\"https://note.com/gonjyu/n/nd7bf3efa0728\">エンジニア歴17年の俺が、事業系の開発タスクをバンバン投げてくる非エンジニアに、保守の必要性を死ぬほど分かりやすく説明する。</a></p>\n\n<p>この記事の中で「issueには \"種類\" がある」と言っていて、issue の種類別に整理された図が載っていました。これだ...！</p>\n\n<h2 id=\"issue-をグルーピング優先順位はそれぞれで\">issue をグルーピング、優先順位はそれぞれで</h2>\n\n<p>上記の記事を参考に、issue を <strong>新機能開発</strong>、<strong>バグ修正/運用改善</strong>、<strong>ライブラリーアップデート</strong> に分けて、それぞれのレーンで優先順位を付けるようにしました。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"issue をグルーピング、優先順位はそれぞれで\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sukechannnn/20210526/20210526215948.png\" alt=\"f:id:sukechannnn:20210526215948p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>issue をグルーピング、優先順位はそれぞれで</figcaption></figure></p>\n\n<p>issue の種類が同じなので、優先順位を付けるのは簡単です。さらに、スプリントバックログに入れるタスクを <strong>新機能開発：運用系 = ６：４</strong> の割合にする、という決めを作りました。さらに、何回かスプリントを回してベロシティも見えてきました。</p>\n\n<p>ここまで情報が揃うと <strong>次のスプリントで何をやるか決める基準</strong> ができてきます。</p>\n\n<p>そもそもの「次のプランニングでどの issue について話すか？」というのも、それぞれのレーンで優先順位が高い issue を６：４のバランスとベロシティを参考に選べるようになりました。<strong>プランニングの前</strong>にプロダクトオーナーが（開発チームと協力しながら）当たりを付けておくことで、プランニングで話すトピックを事前に共有できるようになり、開発メンバーそれぞれが事前に頭を整理しておくこともできるようになりました。</p>\n\n<p>これにより、プランニングがかなりスムーズに進むようになったので、いよいよスクラムが回り始めました。新機能開発はモブプロの同期的な開発で、それ以外のタスクは個人タスク⇢レビューという非同期な開発で進められるようになり、デリバリーの最大化を目指しつつ、個人の稼働率も上げられるようになりました。</p>\n\n<p><figure class=\"figure-image figure-image-fotolife\" title=\"GitHub Project を使ってタスク管理してる様子...横に長いんですが、情報が整理されてる方が優先順位を付けやすい\"><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sukechannnn/20210526/20210526212511.png\" alt=\"f:id:sukechannnn:20210526212511p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span><figcaption>GitHub Project を使ってタスク管理してる様子...横に長いんですが、情報が整理されてる方が優先順位を付けやすい</figcaption></figure></p>\n\n<h2 id=\"まとめ\">まとめ</h2>\n\n<p>issue をグルーピングしそれぞれで優先順位を付けたことで、プランニングが時間内に収まるようになっただけでなく、プランニングで話すトピックを絞ったことでより深い議論をすることができるようになりました。今は「モブプロを取り入れたスクラム」がとても良い感じに回っています！</p>\n\n<p>↓ EC Booster チームでの「スプリントの回し方」資料を公開しているので、気になった方はぜひ見てみてください！（もっとこうしたら良いよ！という助言などあれば頂けると嬉しいです！）</p>\n\n<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vTQY639rUAwDDtLfj_c9WbU1E0IlDSFzAbrP-XFCmbg8V_sNKPX_pCvKpiy50CQpS02nXvZnQHBb6JT/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>\n\n\n<p>こんな感じ開発している EC Booster ですが、ただ今 <strong>バックエンド（Ruby, Rails）が得意なエンジニアを猛烈に必要としています！！！</strong></p>\n\n<p>もしちょっっっとでも興味があれば、 <strong>僕とお話しましょう！</strong> 以下から気軽に応募してください！\n<a href=\"https://open.talentio.com/1/c/feedforce/requisitions/detail/19785\">https://open.talentio.com/1/c/feedforce/requisitions/detail/19785</a></p>\n\n<p>最後まで読んでいただき、ありがとうございました！</p>\n<div class=\"footnote\">\n<p class=\"footnote\"><a href=\"#fn-9495249b\" name=\"f-9495249b\" class=\"footnote-number\">*1</a><span class=\"footnote-delimiter\">:</span><span class=\"footnote-text\"><a href=\"https://www.ryuzee.com/contents/blog/3716\">ストーリーポイント</a>：プロダクトバックログ（タスク）を見積もるためにチームが使う単位で、前回の見積もりに対する相対評価を用いる</span></p>\n<p class=\"footnote\"><a href=\"#fn-33d76d3d\" name=\"f-33d76d3d\" class=\"footnote-number\">*2</a><span class=\"footnote-delimiter\">:</span><span class=\"footnote-text\"><a href=\"https://www.ryuzee.com/contents/blog/4802\">ベロシティ</a>：スプリントの期間でチームが届けることができる見積もり（ストーリーポイント）の合計のこと</span></p>\n<p class=\"footnote\"><a href=\"#fn-339964b7\" name=\"f-339964b7\" class=\"footnote-number\">*3</a><span class=\"footnote-delimiter\">:</span><span class=\"footnote-text\"><a href=\"https://www.ryuzee.com/contents/blog/7143\">プロダクトオーナー</a>：プロダクトバックログの管理をする人で、優先順位を付けることに責任を持つ（１人の人間が務める、委員会ではない）</span></p>\n<p class=\"footnote\"><a href=\"#fn-f2375d03\" name=\"f-f2375d03\" class=\"footnote-number\">*4</a><span class=\"footnote-delimiter\">:</span><span class=\"footnote-text\"><a href=\"https://www.ryuzee.com/contents/blog/5024\">バックログリファインメント</a>：プランニングの前にプロダクトバックログを見直し、プランニング可能な状態にしておくこと</span></p>\n</div>","contentSnippet":"こんにちは。フィードフォースの EC Booster チームで開発（主にプロダクトオーナー）をしている @sukechannnn です。元々ずっとバックエンドエンジニアでしたが、最近プロダクトオーナーをやるようになりました（理由はのちほど！）。昨年のアドベントカレンダーで 半年モブプロしたらチームが大きく成長した話 というブログを書いたのですが、2021年3月から モブプロを取り入れたスクラム開発 をしています。それに伴って、\"モブプロ\" と \"個人タスク⇢レビュー\" の両軸で開発するようになりました（先日リリースしたカイゼンカード はスクラムで開発しました）。今は良い感じに回っていますが、そうなるまでに色々と試行錯誤したので、そこで得た学びをお伝えできればと思います。全員リモートワークで開発するなら、モブプロを取り入れたスクラムはおすすめです！モブプロの良さと難しさそうだ、スクラムしよう！プランニングが終わらない問題原因は「issue が散らかっていること」だったissue をグルーピング、優先順位はそれぞれでまとめモブプロの良さと難しさモブプロ中心の開発を初めた当初は、以下の利点を感じていました。ドメイン知識の共有がしやすいコンテキストの共有がしやすい（\"何をどう作るか\" という議論もしやすい）レビューが要らないリモートワークでもさみしくない（だいじ）しばらくモブプロを続ける中で、開発メンバー全員がドメイン知識やフロント〜バックエンド全体の技術的な知識を共有している状態になりました。なので、なにか悩みがあってモブプロで共有すると「わかる〜」となるし、何より単純に仲良くなったと思います（ﾖｼｯ!!）。一方で、だんだんと モブプロだけ の開発が窮屈になってきました。知識の共有が進んできて \"全員でやらなくても良くない？\" というタスクが増えてきた個人でじっくり考えた方が良いタスクもあるのが分かった（新しい技術の調査、設計の見直しなど）これはチームが成長したことで出てきた嬉しい悩みなのですが、とはいえ完全にモブプロを辞めるのも上述したメリットを失いそうで怖い...。チーム全員で「今後どう開発していこう？」というのを話し合い、モブプロを取り入れたスクラム開発 を試してみることにしました。そうだ、スクラムしよう！スクラム開発をしようと思ったのは、ストーリーポイント*1で見積もって ベロシティ*2を測りたい という別の目的もありました。モブプロで開発していると新機能のメイン開発は着実に進んでいくのですが、それ以外の細かいタスク（主に保守系）が見積もりづらい状況で、空いた時間にやるという形になってしまっていました（それ用に時間は設けていましたが）。モブプロ以外の個人タスクを計画的にやりたい、見積もりもしっかりやりたい、ということで、スクラムを導入することで、モブプロと個人開発のいいとこ取り をしようと考えました。新機能開発などのコンテキストの共有が重要なタスクは引き続きモブプロでやるストーリーポイントで見積もるそれ以外は個人タスクとして各自で進められるように、プランニングでしっかり整理する個人タスクもストーリーポイントで見積もる全てのタスクをストーリーポイントで見積もるのでベロシティが測れるようになる振り返りで見積もりの精度を上げられるめっちゃ良さそう...そう思っていざやってみたところ、１つ大きな壁にぶち当たってしまいました。プランニングが終わらない問題エッセンシャルスクラムにもある通り、１週間のプランニングに２時間以上かけるべきではありません。僕らは「１スプリント=１週間」で回しているため、２時間の予定で始めたプランニングですが、これが終わらない...。最初から何回かは４時間以上かかり、全員ヘトヘトになってしまいました。モブプロはプランニングが簡単です。全員やることが同じなので、基本的にタスクが直列で繋がっていきます。そのため「今スプリントはここから⇢ここまで」という感じで Sprint Backlog 的なものを決めることができました。しかし、スクラムの見積もりはもっと横断的なものです。単純に、今取り組んでいるものだけ見れば良いのではなく、これから取り組むものをたくさんある issue から選ぶ必要があります。そう、この たくさんある issue の中から今スプリントにやるタスクを選ぶこと に時間がかかってしまうのです。以前にもスクラム開発を試したことがあるのですが、その時もこれが原因でプランニングがとても大変でした。気にするトピックが多すぎてだんだん何について議論してるか分からなくなり、空中戦になってしまうんですよね...。その原因は、主に以下の２つでした。バックログの整理/管理に責任を持つ人（プロダクトオーナー*3）がいなかったissue の数と種類が多く、バックログリファインメント*4をしても整理しきれなかったプロダクトオーナー不在の問題は、元々それっぽいことをしていた僕が、改めてプロダクトオーナーやりますと手を上げ、バックログ管理の責任を持つことになりました。それでも、バックログリファインメントが上手く行かない問題は残っていました。リファインメントの概念は理解していて、しっかり時間も取っていたのに、いざプランニングをすると色々な issue を見すぎて伸びてしまう...。過去に何度も直面したこの問題に、改めて取り組むことにしました。原因は「issue が散らかっていること」だった僕たちが開発している EC Booster は、ショッピング広告の自動運用やデータフィードの更新など、様々なジョブが裏で動いています。そのため、運用作業が日々発生し、運用の中で見つかる例外ケースやバグの修正が多々あります。また、フロントエンドとバックエンドを全員が開発するため、１つのリポジトリで管理していることもあり、色々な種類の issue が１つのレーンに入り乱れてしまっていました。そのため、優先順位を付けるのも難しく、また「次スプリントで何をどこまでやるか？」を判断するのが難しくなってしまっていました。プロダクトバックログを整理しなければ、というのは分かっているのですが、スクラムに関する本やブログには整理の方法は書いてありません。どうやって整理したら分かりやすくなるかな...と考えていたところ、同僚が共有してくれた以下の記事が参考になりました。エンジニア歴17年の俺が、事業系の開発タスクをバンバン投げてくる非エンジニアに、保守の必要性を死ぬほど分かりやすく説明する。この記事の中で「issueには \"種類\" がある」と言っていて、issue の種類別に整理された図が載っていました。これだ...！issue をグルーピング、優先順位はそれぞれで上記の記事を参考に、issue を 新機能開発、バグ修正/運用改善、ライブラリーアップデート に分けて、それぞれのレーンで優先順位を付けるようにしました。issue をグルーピング、優先順位はそれぞれでissue の種類が同じなので、優先順位を付けるのは簡単です。さらに、スプリントバックログに入れるタスクを 新機能開発：運用系 = ６：４ の割合にする、という決めを作りました。さらに、何回かスプリントを回してベロシティも見えてきました。ここまで情報が揃うと 次のスプリントで何をやるか決める基準 ができてきます。そもそもの「次のプランニングでどの issue について話すか？」というのも、それぞれのレーンで優先順位が高い issue を６：４のバランスとベロシティを参考に選べるようになりました。プランニングの前にプロダクトオーナーが（開発チームと協力しながら）当たりを付けておくことで、プランニングで話すトピックを事前に共有できるようになり、開発メンバーそれぞれが事前に頭を整理しておくこともできるようになりました。これにより、プランニングがかなりスムーズに進むようになったので、いよいよスクラムが回り始めました。新機能開発はモブプロの同期的な開発で、それ以外のタスクは個人タスク⇢レビューという非同期な開発で進められるようになり、デリバリーの最大化を目指しつつ、個人の稼働率も上げられるようになりました。GitHub Project を使ってタスク管理してる様子...横に長いんですが、情報が整理されてる方が優先順位を付けやすいまとめissue をグルーピングしそれぞれで優先順位を付けたことで、プランニングが時間内に収まるようになっただけでなく、プランニングで話すトピックを絞ったことでより深い議論をすることができるようになりました。今は「モブプロを取り入れたスクラム」がとても良い感じに回っています！↓ EC Booster チームでの「スプリントの回し方」資料を公開しているので、気になった方はぜひ見てみてください！（もっとこうしたら良いよ！という助言などあれば頂けると嬉しいです！）こんな感じ開発している EC Booster ですが、ただ今 バックエンド（Ruby, Rails）が得意なエンジニアを猛烈に必要としています！！！もしちょっっっとでも興味があれば、 僕とお話しましょう！ 以下から気軽に応募してください！https://open.talentio.com/1/c/feedforce/requisitions/detail/19785最後まで読んでいただき、ありがとうございました！*1:ストーリーポイント：プロダクトバックログ（タスク）を見積もるためにチームが使う単位で、前回の見積もりに対する相対評価を用いる*2:ベロシティ：スプリントの期間でチームが届けることができる見積もり（ストーリーポイント）の合計のこと*3:プロダクトオーナー：プロダクトバックログの管理をする人で、優先順位を付けることに責任を持つ（１人の人間が務める、委員会ではない）*4:バックログリファインメント：プランニングの前にプロダクトバックログを見直し、プランニング可能な状態にしておくこと","link":"https://developer.feedforce.jp/entry/2021/05/31/104813","isoDate":"2021-05-31T01:48:13.000Z","dateMiliSeconds":1622425693000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sukechannnn/20210526/20210526215948.png","authorName":"feedforce"},{"title":"エンジニアキャリアパスをアップデートしました","content":"<p>こんにちは、<a href=\"https://twitter.com/meihong\">meihong</a> です。</p>\n\n<p>株式会社フィードフォースでは<a href=\"https://media.feedforce.jp/n/nc7a2e89635eb\">定期評価ではなく本人の希望するタイミングで評価を行う制度</a>を導入しています。具体的には、各等級ごとに満たすべき基準・条件、またはスキルがあらかじめ提示されており、それを満たしていれば次の等級に進める制度になります。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fmedia.feedforce.jp%2Fn%2Fn222a08fd3e2b\" title=\"半年に1回の評価制度を毎月の評価制度に変えた話｜フィードフォースのnote\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://media.feedforce.jp/n/n222a08fd3e2b\">media.feedforce.jp</a></cite></p>\n\n<p>この基準やスキルを私たちはキャリアパスと呼んでいますが、今回、エンジニアのキャリアパスをアップデートしましたのでご紹介したいと思います。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/m/meihong/20210524/20210524010544.png\" alt=\"f:id:meihong:20210524010544p:plain\" width=\"1200\" height=\"630\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h2>なぜキャリアパスをアップデートしたのか</h2>\n\n<p>もともとのキャリアパスは<a href=\"https://media.feedforce.jp/n/n222a08fd3e2b\">導入当初に設計されたもの</a>をベースに、マネージャやエンジニア、新規事業向けエンジニアといった個々人の志向に応じて細分化されていました。</p>\n\n<p>これはこれでよくできたものだったのですが、しばらく運用している中でいくつかの課題点を感じるようになってきました。\n例えば、</p>\n\n<ul>\n<li>志向ごとに分かれすぎていて、志向を横断した動きが想定しづらくなった。</li>\n<li>独り立ちと判断される等級であるメンバーとその一つ上のシニアの境界に「見えない高い壁」が存在するようになった。</li>\n<li>シニア以上の等級になるとチームや会社を牽引することを求められ、技術をそれ以上深掘りすることに対して会社がどう考えているのかが見えづらくなった。</li>\n</ul>\n\n\n<p>といったところです。</p>\n\n<p>特にキャリアパス全体として、職種問わず等級が上がれば上がるほどチームや会社への影響力が求められる設計になっています。</p>\n\n<p>もちろんエンジニアも全体への影響力は持つべきなのですが、その持ち方は他の職種と異なり、技術力の広さ、深さといった持ち方もあるのではないかと考えるようになりました。</p>\n\n<p>ここで、個人的にはプロフェッショナルとしてのスキルは体積であり、その底面積はスキルの幅広さだと考えています。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/m/meihong/20210523/20210523235853.png\" alt=\"f:id:meihong:20210523235853p:plain\" width=\"1200\" height=\"731\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>極端に底面積が狭いのはさすがに現時点では厳しいとは思いますが、</p>\n\n<ul>\n<li>底面積がそれなりである代わりに高さ(= 深さ)がある</li>\n<li>底面積が広い (= 引き出しが多い) 反面高さはそこまででもない</li>\n</ul>\n\n\n<p>の両者は体積という意味では同じはずです。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/m/meihong/20210524/20210524000640.png\" alt=\"f:id:meihong:20210524000640p:plain\" width=\"1200\" height=\"576\" loading=\"lazy\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>この両者が共存できる余地が欲しいと考えていました。</p>\n\n<p>そんな中、弊社デザイナーのキャリアパスがアップデートされました。その中でも目を引いたのは、必須スキルと専門スキルという考え方です。</p>\n\n<p>必須スキルはデザイナーとして必ず持っていて欲しいスキルである一方、専門スキルは本人の志向、特性に応じてピックアップできるというもので、大学の専攻を思い出す建て付けでした。</p>\n\n<p><s>これをパクる</s>これにインスパイアされて、エンジニアのキャリアパスもアップデートすることにしました。</p>\n\n<h2>どのように更新したのか</h2>\n\n<p>結果から先にお伝えしておくと、大まかに以下のような方向性に改訂しました。</p>\n\n<ul>\n<li>志向ごとのキャリアパスは止めた。</li>\n<li>旧来の「志向」を専門スキルに分解し、専門スキルの組み合わせで個々人の志向・特性を表現できるようにした。</li>\n<li>等級が上がれば上がるほど満たすべき専門スキルの最低数が増えるようにした。</li>\n</ul>\n\n\n<p>その結果として、例えば</p>\n\n<ul>\n<li>バックエンドエンジニアに特化</li>\n<li>フルスタックエンジニア</li>\n<li>フルスタックな知識をベースに事業の 0 → 1 フェイズに参画できるエンジニア</li>\n<li>カスタマーサクセスエンジニア</li>\n<li>アジャイルコーチ</li>\n</ul>\n\n\n<p>といった、実際に社内に存在している各エンジニアの志向や得意なポイントを表現できるようになりました。</p>\n\n<h2>産みの苦しみ</h2>\n\n<p>ここに至るまでには色々な葛藤がありました。\n社内の esa にキャリアパスを更新したいと宣言はしたものの、社内のエンジニア個々人の顔を思い浮かべつつ何を専門スキルとして設定するかを考えると想像以上に難しい問題だということに気付きました。</p>\n\n<h3>必須スキルと専門スキル</h3>\n\n<p>そもそも必須スキルと専門スキルとは何か、そこの定義から考えることにしました。</p>\n\n<p>必須スキルとは文字通り、全てのエンジニアが共通に要求されるスキルセットのことです。どちらかというと「バックエンド」「フロントエンド」といった用語で定義されるスキルセットというよりも「フィードフォースに所属するエンジニアとしての振る舞い方」ではないでしょうか。</p>\n\n<p>そう考えながら改訂前のキャリアパスを改めて眺めていると、改訂前のキャリアパスはその振る舞いを定義していることに気付きました。その結果、改訂前のキャリアパスが必須スキルのベースとなりました。</p>\n\n<p>そうです、キャリアパスの改訂によって、より要求水準が上がったとも言えます。</p>\n\n<p>一方、専門スキルは、本人の得意分野、志向、特性を定義するものです。\nその志向・方向性で貢献するのであれば、各等級ごとにどの水準の成果を出すべきか。それを定義するものが専門スキルになります。</p>\n\n<h3>専門スキルとはどうあるべきか</h3>\n\n<p>本人の志向を定義するものが専門スキルと説明しましたが、例えばカスタマーサクセスエンジニアやエンジニアリングマネージャといった職種にしてもエンジニアの延長である以上はエンジニアとしての「共通言語」を身につけているべきです。</p>\n\n<p>その「共通言語」とは、例えば設計力であったり、フロントエンドやバックエンドのスキルが該当します。</p>\n\n<p>こういった知識を前提として例えば事業開発であったりチームビルディングを行うべきで、これらの知識がなければエンジニアとの「共通言語」を持っていないと判断せざるを得ません。</p>\n\n<p>一方で、「フロントエンド力」と「バックエンド力」が同じくらい強いエンジニアというのは SSR エンジニアで、そうそう市場には存在しません。そこで、フルスタックとはいえどこかの分野に軸足を置くことができる制度というのも必須に感じました。</p>\n\n<p>ただ、ここの軸足とはあくまでも「フロントエンド」「バックエンド」「インフラ」といった区分けで、エンジニアとしてコードを書き続ける選択をするのであれば、フロントエンド/バックエンド/インフラといった区分に関係なく設計力・実装力が担保されているべきでしょう。</p>\n\n<h3>17 の専門スキル</h3>\n\n<p>ここのバランス感が非常に難しい点でしたが、これを元に 17 の専門スキルを定義しました。\nただし、17 の専門スキルは完全に独立しているわけではなく、以下 6 つは本人の志向を定義するものとして、必ずどれか一つが必須選択としました。</p>\n\n<ul>\n<li>バックエンド</li>\n<li>フロントエンド</li>\n<li>データベース</li>\n<li>基盤</li>\n<li>カスタマーサクセス</li>\n<li>組織支援</li>\n</ul>\n\n\n<p>さらに、上記のうち以下 4 つを選択した場合は「実装・設計」と呼ばれるスキルが必須となります。</p>\n\n<ul>\n<li>バックエンド</li>\n<li>フロントエンド</li>\n<li>データベース</li>\n<li>基盤</li>\n</ul>\n\n\n<p>これにより、コードを書き続けるのであればただコードを書くだけでなく、実装力・設計力が要求される建て付けを実現しました。</p>\n\n<p>また、詳細は省きますが、さらにいくつかの例外を設置することで、「全ての分野で等しく強い SSR なフルスタックエンジニア」が求められないようにしています。</p>\n\n<hr />\n\n<p>様々なエッジケースを考慮したせいでちょっと複雑になった感の否めない新しいキャリアパスですが、以前のものと比べるとその分より柔軟なものになったと思います。</p>\n\n<p>今回は敢えて詳細を省きましたが、<a href=\"https://engineers.recruit.feedforce.jp/#entry\">ご興味をお持ちいただけたら是非カジュアル面談でねっちょりとご説明します</a>！</p>\n","contentSnippet":"こんにちは、meihong です。株式会社フィードフォースでは定期評価ではなく本人の希望するタイミングで評価を行う制度を導入しています。具体的には、各等級ごとに満たすべき基準・条件、またはスキルがあらかじめ提示されており、それを満たしていれば次の等級に進める制度になります。media.feedforce.jpこの基準やスキルを私たちはキャリアパスと呼んでいますが、今回、エンジニアのキャリアパスをアップデートしましたのでご紹介したいと思います。なぜキャリアパスをアップデートしたのかもともとのキャリアパスは導入当初に設計されたものをベースに、マネージャやエンジニア、新規事業向けエンジニアといった個々人の志向に応じて細分化されていました。これはこれでよくできたものだったのですが、しばらく運用している中でいくつかの課題点を感じるようになってきました。例えば、志向ごとに分かれすぎていて、志向を横断した動きが想定しづらくなった。独り立ちと判断される等級であるメンバーとその一つ上のシニアの境界に「見えない高い壁」が存在するようになった。シニア以上の等級になるとチームや会社を牽引することを求められ、技術をそれ以上深掘りすることに対して会社がどう考えているのかが見えづらくなった。といったところです。特にキャリアパス全体として、職種問わず等級が上がれば上がるほどチームや会社への影響力が求められる設計になっています。もちろんエンジニアも全体への影響力は持つべきなのですが、その持ち方は他の職種と異なり、技術力の広さ、深さといった持ち方もあるのではないかと考えるようになりました。ここで、個人的にはプロフェッショナルとしてのスキルは体積であり、その底面積はスキルの幅広さだと考えています。極端に底面積が狭いのはさすがに現時点では厳しいとは思いますが、底面積がそれなりである代わりに高さ(= 深さ)がある底面積が広い (= 引き出しが多い) 反面高さはそこまででもないの両者は体積という意味では同じはずです。この両者が共存できる余地が欲しいと考えていました。そんな中、弊社デザイナーのキャリアパスがアップデートされました。その中でも目を引いたのは、必須スキルと専門スキルという考え方です。必須スキルはデザイナーとして必ず持っていて欲しいスキルである一方、専門スキルは本人の志向、特性に応じてピックアップできるというもので、大学の専攻を思い出す建て付けでした。これをパクるこれにインスパイアされて、エンジニアのキャリアパスもアップデートすることにしました。どのように更新したのか結果から先にお伝えしておくと、大まかに以下のような方向性に改訂しました。志向ごとのキャリアパスは止めた。旧来の「志向」を専門スキルに分解し、専門スキルの組み合わせで個々人の志向・特性を表現できるようにした。等級が上がれば上がるほど満たすべき専門スキルの最低数が増えるようにした。その結果として、例えばバックエンドエンジニアに特化フルスタックエンジニアフルスタックな知識をベースに事業の 0 → 1 フェイズに参画できるエンジニアカスタマーサクセスエンジニアアジャイルコーチといった、実際に社内に存在している各エンジニアの志向や得意なポイントを表現できるようになりました。産みの苦しみここに至るまでには色々な葛藤がありました。社内の esa にキャリアパスを更新したいと宣言はしたものの、社内のエンジニア個々人の顔を思い浮かべつつ何を専門スキルとして設定するかを考えると想像以上に難しい問題だということに気付きました。必須スキルと専門スキルそもそも必須スキルと専門スキルとは何か、そこの定義から考えることにしました。必須スキルとは文字通り、全てのエンジニアが共通に要求されるスキルセットのことです。どちらかというと「バックエンド」「フロントエンド」といった用語で定義されるスキルセットというよりも「フィードフォースに所属するエンジニアとしての振る舞い方」ではないでしょうか。そう考えながら改訂前のキャリアパスを改めて眺めていると、改訂前のキャリアパスはその振る舞いを定義していることに気付きました。その結果、改訂前のキャリアパスが必須スキルのベースとなりました。そうです、キャリアパスの改訂によって、より要求水準が上がったとも言えます。一方、専門スキルは、本人の得意分野、志向、特性を定義するものです。その志向・方向性で貢献するのであれば、各等級ごとにどの水準の成果を出すべきか。それを定義するものが専門スキルになります。専門スキルとはどうあるべきか本人の志向を定義するものが専門スキルと説明しましたが、例えばカスタマーサクセスエンジニアやエンジニアリングマネージャといった職種にしてもエンジニアの延長である以上はエンジニアとしての「共通言語」を身につけているべきです。その「共通言語」とは、例えば設計力であったり、フロントエンドやバックエンドのスキルが該当します。こういった知識を前提として例えば事業開発であったりチームビルディングを行うべきで、これらの知識がなければエンジニアとの「共通言語」を持っていないと判断せざるを得ません。一方で、「フロントエンド力」と「バックエンド力」が同じくらい強いエンジニアというのは SSR エンジニアで、そうそう市場には存在しません。そこで、フルスタックとはいえどこかの分野に軸足を置くことができる制度というのも必須に感じました。ただ、ここの軸足とはあくまでも「フロントエンド」「バックエンド」「インフラ」といった区分けで、エンジニアとしてコードを書き続ける選択をするのであれば、フロントエンド/バックエンド/インフラといった区分に関係なく設計力・実装力が担保されているべきでしょう。17 の専門スキルここのバランス感が非常に難しい点でしたが、これを元に 17 の専門スキルを定義しました。ただし、17 の専門スキルは完全に独立しているわけではなく、以下 6 つは本人の志向を定義するものとして、必ずどれか一つが必須選択としました。バックエンドフロントエンドデータベース基盤カスタマーサクセス組織支援さらに、上記のうち以下 4 つを選択した場合は「実装・設計」と呼ばれるスキルが必須となります。バックエンドフロントエンドデータベース基盤これにより、コードを書き続けるのであればただコードを書くだけでなく、実装力・設計力が要求される建て付けを実現しました。また、詳細は省きますが、さらにいくつかの例外を設置することで、「全ての分野で等しく強い SSR なフルスタックエンジニア」が求められないようにしています。様々なエッジケースを考慮したせいでちょっと複雑になった感の否めない新しいキャリアパスですが、以前のものと比べるとその分より柔軟なものになったと思います。今回は敢えて詳細を省きましたが、ご興味をお持ちいただけたら是非カジュアル面談でねっちょりとご説明します！","link":"https://developer.feedforce.jp/entry/career_path_revised_2021","isoDate":"2021-05-24T02:00:00.000Z","dateMiliSeconds":1621821600000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/m/meihong/20210524/20210524010544.png","authorName":"feedforce"},{"title":"広告の複数媒体に対するCPA最小化・ROAS最大化となる予算配分を計算しよう","content":"<p>こんにちは　機械学習エンジニアの<a href=\"https://twitter.com/feed_yao\">八百俊哉</a>です。</p>\n\n<p>今回は複数媒体へ広告を出稿する際に、多くの方が悩まれるであろう「各媒体への予算配分」に関して有効な配分手法を紹介します。</p>\n\n<p><strong>今回の記事で登場する広告用語</strong></p>\n\n<ul>\n<li>媒体・・・広告の配信先や配信手法</li>\n<li>ROAS・・・広告経由で発生した売り上げを広告費用で割った値(広告の費用対効果)</li>\n<li>CPA・・・1件のコンバージョン(目標)を獲得するのにかかった広告コスト</li>\n</ul>\n\n\n<h1>広告運用者が抱える課題とは？</h1>\n\n<p>1つの媒体のみで運用している場合は別ですが、<strong>複数の媒体で広告配信を行っている場合は、どの媒体に対していくら予算を割り振れば良いのかわからない場合があると思います。</strong></p>\n\n<p>過去の実績を元に成果が良い媒体に対して、多く予算を割り振れば良いことは理解しているものの、<strong>「どれくらい」「どの媒体から」予算を割り振れば良いのか</strong>は経験則や簡単な分析で決めている方も多いのではないでしょうか？</p>\n\n<p>今回はこれらの課題を解決するために、<strong>数学的に根拠のある予算配分方法</strong>について紹介しようと思います。</p>\n\n<p>まず今回の手法を紹介するにあたり、例題がある方が話が進めやすいので以下の広告運用者さんを例に考えます。</p>\n\n<pre class=\"code\" data-lang=\"\" data-unlink>広告運用者○○さん\n\n現在A,B,Cの３媒体で広告配信を行っています。\n\n全体のROASを高めるために予算配分を見直したいと考えています。\n\n３媒体での合計予算は3万円です。</pre>\n\n\n<p>では、実際にどのようにして最適な予算を求めるのか見ていきましょう。</p>\n\n<h1>過去の実績から各媒体の実績をシミュレーションします</h1>\n\n<p>最初に過去の実績から各媒体での予算とROASの傾向を、式で表現します。</p>\n\n<p>ここで<strong>ROASを最大化するということは、限られた予算の中で売り上げを最大化すると言い換えることができる</strong>ので、今回は <img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%28%E4%BA%88%E7%AE%97%2C%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%29\" alt=\" (&#x4E88;&#x7B97;,&#x58F2;&#x308A;&#x4E0A;&#x3052;)\"/> を2次回帰で近似します。</p>\n\n<p>今回の例だと媒体A,B,Cに対してそれぞれ近似式が用意できるので以下のように表現できます。(各媒体の予算を<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/>とします)</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%5Cdisplaystyle%0AA%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_1%29%20%3D%20a_A%20x_1%5E2%20%2B%20b_A%20x_1%0A\" alt=\" \\displaystyle\nA&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_1) = a_A x_1^2 + b_A x_1\n\"/>\n</div>\n\n\n\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%5Cdisplaystyle%0AB%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_2%29%20%3D%20a_B%20x_2%5E2%20%2B%20b_B%20x_2%0A\" alt=\" \\displaystyle\nB&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_2) = a_B x_2^2 + b_B x_2\n\"/>\n</div>\n\n\n\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%5Cdisplaystyle%0AC%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_3%29%20%3D%20a_C%20x_3%5E2%20%2B%20b_C%20x_3%0A\" alt=\" \\displaystyle\nC&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_3) = a_C x_3^2 + b_C x_3\n\"/>\n</div>\n\n\n<p><br>\n予算が0円の時は、原点を通る(売り上げが0円)になるように切片は使用していないです。</p>\n\n<h1>ラグランジュの未定乗数法を用いて最適な予算配分を見つける</h1>\n\n<h2>ラグランジュの未定乗数法とは？</h2>\n\n<p>ラグランジュの未定乗数法とは、</p>\n\n<blockquote><p>束縛条件のもとで最適化を行うための数学的な方法である。いくつかの変数に対して、いくつかの関数の値を固定するという束縛条件のもとで、別のある1つの関数の極値を求める</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fja.wikipedia.org%2Fwiki%2F%25E3%2583%25A9%25E3%2582%25B0%25E3%2583%25A9%25E3%2583%25B3%25E3%2582%25B8%25E3%2583%25A5%25E3%2581%25AE%25E6%259C%25AA%25E5%25AE%259A%25E4%25B9%2597%25E6%2595%25B0%25E6%25B3%2595\" title=\"ラグランジュの未定乗数法 - Wikipedia\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://ja.wikipedia.org/wiki/%E3%83%A9%E3%82%B0%E3%83%A9%E3%83%B3%E3%82%B8%E3%83%A5%E3%81%AE%E6%9C%AA%E5%AE%9A%E4%B9%97%E6%95%B0%E6%B3%95\">ja.wikipedia.org</a></cite></p></blockquote>\n\n<p>少し小難しく聞こえますが、今回の例題に当てはめて考えてみます。</p>\n\n<pre class=\"code\" data-lang=\"\" data-unlink>いくつかの変数に対して(各媒体の予算)\n\nいくつかの関数の値を固定する束縛条件(3媒体の総予算は3万円)\n\n別のある関数の極値を求める(3媒体の売り上げが最大となるポイントを求める)</pre>\n\n\n<p>ラグランジュの未定乗数法とは、上のような条件を満たす予算<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/>を見つけてくれる手法です。</p>\n\n<p>ラグランジュの未定乗数法では、媒体A,B,Cのそれぞれの<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%28%E4%BA%88%E7%AE%97%2C%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%29\" alt=\" (&#x4E88;&#x7B97;,&#x58F2;&#x308A;&#x4E0A;&#x3052;)\"/> に対して<strong>近似式が二階微分可能である必要がある</strong>ため、今回の例では2次回帰で近似を行いました。</p>\n\n<p>また今回は、<strong>3媒体の総予算(束縛条件)が広告によって全て使用される</strong>という仮説のもとで計算しています。予算を全て使わない場合は、計算が複雑になってしまうので今回は紹介しません。</p>\n\n<h2>実際にどのように計算するのか検証します</h2>\n\n<p>まず初めに束縛条件<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/> を定義します。今回の束縛条件は、それぞれの予算<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/>を足し合わせたものが30000円になるということですので、以下のように書けます。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%0Ax_1%20%2B%20x_2%20%2B%20x_3%20%3D%2030000%0A%5Ctag%7B1%7D%0A\" alt=\" \nx_1 + x_2 + x_3 = 30000\n\\tag{1}\n\"/></div>\n\n\n<p><br>\nここで式(1)を変換し、<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20g%28x_1%2Cx_2%2Cx_3%29\" alt=\" g(x_1,x_2,x_3)\"/>とおきます。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%0Ag%28x_1%2Cx_2%2Cx_3%29%20%3D%20x_1%20%2B%20x_2%20%2B%20x_3%20-%2030000%20%3D%200%0A\" alt=\"\ng(x_1,x_2,x_3) = x_1 + x_2 + x_3 - 30000 = 0\n\"/></div>\n\n\n<p><br>\nまた、今回最大にしたい3媒体の総売り上げを<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20f%28x_1%2Cx_2%2Cx_3%29\" alt=\" f(x_1,x_2,x_3)\"/>と置きます。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%0A%5Cbegin%7Baligned%7D%0A%20f%28x_1%2Cx_2%2Cx_3%29%20%26%3D%20A%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_1%29%20%2B%20B%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_2%29%20%2B%20C%E3%81%AE%E5%A3%B2%E3%82%8A%E4%B8%8A%E3%81%92%28x_3%29%20%5C%5C%0A%26%3D%20a_A%20x_1%5E2%20%2B%20b_A%20x_1%20%2B%20a_B%20x_2%5E2%20%2B%20b_B%20x_2%20%2B%20a_C%20x_3%5E2%20%2B%20b_C%20x_3%0A%5Cend%7Baligned%7D%0A\" alt=\"\n\\begin{aligned}\n f(x_1,x_2,x_3) &amp;= A&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_1) + B&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_2) + C&#x306E;&#x58F2;&#x308A;&#x4E0A;&#x3052;(x_3) \\\\\n&amp;= a_A x_1^2 + b_A x_1 + a_B x_2^2 + b_B x_2 + a_C x_3^2 + b_C x_3\n\\end{aligned}\n\"/></div>\n\n\n<p><br>\nここで未定乗数<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%5Clambda%20\" alt=\" \\lambda \"/>と<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20f%28x_1%2Cx_2%2Cx_3%29%2Cg%28x_1%2Cx_2%2Cx_3%29\" alt=\" f(x_1,x_2,x_3),g(x_1,x_2,x_3)\"/>を用いてラグランジュ関数<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20L\" alt=\" L\"/>を作ります。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%0A%5Cbegin%7Baligned%7D%0AL%28x_1%2Cx_2%2Cx_3%2C%5Clambda%29%20%26%3D%20a_A%20x_1%5E2%20%2B%20b_A%20x_1%20%2B%20a_B%20x_2%5E2%20%2B%20b_B%20x_2%20%2B%20a_C%20x_3%5E2%20%2B%20b_C%20x_3%20-%20%5Clambda%20%28x_1%20%2B%20x_2%20%2B%20x_3%20-%2030000%29%0A%5Cend%7Baligned%7D%0A\" alt=\"\n\\begin{aligned}\nL(x_1,x_2,x_3,\\lambda) &amp;= a_A x_1^2 + b_A x_1 + a_B x_2^2 + b_B x_2 + a_C x_3^2 + b_C x_3 - \\lambda (x_1 + x_2 + x_3 - 30000)\n\\end{aligned}\n\"/></div>\n\n\n<p><br>\nそれぞれの変数で偏微分すると以下のようになります。</p>\n\n<div align=\"center\"><img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%0A%5Cbegin%7Baligned%7D%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_1%7D%20%26%3D%202%20a_A%20x_1%20%2B%20b_A%20-%20%5Clambda%20%20%3D%200%5C%5C%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_2%7D%20%26%3D%202%20a_B%20x_2%20%2B%20b_B%20-%20%5Clambda%20%3D%200%5C%5C%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_3%7D%20%26%3D%202%20a_C%20x_3%20%2B%20b_C%20-%20%5Clambda%20%3D%200%5C%5C%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Clambda%7D%20%26%3D%20-%20x_1%20-%20x_2%20-%20x_3%20%2B%2030000%20%3D%200%5C%5C%0A%5Cend%7Baligned%7D%0A\" alt=\"\n\\begin{aligned}\n\\frac{\\partial L}{\\partial x_1} &amp;= 2 a_A x_1 + b_A - \\lambda  = 0\\\\\n\\frac{\\partial L}{\\partial x_2} &amp;= 2 a_B x_2 + b_B - \\lambda = 0\\\\\n\\frac{\\partial L}{\\partial x_3} &amp;= 2 a_C x_3 + b_C - \\lambda = 0\\\\\n\\frac{\\partial L}{\\partial \\lambda} &amp;= - x_1 - x_2 - x_3 + 30000 = 0\\\\\n\\end{aligned}\n\"/></div>\n\n\n<p><br>\nこれら4変数の4元連立方程式を説くと、予算30000円で総売り上げが最大になる予算配分<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20x_1%2Cx_2%2Cx_3\" alt=\" x_1,x_2,x_3\"/>が求まります。</p>\n\n<p>今回は、ROASを最大化するための方法を紹介しましたがCPAを最小化する場合は2次回帰式を求める際に<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%28cost%2Ccv%29\" alt=\" (cost,cv)\"/>としてcvを最大化するようにラグランジュを適応することで求めることができます。</p>\n\n<p>また今回の例では3媒体までの予算配分を計算しましたが、<strong>媒体数を増やしても計算速度が極端に遅くなることがない</strong>ところが今回の手法の良いところです。</p>\n\n<h1>この手法の課題点</h1>\n\n<p>ここまで「ネット広告の複数媒体に対するCPA最小化・ROAS最大化となる予算配分」を紹介しましたが、この手法には2つほど課題があります。</p>\n\n<p>まず一つ目が、最適予算<img src=\"https://chart.apis.google.com/chart?cht=tx&chl=%20%28x_1%2Cx_2%2Cx_3%29\" alt=\" (x_1,x_2,x_3)\"/>にマイナスの結果が得られる可能性があるということです。売り上げを最大化しようとするあまり、もともとROASが低い媒体に対しては予算を割り振らずにマイナスの予算を割り振り、そのほかのROASが高い媒体により多くの予算を割り振ろうとしてしまうことが確認できています。</p>\n\n<p>次に、媒体の周期性や外部要因を一切考慮していないということです。広告は少なからず外部要因によって成果が左右されますが、この手法では過去の実績のみを用いて最適予算を割り振るので外部要因は一切考慮されていないということに注意が必要です。</p>\n\n<h1>まとめ</h1>\n\n<p>いかがだったでしょうか。\n今回は、ラグランジュの未定乗数法を用いて複数媒体への予算配分方法を紹介しました。流石に手作業では計算できないので私はpythonで上記の流れを実装しています。</p>\n\n<p>ラグランジュの未定乗数法は、理系の方は大学の数学の講義で習っていたかもしれないです。私も大学の時に習いましたが、当時は何に使うのか一切わかりませんでした。社会人になって学生の時に学んだことが活用できると、学んだ甲斐があったと感じることができて良いです。</p>\n\n<p>最後まで読んでいただきありがとうございます。</p>\n","contentSnippet":"こんにちは　機械学習エンジニアの八百俊哉です。今回は複数媒体へ広告を出稿する際に、多くの方が悩まれるであろう「各媒体への予算配分」に関して有効な配分手法を紹介します。今回の記事で登場する広告用語媒体・・・広告の配信先や配信手法ROAS・・・広告経由で発生した売り上げを広告費用で割った値(広告の費用対効果)CPA・・・1件のコンバージョン(目標)を獲得するのにかかった広告コスト広告運用者が抱える課題とは？1つの媒体のみで運用している場合は別ですが、複数の媒体で広告配信を行っている場合は、どの媒体に対していくら予算を割り振れば良いのかわからない場合があると思います。過去の実績を元に成果が良い媒体に対して、多く予算を割り振れば良いことは理解しているものの、「どれくらい」「どの媒体から」予算を割り振れば良いのかは経験則や簡単な分析で決めている方も多いのではないでしょうか？今回はこれらの課題を解決するために、数学的に根拠のある予算配分方法について紹介しようと思います。まず今回の手法を紹介するにあたり、例題がある方が話が進めやすいので以下の広告運用者さんを例に考えます。広告運用者○○さん現在A,B,Cの３媒体で広告配信を行っています。全体のROASを高めるために予算配分を見直したいと考えています。３媒体での合計予算は3万円です。では、実際にどのようにして最適な予算を求めるのか見ていきましょう。過去の実績から各媒体の実績をシミュレーションします最初に過去の実績から各媒体での予算とROASの傾向を、式で表現します。ここでROASを最大化するということは、限られた予算の中で売り上げを最大化すると言い換えることができるので、今回は  を2次回帰で近似します。今回の例だと媒体A,B,Cに対してそれぞれ近似式が用意できるので以下のように表現できます。(各媒体の予算をとします)ラグランジュの未定乗数法を用いて最適な予算配分を見つけるラグランジュの未定乗数法とは？ラグランジュの未定乗数法とは、束縛条件のもとで最適化を行うための数学的な方法である。いくつかの変数に対して、いくつかの関数の値を固定するという束縛条件のもとで、別のある1つの関数の極値を求めるja.wikipedia.org少し小難しく聞こえますが、今回の例題に当てはめて考えてみます。いくつかの変数に対して(各媒体の予算)いくつかの関数の値を固定する束縛条件(3媒体の総予算は3万円)別のある関数の極値を求める(3媒体の売り上げが最大となるポイントを求める)ラグランジュの未定乗数法とは、上のような条件を満たす予算を見つけてくれる手法です。ラグランジュの未定乗数法では、媒体A,B,Cのそれぞれの に対して近似式が二階微分可能である必要があるため、今回の例では2次回帰で近似を行いました。また今回は、3媒体の総予算(束縛条件)が広告によって全て使用されるという仮説のもとで計算しています。予算を全て使わない場合は、計算が複雑になってしまうので今回は紹介しません。実際にどのように計算するのか検証しますまず初めに束縛条件 を定義します。今回の束縛条件は、それぞれの予算を足し合わせたものが30000円になるということですので、以下のように書けます。ここで式(1)を変換し、とおきます。また、今回最大にしたい3媒体の総売り上げをと置きます。ここで未定乗数とを用いてラグランジュ関数を作ります。それぞれの変数で偏微分すると以下のようになります。これら4変数の4元連立方程式を説くと、予算30000円で総売り上げが最大になる予算配分が求まります。今回は、ROASを最大化するための方法を紹介しましたがCPAを最小化する場合は2次回帰式を求める際にとしてcvを最大化するようにラグランジュを適応することで求めることができます。また今回の例では3媒体までの予算配分を計算しましたが、媒体数を増やしても計算速度が極端に遅くなることがないところが今回の手法の良いところです。この手法の課題点ここまで「ネット広告の複数媒体に対するCPA最小化・ROAS最大化となる予算配分」を紹介しましたが、この手法には2つほど課題があります。まず一つ目が、最適予算にマイナスの結果が得られる可能性があるということです。売り上げを最大化しようとするあまり、もともとROASが低い媒体に対しては予算を割り振らずにマイナスの予算を割り振り、そのほかのROASが高い媒体により多くの予算を割り振ろうとしてしまうことが確認できています。次に、媒体の周期性や外部要因を一切考慮していないということです。広告は少なからず外部要因によって成果が左右されますが、この手法では過去の実績のみを用いて最適予算を割り振るので外部要因は一切考慮されていないということに注意が必要です。まとめいかがだったでしょうか。今回は、ラグランジュの未定乗数法を用いて複数媒体への予算配分方法を紹介しました。流石に手作業では計算できないので私はpythonで上記の流れを実装しています。ラグランジュの未定乗数法は、理系の方は大学の数学の講義で習っていたかもしれないです。私も大学の時に習いましたが、当時は何に使うのか一切わかりませんでした。社会人になって学生の時に学んだことが活用できると、学んだ甲斐があったと感じることができて良いです。最後まで読んでいただきありがとうございます。","link":"https://developer.feedforce.jp/entry/2021/05/13/093842","isoDate":"2021-05-17T00:38:42.000Z","dateMiliSeconds":1621211922000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"feedforce"},{"title":"ふりかえりカンファレンスのスタッフをやりました！","content":"<p>こんにちは <a href=\"http://blog.hatena.ne.jp/pokotyamu/\">id:pokotyamu</a> です！\n最近は、モンハンライズにハマっています！ハンマー担いでブンブンしてます！</p>\n\n<p>4月16日(土)に行われた「ふりかえりカンファレンス」のスタッフをやりました！</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/p/pokotyamu/20210416/20210416155303.png\" alt=\"f:id:pokotyamu:20210416155303p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>今回はそこでの学びや感じたことを社内勉強会で発表したので、スライドとコメントをまとめます。</p>\n\n<h2>FFTT 発表資料</h2>\n\n<script async class=\"speakerdeck-embed\" data-id=\"1f772bfe5abd4fa28ff738df3a5e76a2\" data-ratio=\"1.77777777777778\" src=\"//speakerdeck.com/assets/embed.js\"></script>\n\n\n<h2>勉強会の感想コメント</h2>\n\n<blockquote><p> なんのためにふりかえりやってるの？</p></blockquote>\n\n<p>明日の自分やチームを1歩でも楽しくなるためにやってほしいですね！\n連続したサイクルの中にふりかえりを組み込むことで、安全に転んで、次の1歩を早く出せるようになると思います！</p>\n\n<blockquote><p>オンラインセミナーは当日参加が多い\n結構人数集まったようなので準備とか大変そう</p></blockquote>\n\n<p>場所の制約がなくなったのが非常に大きいですね〜！\n国内・国外関係なく、どこでもいけるのが本当に便利！</p>\n\n<blockquote><p>振り返り手法ってあんなにたくさんあるのだなぁ\n振り返りの手法も多いようなのでどのタイミングで何を使うのが適切かを考えるの難しそう\n会社やチームよって向き不向きがありそうだけど、選ぶには知らないといけないので専門的な人がいる意味がよく分かる</p></blockquote>\n\n<p>そーなんですよね。\n次の Action を決めたい時や、関係構築したい時など、用途に合わせてやるのがいいと思います！\nもちろん KPT も素晴らしい手法なので、たまに気分を変えてみるみたいな感じでどうぞ！</p>\n\n<blockquote><p>振り返りとの因果関係を感じられる強い人やチームの実例を見たら、もう少しイメージが付くのかな\nと思っていたが、21卒の方の日報をザッピングしていたら、振り返りが役に立った、楽しいという風に書かれていた</p></blockquote>\n\n<p>今年は特に楽しいにフォーカスしてふりかえりをしているのもあると思います！</p>\n\n<blockquote><p>オンラインカンファレンスのスタッフの話って結構レアな気がするので興味深かった</p></blockquote>\n\n<p>楽しいのでぜひぜひ！</p>\n\n<h2>感想</h2>\n\n<p>私は、初めてカンファレンスのスタッフをやらせてもらったのですが、非常に楽しかったです！\n他の人の感想やブログレポートを見るのも、それそれ！その言葉待ってた！という感じでいつもの一般参加とは違う感覚で聞くことができました！</p>\n\n<p>今回のスタッフを経験したことで、「楽しくふりかえる」の意味を体で感じることができたと思います。もちろん当日の発表もどれも素晴らしくてそれも含みで楽しかったところではありましたが ☺️\n気軽に試す、実験してみるを最近のふりかえりでも挑戦中です。</p>\n\n<p>また、オンラインカンファレンスということもあり、夜の2次会が3時ぐらいまで盛り上がっていたのも楽しかったポイントでした。\n新しいつながりも持てたので、社内の知見をどんどん外に発信して自分の魅力を高めていければと思います。</p>\n\n<p>改めて、スタッフに誘っていただいた <a href=\"https://twitter.com/viva_tweet_x\">@viva_tweet_x</a> さんに改めて感謝です！ありがとうございました！これからもよろしくおねがいします！</p>\n","contentSnippet":"こんにちは id:pokotyamu です！最近は、モンハンライズにハマっています！ハンマー担いでブンブンしてます！4月16日(土)に行われた「ふりかえりカンファレンス」のスタッフをやりました！今回はそこでの学びや感じたことを社内勉強会で発表したので、スライドとコメントをまとめます。FFTT 発表資料勉強会の感想コメント なんのためにふりかえりやってるの？明日の自分やチームを1歩でも楽しくなるためにやってほしいですね！連続したサイクルの中にふりかえりを組み込むことで、安全に転んで、次の1歩を早く出せるようになると思います！オンラインセミナーは当日参加が多い結構人数集まったようなので準備とか大変そう場所の制約がなくなったのが非常に大きいですね〜！国内・国外関係なく、どこでもいけるのが本当に便利！振り返り手法ってあんなにたくさんあるのだなぁ振り返りの手法も多いようなのでどのタイミングで何を使うのが適切かを考えるの難しそう会社やチームよって向き不向きがありそうだけど、選ぶには知らないといけないので専門的な人がいる意味がよく分かるそーなんですよね。次の Action を決めたい時や、関係構築したい時など、用途に合わせてやるのがいいと思います！もちろん KPT も素晴らしい手法なので、たまに気分を変えてみるみたいな感じでどうぞ！振り返りとの因果関係を感じられる強い人やチームの実例を見たら、もう少しイメージが付くのかなと思っていたが、21卒の方の日報をザッピングしていたら、振り返りが役に立った、楽しいという風に書かれていた今年は特に楽しいにフォーカスしてふりかえりをしているのもあると思います！オンラインカンファレンスのスタッフの話って結構レアな気がするので興味深かった楽しいのでぜひぜひ！感想私は、初めてカンファレンスのスタッフをやらせてもらったのですが、非常に楽しかったです！他の人の感想やブログレポートを見るのも、それそれ！その言葉待ってた！という感じでいつもの一般参加とは違う感覚で聞くことができました！今回のスタッフを経験したことで、「楽しくふりかえる」の意味を体で感じることができたと思います。もちろん当日の発表もどれも素晴らしくてそれも含みで楽しかったところではありましたが ☺️気軽に試す、実験してみるを最近のふりかえりでも挑戦中です。また、オンラインカンファレンスということもあり、夜の2次会が3時ぐらいまで盛り上がっていたのも楽しかったポイントでした。新しいつながりも持てたので、社内の知見をどんどん外に発信して自分の魅力を高めていければと思います。改めて、スタッフに誘っていただいた @viva_tweet_x さんに改めて感謝です！ありがとうございました！これからもよろしくおねがいします！","link":"https://developer.feedforce.jp/entry/2021/04/19/141153","isoDate":"2021-04-19T05:11:53.000Z","dateMiliSeconds":1618809113000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/p/pokotyamu/20210416/20210416155303.png","authorName":"feedforce"},{"title":" 夜間光データから土地価格を予測 コンペの参加記録","content":"<p>こんにちは\n株式会社フィードフォース2020年入社の機械学習エンジニア\n<a href=\"https://twitter.com/feed_yao\">八百　俊哉</a>と申します。</p>\n\n<p>今回は、solafuneで開催された「<a href=\"https://solafune.com/#/competitions/f03f39cc-597b-4819-b1a5-41479d4b73d6\">夜間光データから土地価格を予測</a>」という機械学習コンペに参加したので工夫した点や反省点などを紹介します。</p>\n\n<p>コンペ参加の目標設定としては、「賞金獲得！！（4位以内）」を設定していましたが、36位/201人中と目標達成できませんでした。残念な結果に終わってしまいましたが、多くのことを学ぶことができました。</p>\n\n<h1>参加経緯</h1>\n\n<p>私は、2020年10月から2021年2月ごろまで顧客の課題解決のために機械学習を応用する方法を学ぶためにAI Questというイベントに参加していました。そのイベントをきっかけに私は精度の高いモデルや良い特徴量を作成することに興味を持ちました。</p>\n\n<p>そこでより多くのコンペに参加することで精度を上げるためのノウハウを身に付けたいと思ったことが今回のコンペに参加したきっかけです。</p>\n\n<p>また、今回参加したコンペは与えられている特徴量が4つしかないので、初心者が参加しやすいコンペだったということも魅力的なポイントでした。</p>\n\n<h1>課題と与えられているデータ</h1>\n\n<p>課題としては、「夜間光データを元に土地価格を予測するアルゴリズムを開発する」というものです。\n使用可能なデータとしては、以下のものが与えられました。</p>\n\n<ul>\n<li>地域ごとのデータ・・・地域固有のID</li>\n<li>年代・・・1992~2013年まで</li>\n<li>土地の平均価格（目的変数）・・・1992~2013年まで</li>\n<li>夜間光量の平均値・・・0~63までのレンジでその地域の平均光量</li>\n<li>夜間光量の合計値・・・その地域の合計光量</li>\n</ul>\n\n\n<h1>全体構成</h1>\n\n<p>今回最終submitとして選択したモデルの全体構成は以下です。\n<span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210409/20210409155716.png\" alt=\"f:id:newton800:20210409155716p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h1>前処理に関して</h1>\n\n<h2>集約的特徴量について</h2>\n\n<p>集約的特徴量の作成にあたっては<a href=\"https://twitter.com/mst_8823\">masato8823 (@mst_8823) | Twitter</a>さんがBaseLineとして公開されていた以下のものを使用しました。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fzenn.dev%2Fmst8823%2Farticles%2Fcd40cb971f702e\" title=\"[solafune] 夜間光データから土地価格を予測 BaseLine\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://zenn.dev/mst8823/articles/cd40cb971f702e\">zenn.dev</a></cite></p>\n\n<p>作成した特徴量としては以下です。</p>\n\n<table>\n<thead>\n<tr>\n<th>    </th>\n<th>    </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>  面積  </td>\n<td>  夜間光量の合計値/夜間光量の平均値を行い面積を算出した </td>\n</tr>\n<tr>\n<td>   PlaceID,Yearごとの統計情報 </td>\n<td>  PlaceID,Yearをキーとして平均光量、合計光量、面積のmin,max,median,mean,std,max-min,q75-q25を算出した  </td>\n</tr>\n<tr>\n<td> PlaceID をキーにしたグループ内差分</td>\n<td>  平均光量、合計光量の年ごとの差分を算出した</td>\n</tr>\n<tr>\n<td>PlaceID をキーにしたグループ内シフト </td>\n<td> 平均光量、合計光量の年ごとの値をシフトした</td>\n</tr>\n<tr>\n<td>ピボットテーブルを用いた特徴量</td>\n<td>index=PlaceID,columns=Yearとして平均光量、合計光量、面積のピボットテーブルを作成し、PCAで次元削減したものを算出した</td>\n</tr>\n<tr>\n<td>PlaceIDをキーにしたグループ内相関係数</td>\n<td>PlaceIDごとにデータを集約しYearと平均光量、合計光量、面積との相関係数を算出した</td>\n</tr>\n<tr>\n<td>平均光量が63であった回数</td>\n<td>平均光量の最大値が63であることから平均光量が63である数を追加した</td>\n</tr>\n</tbody>\n</table>\n\n\n<h2>Area特徴量について</h2>\n\n<p>先ほど<i>集約的特徴量について</i>で面積の求め方について書きました。面積=合計光量/平均光量で算出しています。ここで求められる<b>土地の面積は、年が変化しようと変化しないと思われますが、実際のデータを確認すると年が変化すると面積も変化していました。</b></p>\n\n<p>そこで<b>合計光量/平均光量より算出された面積をPlaceIDをキーとして平均を取ったものを新たな面積としました。</b>\n新たな面積が求まると <b>新たな合計光量 =  平均光量×新たな面積,新たな平均面積 = 合計光量/新たな面積</b> が求まります。</p>\n\n<p>これらより求まる新たな合計光量、新たな平均光量、新たな面積を元々の合計光量、平均光量、面積と置き換えて集約的特徴量の作成を行いました。</p>\n\n<h2>gplearnについて</h2>\n\n<p>上で紹介した集約的特徴量とArea特徴量のそれぞれに対して<a href=\"https://gplearn.readthedocs.io/en/stable/\">gplearn</a>というライブラリを用いて新たな特徴量を作成しました。このライブラリは遺伝的アルゴリズムにより目的変数をよく表している変数を作成してくれるものです。</p>\n\n<p>このライブラリを用いて新しい特徴量を10個,25個,50個作成し、元々の集約的特徴量、Area特徴量と組み合わせてそれぞれに対して予測を行いました。</p>\n\n<p>gplearnでの特徴量作成については以下のサイトが参考になります。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fqiita.com%2FHatomugi%2Fitems%2F3bb16ed9c6bdc15f1e00\" title=\"遺伝的アルゴリズムを使って特徴量エンジニアリングしてみた - Qiita\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://qiita.com/Hatomugi/items/3bb16ed9c6bdc15f1e00\">qiita.com</a></cite></p>\n\n<h1>モデル構築に関して</h1>\n\n<p>モデルの構築としてはgroup k fold(fold=5)でStackingのモデルを採用しました。</p>\n\n<p>1層目はrandom forest,lgb,multi regression,catboost,xgboostに加えてAutoMLの<a href=\"https://auto.gluon.ai/stable/index.html\">Autogluon</a>を採用しました。</p>\n\n<p>Autogluonは以下のようにデータを渡すだけで、11個のモデルを検証し最後に出力結果を重量平均で作成してくれます。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>\npredictor = TabularPredictor(\n                                label=<span class=\"synConstant\">'label'</span>,\n                                problem_type=<span class=\"synConstant\">'regression'</span>, \n                                eval_metric=<span class=\"synConstant\">'root_mean_squared_error'</span>, <span class=\"synComment\"># 評価指標</span>\n)\n\nX_train[<span class=\"synConstant\">'label'</span>] = y_train\nX_test[<span class=\"synConstant\">'label'</span>] = y_test\n\npredictor.fit(\n            train_data=X_train,\n            tuning_data=X_test, <span class=\"synComment\"># これを渡さない場合はランダムスプリット</span>\n            time_limit=<span class=\"synIdentifier\">None</span>, <span class=\"synComment\"># おおよその時間制限を設けられる</span>\n)\n</pre>\n\n\n<p>そして2層目は1層目でも採用しているAutogluonで出力を作成しました。</p>\n\n<h1>感想・反省点</h1>\n\n<p>Public Scoreの時点では6位と賞金獲得の可能性が十分にありましたが、Private Scoreでは36位と大幅にshake downしてしまいました。今回目標達成できなかった理由としては以下の2つが考えられます。</p>\n\n<p><b>1 CVの値とPublic ScoreからPrivate Scoreについて考えられなかった\n</b></p>\n\n<p>1つ目の要因としては、Public Scoreが下がることのみを考えてモデルの改善・特徴量の作成を行っていたということです。その時CVの値とPublic Scoreをどこかに記録しておけばよかったのですが、どこにも保存せずPublic Scoreが下がることが最も良いことであると捉えていました。実際は、CVが下がったモデル・特徴量においてPublic Scoreも同じように下がることが望ましく、その記録を取っておくべきでした。</p>\n\n<p>実際これまで提出していたファイルの中にPrivate Scoreが0.48774というものがあり、このファイルを最終提出としておけば3位に入ることができていました。しっかりとPrivate Scoreに効いているであろう提出ファイルが選べるようにCVとPublic Scoreに着目できるようにならないといけないと感じました。</p>\n\n<p><b>2 gplearnを行う位置が悪かった\n</b></p>\n\n<p>2つめは、group k foldを行う前にgplearnを行ったことによって、validationの目的変数が確認できる状態でgplearnが特徴量作成を行ってたことです。これは本来見ることができないデータを確認しながらデータ生成を行っていることになるので過学習を引き起こす可能性がありました。\n　\n　<span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210413/20210413095008.png\" alt=\"f:id:newton800:20210413095008p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>あるべき姿としては、group k foldでtrainをtrain,validationに分割した後にtrainのみのデータを用いてgplearnをfitさせるべきだったと思います。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210413/20210413112402.png\" alt=\"f:id:newton800:20210413112402p:plain\" title=\"\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h1>次回コンペでは</h1>\n\n<p>今回のコンペを通じて集約的特徴量の作成方法、Stackingの実装方法、gplearnの実行位置、CVとPublic Scoreの関係性の重要度について学ぶことができました。\nテーブルコンペ において有効な手法を多く学ぶことができたので、次回参加するコンペでは賞金獲得を目標に頑張ります！！</p>\n","contentSnippet":"こんにちは株式会社フィードフォース2020年入社の機械学習エンジニア八百　俊哉と申します。今回は、solafuneで開催された「夜間光データから土地価格を予測」という機械学習コンペに参加したので工夫した点や反省点などを紹介します。コンペ参加の目標設定としては、「賞金獲得！！（4位以内）」を設定していましたが、36位/201人中と目標達成できませんでした。残念な結果に終わってしまいましたが、多くのことを学ぶことができました。参加経緯私は、2020年10月から2021年2月ごろまで顧客の課題解決のために機械学習を応用する方法を学ぶためにAI Questというイベントに参加していました。そのイベントをきっかけに私は精度の高いモデルや良い特徴量を作成することに興味を持ちました。そこでより多くのコンペに参加することで精度を上げるためのノウハウを身に付けたいと思ったことが今回のコンペに参加したきっかけです。また、今回参加したコンペは与えられている特徴量が4つしかないので、初心者が参加しやすいコンペだったということも魅力的なポイントでした。課題と与えられているデータ課題としては、「夜間光データを元に土地価格を予測するアルゴリズムを開発する」というものです。使用可能なデータとしては、以下のものが与えられました。地域ごとのデータ・・・地域固有のID年代・・・1992~2013年まで土地の平均価格（目的変数）・・・1992~2013年まで夜間光量の平均値・・・0~63までのレンジでその地域の平均光量夜間光量の合計値・・・その地域の合計光量全体構成今回最終submitとして選択したモデルの全体構成は以下です。前処理に関して集約的特徴量について集約的特徴量の作成にあたってはmasato8823 (@mst_8823) | TwitterさんがBaseLineとして公開されていた以下のものを使用しました。zenn.dev作成した特徴量としては以下です。          面積    夜間光量の合計値/夜間光量の平均値を行い面積を算出した    PlaceID,Yearごとの統計情報   PlaceID,Yearをキーとして平均光量、合計光量、面積のmin,max,median,mean,std,max-min,q75-q25を算出した   PlaceID をキーにしたグループ内差分  平均光量、合計光量の年ごとの差分を算出したPlaceID をキーにしたグループ内シフト  平均光量、合計光量の年ごとの値をシフトしたピボットテーブルを用いた特徴量index=PlaceID,columns=Yearとして平均光量、合計光量、面積のピボットテーブルを作成し、PCAで次元削減したものを算出したPlaceIDをキーにしたグループ内相関係数PlaceIDごとにデータを集約しYearと平均光量、合計光量、面積との相関係数を算出した平均光量が63であった回数平均光量の最大値が63であることから平均光量が63である数を追加したArea特徴量について先ほど集約的特徴量についてで面積の求め方について書きました。面積=合計光量/平均光量で算出しています。ここで求められる土地の面積は、年が変化しようと変化しないと思われますが、実際のデータを確認すると年が変化すると面積も変化していました。そこで合計光量/平均光量より算出された面積をPlaceIDをキーとして平均を取ったものを新たな面積としました。新たな面積が求まると 新たな合計光量 =  平均光量×新たな面積,新たな平均面積 = 合計光量/新たな面積 が求まります。これらより求まる新たな合計光量、新たな平均光量、新たな面積を元々の合計光量、平均光量、面積と置き換えて集約的特徴量の作成を行いました。gplearnについて上で紹介した集約的特徴量とArea特徴量のそれぞれに対してgplearnというライブラリを用いて新たな特徴量を作成しました。このライブラリは遺伝的アルゴリズムにより目的変数をよく表している変数を作成してくれるものです。このライブラリを用いて新しい特徴量を10個,25個,50個作成し、元々の集約的特徴量、Area特徴量と組み合わせてそれぞれに対して予測を行いました。gplearnでの特徴量作成については以下のサイトが参考になります。qiita.comモデル構築に関してモデルの構築としてはgroup k fold(fold=5)でStackingのモデルを採用しました。1層目はrandom forest,lgb,multi regression,catboost,xgboostに加えてAutoMLのAutogluonを採用しました。Autogluonは以下のようにデータを渡すだけで、11個のモデルを検証し最後に出力結果を重量平均で作成してくれます。'label',                                problem_type='regression',                                 eval_metric='root_mean_squared_error', # 評価指標)X_train['label'] = y_trainX_test['label'] = y_testpredictor.fit(            train_data=X_train,            tuning_data=X_test, # これを渡さない場合はランダムスプリット            time_limit=None, # おおよその時間制限を設けられる)そして2層目は1層目でも採用しているAutogluonで出力を作成しました。感想・反省点Public Scoreの時点では6位と賞金獲得の可能性が十分にありましたが、Private Scoreでは36位と大幅にshake downしてしまいました。今回目標達成できなかった理由としては以下の2つが考えられます。1 CVの値とPublic ScoreからPrivate Scoreについて考えられなかった1つ目の要因としては、Public Scoreが下がることのみを考えてモデルの改善・特徴量の作成を行っていたということです。その時CVの値とPublic Scoreをどこかに記録しておけばよかったのですが、どこにも保存せずPublic Scoreが下がることが最も良いことであると捉えていました。実際は、CVが下がったモデル・特徴量においてPublic Scoreも同じように下がることが望ましく、その記録を取っておくべきでした。実際これまで提出していたファイルの中にPrivate Scoreが0.48774というものがあり、このファイルを最終提出としておけば3位に入ることができていました。しっかりとPrivate Scoreに効いているであろう提出ファイルが選べるようにCVとPublic Scoreに着目できるようにならないといけないと感じました。2 gplearnを行う位置が悪かった2つめは、group k foldを行う前にgplearnを行ったことによって、validationの目的変数が確認できる状態でgplearnが特徴量作成を行ってたことです。これは本来見ることができないデータを確認しながらデータ生成を行っていることになるので過学習を引き起こす可能性がありました。　　あるべき姿としては、group k foldでtrainをtrain,validationに分割した後にtrainのみのデータを用いてgplearnをfitさせるべきだったと思います。次回コンペでは今回のコンペを通じて集約的特徴量の作成方法、Stackingの実装方法、gplearnの実行位置、CVとPublic Scoreの関係性の重要度について学ぶことができました。テーブルコンペ において有効な手法を多く学ぶことができたので、次回参加するコンペでは賞金獲得を目標に頑張ります！！","link":"https://developer.feedforce.jp/entry/2021/04/13/174808","isoDate":"2021-04-13T08:48:08.000Z","dateMiliSeconds":1618303688000,"imageUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/n/newton800/20210409/20210409155716.png","authorName":"feedforce"},{"title":"エンジニア向けミートアップを開催します！","content":"<p>こんにちは。人事チームからエンジニアミートアップについてお知らせです。</p>\n\n<p>3月26日（金）19：00から、エンジニア向けのミートアップを開催することになりました！\n選考とは関係ないので、純粋に「どんなエンジニアがいるかみてみたい」「会社の雰囲気を知りたい」\nという方もぜひご参加いただけたら嬉しいです。</p>\n\n<p>初回のLT登壇者は <a href=\"https://twitter.com/daido1976\">@daido1976</a>です！</br>\n詳しくは下記記事をご覧ください。</p>\n\n<p><iframe src=\"https://hatenablog-parts.com/embed?url=https%3A%2F%2Fmedia.feedforce.jp%2Fn%2Fn53b8be5eae4a\" title=\"現場エンジニアと気軽に話せる！エンジニアミートアップを開催します｜フィードフォースのnote\" class=\"embed-card embed-webcard\" scrolling=\"no\" frameborder=\"0\" style=\"display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;\"></iframe><cite class=\"hatena-citation\"><a href=\"https://media.feedforce.jp/n/n53b8be5eae4a\">media.feedforce.jp</a></cite></p>\n\n<p>ご参加お待ちしています！</p>\n","contentSnippet":"こんにちは。人事チームからエンジニアミートアップについてお知らせです。3月26日（金）19：00から、エンジニア向けのミートアップを開催することになりました！選考とは関係ないので、純粋に「どんなエンジニアがいるかみてみたい」「会社の雰囲気を知りたい」という方もぜひご参加いただけたら嬉しいです。初回のLT登壇者は @daido1976です！media.feedforce.jpご参加お待ちしています！","link":"https://developer.feedforce.jp/entry/2021/03/15/113230","isoDate":"2021-03-15T02:32:30.000Z","dateMiliSeconds":1615775550000,"imageUrl":"https://cdn.user.blog.st-hatena.com/default_entry_og_image/4268819/1588226000876991","authorName":"feedforce"}]},"__N_SSG":true}